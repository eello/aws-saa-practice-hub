---
created: 2025-10-18 11:08:48
last_modified: 2025-10-18 12:14:15
---
## #621
한 온라인 사진 공유 회사가 us-west-1 리전에 존재하는 Amazon S3 버킷에 사진을 저장하고 있습니다. 회사는 모든 새로운 사진의 사본을 us-east-1 리전에 저장해야 합니다.

다음 중 최소한의 운영 노력으로 이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. us-east-1에 두 번째 S3 버킷을 생성합니다. S3 교차 리전 복제(CRR)를 사용하여 기존 S3 버킷에서 두 번째 S3 버킷으로 사진을 복사합니다.
B. 기존 S3 버킷의 CORS(교차 출처 리소스 공유) 구성을 생성합니다. CORS 규칙의 AllowedOrigin 요소에 us-east-1을 지정합니다.
C. 여러 가용 영역에 걸쳐 us-east-1에 두 번째 S3 버킷을 생성합니다. S3 수명 주기(Lifecycle) 규칙을 생성하여 사진을 두 번째 S3 버킷에 저장합니다.
D. us-east-1에 두 번째 S3 버킷을 생성합니다. 객체 생성 및 업데이트 이벤트에 대해 S3 이벤트 알림을 구성하여 AWS Lambda 함수를 호출하고, 기존 S3 버킷에서 두 번째 S3 버킷으로 사진을 복사합니다.

```
An online photo-sharing company stores its photos in an Amazon S3 bucket that exists in the us-west-1 Region. The company needs to store a copy of all new photos in the us-east-1 Region.  
  
Which solution will meet this requirement with the LEAST operational effort?

- A. Create a second S3 bucket in us-east-1. Use S3 Cross-Region Replication to copy photos from the existing S3 bucket to the second S3 bucket.
- B. Create a cross-origin resource sharing (CORS) configuration of the existing S3 bucket. Specify us-east-1 in the CORS rule's AllowedOrigin element.
- C. Create a second S3 bucket in us-east-1 across multiple Availability Zones. Create an S3 Lifecycle rule to save photos into the second S3 bucket.
- D. Create a second S3 bucket in us-east-1. Configure S3 event notifications on object creation and update events to invoke an AWS Lambda function to copy photos from the existing S3 bucket to the second S3 bucket.
```

정답 : `A`

- S3 Cross-Region Replication(CRR)은 소스 버킷에 새 객체가 업로드될 때 자동으로 대상 리전 버킷으로 복제해주는 관리형 기능
- 설정만 하면 지속 운영이 필요 없고, 메타데이터/버전/암호화 정책 등도 정책에 따라 함께 복제할 수있어 운영 오버헤드가 가장 적음

오답 이유

- B. CORS는 브라우저의 크로스 도메인 요청 제어를 위한 설정으로, 버킷 간 데이터 복제를 수행하지 않습니다.

- C. Lifecycle 규칙은 보관/이전/만료 정책(클래스 전환 등)이며, 다른 버킷/리전으로의 복제 기능이 없습니다.

- D. 이벤트 + Lambda로 자체 복제 로직을 만들 수 있으나, 오류 처리/재시도/권한/중복 처리 등 운영 복잡도가 커집니다. CRR이 관리형으로 더 단순합니다.

## #622
한 회사가 구독자를 위한 새로운 웹 애플리케이션을 만들고 있습니다. 애플리케이션은 정적 단일 페이지와 영속적인 데이터베이스 계층으로 구성됩니다. 애플리케이션은 오전 4시간 동안 수백만 명의 사용자가 있을 것이지만, 나머지 시간에는 수천 명의 사용자만 있을 것입니다. 회사의 데이터 아키텍트는 스키마를 빠르게 발전시킬 수 있는 능력을 요청했습니다.

다음 중 이러한 요구 사항을 충족하고 가장 뛰어난 확장성을 제공하는 솔루션은 무엇입니까? (두 가지 선택)

A. 데이터베이스 솔루션으로 Amazon DynamoDB를 배포합니다. 온디맨드 용량을 프로비저닝합니다.
B. 데이터베이스 솔루션으로 Amazon Aurora를 배포합니다. 서버리스 DB 엔진 모드를 선택합니다.
C. 데이터베이스 솔루션으로 Amazon DynamoDB를 배포합니다. DynamoDB 자동 스케일링이 활성화되어 있는지 확인합니다.
D. 정적 콘텐츠를 Amazon S3 버킷에 배포합니다. S3 버킷을 오리진으로 하는 Amazon CloudFront 배포를 프로비저닝합니다.
E. 정적 콘텐츠를 위한 웹 서버를 Amazon EC2 인스턴스의 Auto Scaling 그룹 전반에 배포합니다. 인스턴스가 Amazon Elastic File System(Amazon EFS) 볼륨에서 콘텐츠를 주기적으로 새로 고치도록 구성합니다.

```
A company is creating a new web application for its subscribers. The application will consist of a static single page and a persistent database layer. The application will have millions of users for 4 hours in the morning, but the application will have only a few thousand users during the rest of the day. The company's data architects have requested the ability to rapidly evolve their schema.  
  
Which solutions will meet these requirements and provide the MOST scalability? (Choose two.)

- A. Deploy Amazon DynamoDB as the database solution. Provision on-demand capacity.
- B. Deploy Amazon Aurora as the database solution. Choose the serverless DB engine mode.
- C. Deploy Amazon DynamoDB as the database solution. Ensure that DynamoDB auto scaling is enabled.
- D. Deploy the static content into an Amazon S3 bucket. Provision an Amazon CloudFront distribution with the S3 bucket as the origin.
- E. Deploy the web servers for static content across a fleet of Amazon EC2 instances in Auto Scaling groups. Configure the instances to periodically refresh the content from an Amazon Elastic File System (Amazon EFS) volume.
```

정답 : `A, D`

- A. DynamoDB 온디맨드: 스키마 유연성과 예측 불가한 대규모 스파이크에 즉시 대응하는 온디맨드 과금/성능으로 최고 수준의 확장성과 운영 단순성을 제공
- D. S3 + CloudFront: 정적 단일 페이지는 S3에 호스팅 + CloudFront 전 세계 엣지 캐싱으로 수백만 사용자 트래픽을 저지연/고확장으로 처리

오답 이유

- B. Aurora Serverless v2는 변동 트래픽을 잘 처리하지만 관계형 스키마 변경은 NoSQL(DynamoDB)만큼 신속·유연하지 않습니다. 또한 대규모 스파이크에 대한 수평 확장 탄력성/비용 최적화 측면에서 DynamoDB 온디맨드가 더 적합합니다.

- C. DynamoDB 자동 스케일링은 유효한 선택이나, 급격한 스파이크에 비해 반응이 느리거나 예열이 필요할 수 있습니다. 온디맨드 모드는 별도 용량 계획 없이 즉시 확장되어 스파이크 패턴에 더 적합합니다.

- E. 정적 콘텐츠를 EC2 플릿+EFS로 제공하는 것은 불필요하게 복잡하고 비용이 증가합니다. 정적 사이트는 S3+CloudFront가 더 단순하고 확장성이 뛰어납니다.


## #623
한 회사는 제3자 서비스 공급자가 액세스하는 REST API를 관리하기 위해 Amazon API Gateway를 사용합니다.  
회사는 REST API를 **SQL 인젝션(SQL injection)** 및 **크로스 사이트 스크립팅(XSS)** 공격으로부터 보호해야 합니다.

이 요구 사항을 충족하면서 **운영 효율성이 가장 높은 솔루션**은 무엇입니까?

A. AWS Shield를 구성합니다.  
B. AWS WAF를 구성합니다.  
C. API Gateway를 Amazon CloudFront 배포와 함께 설정합니다. CloudFront에 AWS Shield를 구성합니다.  
D. API Gateway를 Amazon CloudFront 배포와 함께 설정합니다. CloudFront에 AWS WAF를 구성합니다.

```
A company uses Amazon API Gateway to manage its REST APIs that third-party service providers access. The company must protect the REST APIs from SQL injection and cross-site scripting attacks.  
  
What is the MOST operationally efficient solution that meets these requirements?

- A. Configure AWS Shield.
- B. Configure AWS WAF.
- C. Set up API Gateway with an Amazon CloudFront distribution. Configure AWS Shield in CloudFront.
- D. Set up API Gateway with an Amazon CloudFront distribution. Configure AWS WAF in CloudFront.
```

정답 : `B`

- SQL 인젝션 및 XSS 공격은 애플리케이션 계층(L7) 공격으로, 이를 탐지하고 차단하기 위한 AWS 서비스는 AWS WAF
- AWS WAF는 API Gateway와 직접 통합 가능하며, 별도의 CloudFront 배포 없이도 REST API 엔드포인트에 바로 연결 가능
- 구성이 단순하고 운영오버헤드가 최소화되는 가장 효율적인 방법

오답 이유

- A. AWS Shield는 DDoS(분산 서비스 거부) 공격 방어 서비스로, SQL 인젝션이나 XSS 같은 애플리케이션 계층 공격을 차단하지 못합니다.

- C. Shield는 여전히 DDoS 방어 전용이며, CloudFront를 추가하는 것은 불필요한 구성 복잡도를 초래합니다. SQL/XSS 방어는 수행하지 않습니다.

- D. AWS WAF를 CloudFront에 연결하는 방법도 가능하지만, CloudFront를 추가로 설정할 필요가 없습니다.  
     API Gateway는 AWS WAF와 직접 통합되므로 CloudFront를 사용하는 것은 불필요한 오버헤드입니다.


## #624
회사는 사용자들에게 AWS 리소스에 대한 액세스를 제공하려고 합니다. 회사에는 1,500명의 사용자가 있으며, 사내 네트워크의 Active Directory 사용자 그룹을 통해 온프레미스 리소스에 대한 액세스를 관리하고 있습니다. 그러나 회사는 사용자가 리소스에 액세스하기 위해 다른 ID를 별도로 유지관리하는 것을 원하지 않습니다. 솔루션스 아키텍트는 온프레미스 리소스에 대한 액세스를 유지하면서 AWS 리소스에 대한 사용자 액세스를 관리해야 합니다.

이 요구 사항을 충족하려면 솔루션스 아키텍트는 무엇을 해야 합니까?

A. 회사의 각 사용자에 대해 IAM 사용자를 생성합니다. 각 사용자에 적절한 정책을 연결합니다.
B. Amazon Cognito를 Active Directory 사용자 풀과 함께 사용합니다. 적절한 정책이 연결된 역할을 생성합니다.
C. 적절한 정책이 연결된 교차 계정 역할을 정의합니다. 역할을 Active Directory 그룹에 매핑합니다.
D. SAML 2.0 기반 페더레이션을 구성합니다. 적절한 정책이 연결된 역할을 생성합니다. 역할을 Active Directory 그룹에 매핑합니다.

```
A company wants to provide users with access to AWS resources. The company has 1,500 users and manages their access to on-premises resources through Active Directory user groups on the corporate network. However, the company does not want users to have to maintain another identity to access the resources. A solutions architect must manage user access to the AWS resources while preserving access to the on-premises resources.  
  
What should the solutions architect do to meet these requirements?

- A. Create an IAM user for each user in the company. Attach the appropriate policies to each user.
- B. Use Amazon Cognito with an Active Directory user pool. Create roles with the appropriate policies attached.
- C. Define cross-account roles with the appropriate policies attached. Map the roles to the Active Directory groups.
- D. Configure Security Assertion Markup Language (SAML) 2 0-based federation. Create roles with the appropriate policies attached Map the roles to the Active Directory groups.
```

정답 : `D`

- SAML 2.0 연동(예: AD FS, Azure AD 등 idP)으로 사내 AD 자격 증명으로 AWS에 SSO를 제공할 수 있음
- AD 그룹을 IAM 역할에 매핑하면, 별도 계정 생성/관리 없이 기존 그룹 기반 권한 모델을 그대로 활용해 온프레미스 접근은 유지하면서 AWS 접근도 중앙에서 관리 가능

오답 이유

- A. 1,500명의 사용자별 IAM 사용자 생성은 계정/암호/키 관리 오버헤드가 매우 큽니다. 또한 “별도 ID 관리 불원” 요구에 어긋납니다.

- B. Amazon Cognito는 주로 애플리케이션 엔드유저 인증에 적합하며, 기업 내부의 직원 SSO로 AWS 콘솔/리소스 접근 권한 관리를 표준적으로 대체하지 않습니다. AD와의 직접적 역할 매핑 운영 모델에도 맞지 않습니다.

- C. 교차 계정 역할은 계정 간 권한 위임 용도입니다. AD 그룹과 직접 매핑하려면 결국 SAML 등 페더레이션이 필요합니다. 단독으로는 인증(SSO) 문제를 해결하지 못합니다.


## #625
한 회사가 여러 개의 Application Load Balancer(ALB) 뒤에서 웹사이트를 호스팅하고 있습니다.  
이 회사는 전 세계적으로 서로 다른 지역에 대해 서로 다른 콘텐츠 배포 권한(distribution rights)을 가지고 있습니다.  
솔루션스 아키텍트는 사용자가 배포 권한을 위반하지 않도록, 올바른 콘텐츠를 제공받도록 구성해야 합니다.

이 요구 사항을 충족하기 위해 솔루션스 아키텍트는 어떤 구성을 선택해야 합니까?

A. Amazon CloudFront를 AWS WAF와 함께 구성합니다.  
B. Application Load Balancer를 AWS WAF와 함께 구성합니다.  
C. Amazon Route 53에서 지리적 위치(geolocation) 정책을 구성합니다.  
D. Amazon Route 53에서 지리적 근접(geoproximity) 라우팅 정책을 구성합니다.

```
A company is hosting a website behind multiple Application Load Balancers. The company has different distribution rights for its content around the world. A solutions architect needs to ensure that users are served the correct content without violating distribution rights.  
  
Which configuration should the solutions architect choose to meet these requirements?

- A. Configure Amazon CloudFront with AWS WAF.
- B. Configure Application Load Balancers with AWS WAF
- C. Configure Amazon Route 53 with a geolocation policy
- D. Configure Amazon Route 53 with a geoproximity routing policy
```

정답 : `C`

- Route 53의 지리적 위치 라우팅 정책은 사용자의 요청 위치(국가/대륙/IP 기반 지역)에 따라 DNS 수준에서 특정 ALB나 콘텐츠 엔드포인트로 트래픽을 분배
- 이를 통해 사용자가 자신의 지역에 허용된 콘텐츠만 접근하도록 보장하므로, 콘텐츠 배포 권한을 준수하는데 적합

오답 이유

- A. CloudFront는 콘텐츠 전송 최적화 및 캐싱에는 적합하지만, 지역별 배포 제한(법적 라이선스 기반)을 직접 제어하기 위해선 Route 53 지리 라우팅이 더 단순하고 명시적입니다.  
     또한 ALB가 여러 개인 경우, CloudFront 단일 배포 구성으로 세밀한 지역별 라우팅 제어가 어렵습니다.

- B. ALB + WAF는 애플리케이션 계층 보안(예: SQL injection, XSS) 제공 목적이지, 지역별 콘텐츠 제공이나 지리적 제어 기능을 제공하지 않습니다.

- D. Geoproximity 라우팅 정책은 "지리적 거리 기반 트래픽 분배"를 수행하며, 콘텐츠 배포 권한(법적 제한)처럼 "국가별 지정"에는 부적합합니다.  
     예: “가까운 리전으로 보내기”는 가능하지만 “특정 국가만 허용”은 불가능합니다.


## #626
한 회사는 데이터를 온프레미스에 저장하고 있습니다. 데이터의 양이 회사의 가용 용량을 초과하여 계속 증가하고 있습니다.

회사는 온프레미스 위치의 데이터를 Amazon S3 버킷으로 마이그레이션하려고 합니다.  
이때, 데이터 전송 후 자동으로 데이터 무결성을 검증해야 합니다.

이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. AWS Snowball Edge 디바이스를 주문합니다. Snowball Edge 디바이스를 구성하여 S3 버킷으로 온라인 데이터 전송을 수행합니다.  
B. 온프레미스에 AWS DataSync 에이전트를 배포합니다. DataSync 에이전트를 구성하여 S3 버킷으로 온라인 데이터 전송을 수행합니다.  
C. 온프레미스에 Amazon S3 File Gateway를 생성합니다. S3 File Gateway를 구성하여 S3 버킷으로 온라인 데이터 전송을 수행합니다.  
D. 온프레미스에 Amazon S3 Transfer Acceleration 가속기를 구성합니다. 가속기를 구성하여 S3 버킷으로 온라인 데이터 전송을 수행합니다.

```
A company stores its data on premises. The amount of data is growing beyond the company's available capacity.  
  
The company wants to migrate its data from the on-premises location to an Amazon S3 bucket. The company needs a solution that will automatically validate the integrity of the data after the transfer.  
  
Which solution will meet these requirements?

- A. Order an AWS Snowball Edge device. Configure the Snowball Edge device to perform the online data transfer to an S3 bucket
- B. Deploy an AWS DataSync agent on premises. Configure the DataSync agent to perform the online data transfer to an S3 bucket.
- C. Create an Amazon S3 File Gateway on premises Configure the S3 File Gateway to perform the online data transfer to an S3 bucket
- D. Configure an accelerator in Amazon S3 Transfer Acceleration on premises. Configure the accelerator to perform the online data transfer to an S3 bucket.
```

정답 : `B`

- AWS DataSync는 온프레미스와 AWS 간의 대규모 데이터 전송을 자동화하고 검증하는 관리형 서비스
- 전송 중 및 전송 후 데이터 무결성 검사를 자동으로 수행하며, 체크섬을 사용해 데이터 손상 여부를 검증
- 별도의 스크립트나 도구 없이 신뢰성 있고 자동화된 데이터 마이그레이션 가능

오답 이유

- A. Snowball Edge는 대규모 오프라인(또는 일시적 온라인) 데이터 마이그레이션에 적합하지만, 
     문제에서 요구하는 “자동 무결성 검증을 포함한 온라인 전송”에는 부적합합니다. 
     Snowball은 전송 후 AWS로 배송하여 업로드를 수행하므로 실시간 검증이 어렵습니다.

- C. S3 File Gateway는 자주 접근하는 파일을 캐싱하고 S3 객체로 비동기 업로드하는 스토리지 게이트웨이 솔루션으로,
     마이그레이션용 대규모 전송/검증 자동화 기능은 없습니다.

- D. S3 Transfer Acceleration은 전송 속도를 높이기 위한 기능으로, 데이터 무결성 검증은 수행하지 않습니다. 
     주로 글로벌 사용자 업로드 성능 향상에 사용됩니다.


## #627
한 회사가 두 개의 DNS 서버를 AWS로 마이그레이션하려고 합니다. 이 서버들은 총 약 200개의 존을 호스팅하고 있으며, 평균적으로 매일 100만 건의 요청을 받습니다. 회사는 두 서버 관리와 관련된 운영 오버헤드를 최소화하면서 가용성을 최대화하고자 합니다.

이 요구 사항을 충족하기 위해 솔루션스 아키텍트는 무엇을 권장해야 합니까?

A. Amazon Route 53 콘솔에서 200개의 새 호스티드 존을 생성합니다. 존 파일을 가져옵니다.
B. 하나의 대형 Amazon EC2 인스턴스를 시작합니다. 존 파일을 가져옵니다. 다운타임에 대해 회사에 알림을 보내도록 Amazon CloudWatch 알람과 알림을 구성합니다.
C. AWS Server Migration Service(AWS SMS)를 사용하여 서버를 AWS로 마이그레이션합니다. 다운타임에 대해 회사에 알림을 보내도록 Amazon CloudWatch 알람과 알림을 구성합니다.
D. 두 개의 가용 영역에 걸친 Auto Scaling 그룹에서 Amazon EC2 인스턴스를 시작합니다. 존 파일을 가져옵니다. Auto Scaling 그룹의 원하는 용량을 1로, 최대 용량을 3으로 설정합니다. CPU 사용률을 기준으로 스케일링 알람을 구성합니다.

```
A company wants to migrate two DNS servers to AWS. The servers host a total of approximately 200 zones and receive 1 million requests each day on average. The company wants to maximize availability while minimizing the operational overhead that is related to the management of the two servers.  
  
What should a solutions architect recommend to meet these requirements?

- A. Create 200 new hosted zones in the Amazon Route 53 console Import zone files.
- B. Launch a single large Amazon EC2 instance Import zone tiles. Configure Amazon CloudWatch alarms and notifications to alert the company about any downtime.
- C. Migrate the servers to AWS by using AWS Server Migration Service (AWS SMS). Configure Amazon CloudWatch alarms and notifications to alert the company about any downtime.
- D. Launch an Amazon EC2 instance in an Auto Scaling group across two Availability Zones. Import zone files. Set the desired capacity to 1 and the maximum capacity to 3 for the Auto Scaling group. Configure scaling alarms to scale based on CPU utilization.
```

정답 : `A`

- DNS는 관리형 서비스인 Amazon Route 53으로 이전하는 것이 가용성을 극대화하고 운영 오버헤드를 최소화하는 최선의 선택
- Route 53은 전 세계 분산 Anycast 네트워크로 고가용성･고확장성을 제공하며, 존 파일 import로 수백 개의 존을 쉽게 이전 가능
- 100만 QPS/일 규모의 트래픽을 별도 운영 없이 처리 가능

오답 이유

- B. EC2에서 자체 DNS를 운영하면 패치/백업/모니터링/장애 조치 등 운영 부담이 큽니다. 단일 인스턴스는 단일 장애점(SPOF) 위험도 있습니다.

- C. AWS SMS는 VM 마이그레이션용으로 DNS 애플리케이션을 그대로 옮겨도 운영 오버헤드와 가용성 문제는 해소되지 않습니다.

- D. Auto Scaling으로 일부 가용성은 개선되지만, 여전히 OS/소프트웨어 관리 책임이 큽니다. DNS는 상태/구성 동기화, failover 검증 등 운영 복잡도가 큽니다.


## #628
전 세계적으로 운영되는 한 회사가 AWS Organizations의 여러 AWS 계정에서 애플리케이션을 실행하고 있습니다. 회사의 애플리케이션은 여러 리전의 여러 Amazon S3 버킷에 데이터를 업로드하기 위해 멀티파트 업로드를 사용합니다. 회사는 비용 준수 목적을 위해 완료되지 않은 멀티파트 업로드에 대한 보고가 필요합니다.

다음 중 운영 오버헤드가 가장 낮으면서 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. AWS Config를 구성하여 완료되지 않은 멀티파트 업로드 객체 개수를 보고하는 규칙을 설정합니다.
B. 서비스 제어 정책(SCP)을 생성하여 완료되지 않은 멀티파트 업로드 객체 개수를 보고합니다.
C. S3 Storage Lens를 구성하여 완료되지 않은 멀티파트 업로드 객체 개수를 보고합니다.
D. S3 다중 리전 액세스 포인트(S3 Multi-Region Access Point)를 생성하여 완료되지 않은 멀티파트 업로드 객체 개수를 보고합니다.

```
A global company runs its applications in multiple AWS accounts in AWS Organizations. The company's applications use multipart uploads to upload data to multiple Amazon S3 buckets across AWS Regions. The company wants to report on incomplete multipart uploads for cost compliance purposes.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Configure AWS Config with a rule to report the incomplete multipart upload object count.
- B. Create a service control policy (SCP) to report the incomplete multipart upload object count.
- C. Configure S3 Storage Lens to report the incomplete multipart upload object count.
- D. Create an S3 Multi-Region Access Point to report the incomplete multipart upload object count.
```

정답 : `C`

- S3 Storage Lens는 조직 전 계정･전 리전･전 버킷에 대한 스토리지 사용/활동 메트릭을 중앙 대시보드와 보고서로 제공
- Incomplete Multipart Upload 객체 수/바이트 등 비용에 직결되는 항목을 기본 메트릭으로 집계
- 별도 에이전트나 규칙 코드 없이 설정만으로 수집되므로 운영 오버헤드가 가장 낮음

오답 이유

- A. AWS Config에는 S3의 “미완료 멀티파트 업로드 개수”를 직접 집계·보고하는 관리형 규칙이 없습니다. 커스텀 구현 시 오버헤드가 큽니다.

- B. SCP는 계정의 허용/거부 권한을 제어하는 정책일 뿐, 메트릭을 “보고”하는 기능은 제공하지 않습니다.

- D. S3 Multi-Region Access Point는 다중 리전 버킷에 대한 단일 글로벌 엔드포인트 제공/라우팅 최적화를 위한 기능으로, 미완료 멀티파트 업로드 집계/보고 기능과 무관합니다.



## #629
한 회사가 Amazon RDS for MySQL에서 프로덕션 데이터베이스를 운영하고 있습니다. 회사는 보안 규정 준수를 이유로 데이터베이스 버전을 업그레이드하고자 합니다. 데이터베이스에는 중요한 데이터가 포함되어 있으므로, 데이터를 잃지 않고 기능을 업그레이드 및 테스트할 수 있는 신속한 솔루션을 원합니다.

다음 중 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. RDS 수동 스냅샷을 생성합니다. Amazon RDS for MySQL의 새 버전으로 업그레이드합니다.
B. 네이티브 백업 및 복원을 사용합니다. 업그레이드된 새 버전의 Amazon RDS for MySQL로 데이터를 복원합니다.
C. AWS Database Migration Service(AWS DMS)를 사용하여 업그레이드된 새 버전의 Amazon RDS for MySQL로 데이터를 복제합니다.
D. Amazon RDS Blue/Green Deployments를 사용하여 프로덕션 변경을 배포하고 테스트합니다.

```
A company runs a production database on Amazon RDS for MySQL. The company wants to upgrade the database version for security compliance reasons. Because the database contains critical data, the company wants a quick solution to upgrade and test functionality without losing any data.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an RDS manual snapshot. Upgrade to the new version of Amazon RDS for MySQL.
- B. Use native backup and restore. Restore the data to the upgraded new version of Amazon RDS for MySQL.
- C. Use AWS Database Migration Service (AWS DMS) to replicate the data to the upgraded new version of Amazon RDS for MySQL.
- D. Use Amazon RDS Blue/Green Deployments to deploy and test production changes.
```

정답 : `D`

- RDS Blue/Green Deployments는 프로덕션 데이터베이스(Blue)를 기준으로 동기화되는 사본(Green)을 자동 생성해 엔진 버전 업그레이드, 파라미터 변경, 스키마 변경 등을 사전에 테스트 가능
- 변경 검증 후에는 한 번의 스위치오버로 짧은 다운타임에 무중단에 가깝게 전환
- 데이터 동기화가 지속되므로 데이터 손실 위험이 최소화되고 운영 오버헤드가 가장 낮음

오답 이유

- A. 수동 스냅샷 후 인플레이스 업그레이드는 롤백/검증 절차가 번거롭고 다운타임이 길어질 수 있습니다. 테스트 환경 분리가 어렵습니다.

- B. 네이티브 백업/복원은 수작업과 절차가 많아 운영 오버헤드가 큽니다. 대규모 데이터에서 복원 시간과 검증 부담이 큽니다.

- C. AWS DMS는 이기종/동기화 마이그레이션에 유용하지만, 구성·모니터링·커튼오버 절차가 추가되어 상대적으로 운영 부담이 큽니다. 버전 업그레이드 전용의 관리형 절차는 Blue/Green이 더 적합합니다.


## #630
솔루션스 아키텍트가 하루에 한 번 실행되며 완료까지 최대 2시간이 걸리는 데이터 처리 작업을 만들고 있습니다. 작업이 중단되면 처음부터 다시 시작해야 합니다.

가장 비용 효율적인 방식으로 이 문제를 어떻게 해결해야 합니까?

A. 크론 잡에 의해 트리거되는 로컬 스크립트를 Amazon EC2 예약 인스턴스에서 실행합니다.
B. Amazon EventBridge의 예약 이벤트로 트리거되는 AWS Lambda 함수를 생성합니다.
C. Amazon EventBridge의 예약 이벤트로 트리거되는 Amazon Elastic Container Service(Amazon ECS) Fargate 태스크를 사용합니다.
D. Amazon EventBridge의 예약 이벤트로 트리거되는 Amazon EC2에서 실행되는 Amazon Elastic Container Service(Amazon ECS) 태스크를 사용합니다.

```
A solutions architect is creating a data processing job that runs once daily and can take up to 2 hours to complete. If the job is interrupted, it has to restart from the beginning.  
  
How should the solutions architect address this issue in the MOST cost-effective manner?

- A. Create a script that runs locally on an Amazon EC2 Reserved Instance that is triggered by a cron job.
- B. Create an AWS Lambda function triggered by an Amazon EventBridge scheduled event.
- C. Use an Amazon Elastic Container Service (Amazon ECS) Fargate task triggered by an Amazon EventBridge scheduled event.
- D. Use an Amazon Elastic Container Service (Amazon ECS) task running on Amazon EC2 triggered by an Amazon EventBridge scheduled event.
```

정답 : `C`

- ECS Fargate는 서버리스 컨테이너 실행으로, 필요한 시간(하루 2시간) 동안만 비용이 발생하고, 인스턴스 관리가 없어 운영 오버헤드와 비용 최소화
- EventBridge 예약 규칙으로 매일 실행을 보장할 수 있으며, 온디맨드 Fargate는 스팟 중단 위험이 없어 작업이 중간에 끊겨 재시작해야하는 리스크를 낮춤

오답 이유

- A. 예약 인스턴스는 24/7 요금 약정이 필요하여 하루 2시간 작업에는 비경제적입니다. OS/패치 등 관리 오버헤드도 큽니다.

- B. Lambda의 최대 실행 시간은 15분으로, 최대 2시간이 필요한 작업을 실행할 수 없습니다.

- D. ECS on EC2는 작업 외 시간에 인스턴스를 0으로 유지하기가 까다롭고(ASG/스케일링 설계 필요), 인스턴스 비용/운영 오버헤드가 발생합니다.



## #631
한 소셜 미디어 회사가 사용자 프로필, 관계, 상호작용으로 구성된 데이터베이스를 AWS 클라우드에 저장하고자 합니다. 애플리케이션은 데이터베이스의 모든 변경 사항을 모니터링해야 합니다. 또한 데이터 엔터티 간의 관계를 분석하고 사용자에게 추천을 제공해야 합니다.

다음 중 운영 오버헤드가 가장 낮은 솔루션은 무엇입니까?

A. 정보를 저장하기 위해 Amazon Neptune을 사용합니다. 데이터베이스의 변경 사항을 처리하기 위해 Amazon Kinesis Data Streams를 사용합니다.
B. 정보를 저장하기 위해 Amazon Neptune을 사용합니다. 데이터베이스의 변경 사항을 처리하기 위해 Neptune Streams를 사용합니다.
C. 정보를 저장하기 위해 Amazon Quantum Ledger Database(Amazon QLDB)를 사용합니다. 데이터베이스의 변경 사항을 처리하기 위해 Amazon Kinesis Data Streams를 사용합니다.
D. 정보를 저장하기 위해 Amazon Quantum Ledger Database(Amazon QLDB)를 사용합니다. 데이터베이스의 변경 사항을 처리하기 위해 Neptune Streams를 사용합니다.

```
A social media company wants to store its database of user profiles, relationships, and interactions in the AWS Cloud. The company needs an application to monitor any changes in the database. The application needs to analyze the relationships between the data entities and to provide recommendations to users.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Use Amazon Neptune to store the information. Use Amazon Kinesis Data Streams to process changes in the database.
- B. Use Amazon Neptune to store the information. Use Neptune Streams to process changes in the database.
- C. Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Amazon Kinesis Data Streams to process changes in the database.
- D. Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Neptune Streams to process changes in the database.
```

정답 : `B`

- Neptune은 그래프 모델(예: Property Graph/Gremlin, RDF/SPARQL)에 최적화되어 소셜 그래프(프로필･관계･상호작용)를 저장･탐색하고 추천/연결 분석을 수행하기에 적합
- Neptune Streams는 그래프 변경 이벤트를 시간 순으로 스트리밍해 애플리케이션이 직접 소비하도록 하므로, 별도 파이프라인 운영 없이 변경 모니터링을 최소 이벤트로 구현 가능

오답 이유

- A. Neptune + Kinesis Data Streams도 가능하지만, 외부 스트림 리소스(스트림 생성/처리기/확장/모니터링)를 운영해야 하므로 Neptune Streams를 직접 사용하는 것보다 운영 오버헤드가 큽니다.

- C. QLDB는 변경 불가(append-only) 원장 데이터베이스로, 관계 그래프 탐색과 추천 같은 그래프 질의에 적합하지 않습니다.

- D. QLDB에 Neptune Streams를 붙일 수 없습니다. Neptune Streams는 Neptune 전용 기능이며, 그래프 변경 스트림을 제공합니다.


## #632
한 회사가 대량의 데이터를 저장할 새로운 애플리케이션을 만들고 있습니다. 이 데이터는 매시간 분석될 것이며, 여러 가용 영역에 배포된 여러 Amazon EC2 Linux 인스턴스에 의해 수정될 것입니다. 필요한 저장 용량은 향후 6개월 동안 계속 증가할 것입니다.

이 요구 사항을 충족하기 위해 솔루션스 아키텍트는 어떤 스토리지 솔루션을 권장해야 합니까?

A. 데이터를 Amazon S3 Glacier에 저장합니다. 애플리케이션 인스턴스가 액세스할 수 있도록 S3 Glacier 볼트 정책을 업데이트합니다.
B. 데이터를 Amazon Elastic Block Store(Amazon EBS) 볼륨에 저장합니다. EBS 볼륨을 애플리케이션 인스턴스에 마운트합니다.
C. 데이터를 Amazon Elastic File System(Amazon EFS) 파일 시스템에 저장합니다. 파일 시스템을 애플리케이션 인스턴스에 마운트합니다.
D. 데이터를 애플리케이션 인스턴스 간에 공유되는 Amazon Elastic Block Store(Amazon EBS) 프로비저닝된 IOPS 볼륨에 저장합니다.

```
A company is creating a new application that will store a large amount of data. The data will be analyzed hourly and will be modified by several Amazon EC2 Linux instances that are deployed across multiple Availability Zones. The needed amount of storage space will continue to grow for the next 6 months.  
  
Which storage solution should a solutions architect recommend to meet these requirements?

- A. Store the data in Amazon S3 Glacier. Update the S3 Glacier vault policy to allow access to the application instances.
- B. Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on the application instances.
- C. Store the data in an Amazon Elastic File System (Amazon EFS) file system. Mount the file system on the application instances.
- D. Store the data in an Amazon Elastic Block Store (Amazon EBS) Provisioned IOPS volume shared between the application instances.
```

정답 : `C`

- EFS는 다수의 EC2 인스턴스가 동시에 접근할 수 있는 공유 POSIX 파일 시스템
- 여러 AZ에 걸쳐 탄력적으로 확장. 용량을 사전에 프로비저닝할 필요 없이 데이터가 증가함에 따라 자동 확장되므로
- 매시간 분석/다중 인스턴스 수정/6개월간 용량 증가를 모두 만족

오답 이유

- A. S3 Glacier는 아카이브용으로 복구 지연(분~시간)이 있어 매시간 분석·수정 워크로드에 부적합합니다. 인스턴스에 직접 마운트할 수도 없습니다.

- B. EBS 볼륨은 단일 인스턴스(또는 제한적 멀티어태치) 및 단일 AZ 범위에 종속됩니다. 여러 AZ의 여러 인스턴스에서 동시에 공유 접근하는 용도로 적합하지 않습니다.

- D. EBS Provisioned IOPS라도 멀티어태치(io1/io2)는 **같은 AZ** 내에서만 가능하며, 클러스터 인식 파일 시스템이 필요합니다. 문제의 “여러 AZ” 요구와 일반적 파일 공유 시나리오에 맞지 않습니다.



## #633
한 회사가 Amazon RDS for PostgreSQL Multi-AZ DB 인스턴스에 데이터를 저장하는 애플리케이션을 관리하고 있습니다. 트래픽 증가로 인해 성능 문제가 발생하고 있습니다. 회사는 데이터베이스 쿼리가 성능 저하의 주요 원인임을 확인했습니다.

솔루션스 아키텍트는 애플리케이션의 성능을 향상시키기 위해 무엇을 해야 합니까?

A. Multi-AZ 대기(standby) 복제본에서 읽기 트래픽을 처리합니다.  
B. DB 인스턴스가 Transfer Acceleration을 사용하도록 구성합니다.  
C. 소스 DB 인스턴스에서 읽기 복제본(read replica)을 생성하고, 읽기 트래픽을 읽기 복제본으로 처리합니다.  
D. 데이터베이스 요청의 동시성을 높이기 위해 애플리케이션과 Amazon RDS 사이에 Amazon Kinesis Data Firehose를 사용합니다.

```
A company manages an application that stores data on an Amazon RDS for PostgreSQL Multi-AZ DB instance. Increases in traffic are causing performance problems. The company determines that database queries are the primary reason for the slow performance.  
  
What should a solutions architect do to improve the application's performance?

- A. Serve read traffic from the Multi-AZ standby replica.
- B. Configure the DB instance to use Transfer Acceleration.
- C. Create a read replica from the source DB instance. Serve read traffic from the read replica.
- D. Use Amazon Kinesis Data Firehose between the application and Amazon RDS to increase the concurrency of database requests.
```

정답 : `C`

- Multi-AZ 구성은 고가용성 목적이며, 읽기 부하 분산 용도로는 사용할 수 없음
- 읽기 쿼리가 많아 성능 저하가 발생하는 경우, RDS 읽기 복제본을 생성하여 읽기 전용 트래픽을 분산시키는 것이 가장 효과적

오답 이유

- A. Multi-AZ의 standby 인스턴스는 장애 조치(Failover) 시에만 사용되며, 평상시 읽기 트래픽을 처리하지 않습니다. 읽기 성능 향상에는 도움이 되지 않습니다.

- B. Transfer Acceleration은 S3 업로드/다운로드 성능 향상 기능이며, RDS와는 무관합니다.

- D. Kinesis Data Firehose는 스트리밍 데이터 파이프라인 서비스로, 데이터베이스 쿼리 부하를 줄이는 용도로 사용되지 않습니다. 오히려 지연이 증가할 수 있습니다.



## #634
한 회사는 다양한 기계로부터 매일 10GB의 텔레메트리 데이터를 수집합니다. 회사는 소스 데이터 계정의 Amazon S3 버킷에 데이터를 저장합니다.

회사는 여러 컨설팅 기관을 고용하여 이 데이터를 분석하게 했습니다. 각 기관은 자사 분석가들을 위해 데이터에 대한 읽기 권한이 필요합니다. 회사는 보안성과 운영 효율성을 극대화하는 솔루션을 선택하여 소스 데이터 계정에서 데이터를 공유해야 합니다.

다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 각 기관에 대해 데이터를 복제하도록 S3 글로벌 테이블을 구성합니다.
B. 제한된 시간 동안 S3 버킷을 퍼블릭으로 만듭니다. 해당 기관들에게만 알립니다.
C. 기관이 소유한 계정들에 대해 S3 버킷의 교차 계정 액세스를 구성합니다.
D. 소스 데이터 계정에 각 분석가용 IAM 사용자를 설정합니다. 각 사용자에게 S3 버킷 액세스를 부여합니다.

```
A company collects 10 GB of telemetry data daily from various machines. The company stores the data in an Amazon S3 bucket in a source data account.  
  
The company has hired several consulting agencies to use this data for analysis. Each agency needs read access to the data for its analysts. The company must share the data from the source data account by choosing a solution that maximizes security and operational efficiency.  
  
Which solution will meet these requirements?

- A. Configure S3 global tables to replicate data for each agency.
- B. Make the S3 bucket public for a limited time. Inform only the agencies.
- C. Configure cross-account access for the S3 bucket to the accounts that the agencies own.
- D. Set up an IAM user for each analyst in the source data account. Grant each user access to the S3 bucket.
```

정답 : `C`

- S3는 버킷 정책과 교차 계정 IAM 역할/정책을 통해 외부(타 계정)에서 읽기 전용으로 안전하게 접근을 허용할 수 있음
- 퍼블릭 공개 없이 필요 계정에만 최소 권한을 부여하며, 사용자 단위 관리 없이 기관 계정 단위로 제어할 수 있어 운영 오버헤드가 낮고 보안도 우수

오답 이유

- A. “S3 글로벌 테이블”은 존재하지 않습니다(해당 개념은 DynamoDB Global Tables). S3 데이터 공유/복제 요구와 무관합니다.

- B. 버킷을 퍼블릭으로 여는 것은 보안 위반 위험이 큽니다. IP/시간 제한을 해도 링크 유출, 오구성 위험이 있어 요구(보안 극대화)에 부정합합니다.

- D. 소스 계정에 외부 분석가별 IAM 사용자를 만드는 것은 계정 내 자격 증명 관리(사용자/암호/키) 오버헤드가 매우 크고 보안상 바람직하지 않습니다. 계정 경계 밖 주체에는 교차 계정 액세스가 모범 사례입니다.


## #635
한 회사는 기본(프라이머리) AWS 리전에서 Amazon FSx for NetApp ONTAP을 사용하여 CIFS(=SMB)와 NFS 파일 공유를 제공하고 있습니다. Amazon EC2 인스턴스에서 실행되는 애플리케이션이 이 파일 공유에 접근합니다. 회사는 보조(세컨더리) 리전에 스토리지 재해 복구(DR) 솔루션이 필요합니다. 보조 리전에 복제된 데이터도 기본 리전과 동일한 프로토콜로 접근할 수 있어야 합니다.

다음 중 운영 오버헤드가 가장 적으면서 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 데이터를 Amazon S3 버킷으로 복사하는 AWS Lambda 함수를 생성합니다. S3 버킷을 보조 리전으로 복제합니다.
B. AWS Backup을 사용하여 FSx for ONTAP 볼륨의 백업을 생성합니다. 볼륨을 보조 리전으로 복사합니다. 백업에서 새로운 FSx for ONTAP 인스턴스를 생성합니다.
C. 보조 리전에 FSx for ONTAP 인스턴스를 생성합니다. NetApp SnapMirror를 사용하여 기본 리전에서 보조 리전으로 데이터를 복제합니다.
D. Amazon Elastic File System(Amazon EFS) 볼륨을 생성합니다. 현재 데이터를 해당 볼륨으로 마이그레이션합니다. 볼륨을 보조 리전으로 복제합니다.

```
A company uses Amazon FSx for NetApp ONTAP in its primary AWS Region for CIFS and NFS file shares. Applications that run on Amazon EC2 instances access the file shares. The company needs a storage disaster recovery (DR) solution in a secondary Region. The data that is replicated in the secondary Region needs to be accessed by using the same protocols as the primary Region.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an AWS Lambda function to copy the data to an Amazon S3 bucket. Replicate the S3 bucket to the secondary Region.
- B. Create a backup of the FSx for ONTAP volumes by using AWS Backup. Copy the volumes to the secondary Region. Create a new FSx for ONTAP instance from the backup.
- C. Create an FSx for ONTAP instance in the secondary Region. Use NetApp SnapMirror to replicate data from the primary Region to the secondary Region.
- D. Create an Amazon Elastic File System (Amazon EFS) volume. Migrate the current data to the volume. Replicate the volume to the secondary Region.
```

정답 : `C`

- FSx for NetApp ONTAP간 NetApp SnapMirror는 관리형으로 손쉽게 리전 간 볼륨 복제를 제공
- 대상에서도 동일한 SMB/CIFS와 NFS 프로토콜로 마운트가 가능
- DR 시 대상 FSx for ONTAP에서 신속히 전환(볼륨 전환/승격)할 수 있어 운영 오버헤드와 RPO/RTO 모두 유리

오답 이유

- A. S3로 복제하면 객체 스토리지이므로 SMB/NFS로 직접 접근할 수 없습니다. 애플리케이션의 파일 공유 요구를 충족하지 못합니다.

- B. 백업/복사 후 복구(restore)가 필요하여 RTO가 길고, 전환 자동화도 추가 작업이 필요합니다. 지속 복제 기반 DR보다 운영 오버헤드가 큽니다.

- D. EFS는 NFS만 지원하며 SMB/CIFS를 지원하지 않습니다. 또한 현재 ONTAP 워크로드를 EFS로 재설계·마이그레이션해야 하므로 요구사항 및 최소 오버헤드 조건에 부합하지 않습니다.


## #636
개발 팀이 AWS Lambda 함수를 사용하는 이벤트 기반 애플리케이션을 만들고 있습니다. Amazon S3 버킷에 파일이 추가되면 이벤트가 생성됩니다. 개발 팀은 현재 Amazon S3에서 발생한 이벤트의 대상(target)으로 Amazon Simple Notification Service(Amazon SNS)를 구성해 두었습니다.

Amazon S3에서 온 이벤트를 확장 가능하게 처리하려면 솔루션스 아키텍트는 무엇을 해야 합니까?

A. SNS 구독을 생성하여 이벤트가 Lambda에서 실행되기 전에 Amazon Elastic Container Service(Amazon ECS)에서 이벤트를 처리합니다.
B. SNS 구독을 생성하여 이벤트가 Lambda에서 실행되기 전에 Amazon Elastic Kubernetes Service(Amazon EKS)에서 이벤트를 처리합니다.
C. SNS 구독을 생성하여 이벤트를 Amazon Simple Queue Service(Amazon SQS)로 보냅니다. SQS 큐가 Lambda 함수를 트리거하도록 구성합니다.
D. SNS 구독을 생성하여 이벤트를 AWS Server Migration Service(AWS SMS)로 보냅니다. Lambda 함수가 SMS 이벤트를 폴링하도록 구성합니다.

```
A development team is creating an event-based application that uses AWS Lambda functions. Events will be generated when files are added to an Amazon S3 bucket. The development team currently has Amazon Simple Notification Service (Amazon SNS) configured as the event target from Amazon S3.  
  
What should a solutions architect do to process the events from Amazon S3 in a scalable way?

- A. Create an SNS subscription that processes the event in Amazon Elastic Container Service (Amazon ECS) before the event runs in Lambda.
- B. Create an SNS subscription that processes the event in Amazon Elastic Kubernetes Service (Amazon EKS) before the event runs in Lambda
- C. Create an SNS subscription that sends the event to Amazon Simple Queue Service (Amazon SQS). Configure the SOS queue to trigger a Lambda function.
- D. Create an SNS subscription that sends the event to AWS Server Migration Service (AWS SMS). Configure the Lambda function to poll from the SMS event.
```

정답 : `C`

- S3 → SNS → SQS → Lambda 패턴은 팬아웃과 버퍼링/재시도/후속 처리 제어(DLQ)를 제공해 스파이크 트래픽에도 확장 가능하고 내구성 있는 이벤트 처리를 구현
- SQS가 이벤트를 큐잉하고 Lambda가 자동으로 폴링･병렬 확장하므로 손실 없이 안정적으로 처리 가능

오답 이유

- A. ECS를 중간 처리로 넣으면 컨테이너 클러스터 관리, 오토스케일, 운영 복잡도가 증가합니다. 단순 이벤트 처리에 불필요합니다.

- B. EKS 역시 쿠버네티스 운영이 필요해 오버헤드가 큽니다. 서버리스 이벤트 처리에 비효율적입니다.

- D. AWS Server Migration Service는 서버 마이그레이션 서비스이며 이벤트 큐/버퍼로 사용되지 않습니다. 목적에 맞지 않습니다.


## #637
한 솔루션스 아키텍트가 Amazon API Gateway 뒤에 새로운 서비스를 설계하고 있습니다. 이 서비스의 요청 패턴은 예측 불가능하며, 초당 0건에서 500건 이상으로 갑자기 변할 수 있습니다. 백엔드 데이터베이스에 영구 저장해야 하는 데이터의 총 크기는 현재 1GB 미만이며, 향후 성장도 예측하기 어렵습니다. 데이터는 간단한 키-값 요청으로 조회할 수 있습니다.

어떤 AWS 서비스 조합이 이러한 요구 사항을 충족합니까? (두 가지 선택)

A. AWS Fargate
B. AWS Lambda
C. Amazon DynamoDB
D. Amazon EC2 Auto Scaling
E. MySQL 호환 Amazon Aurora

```
A solutions architect is designing a new service behind Amazon API Gateway. The request patterns for the service will be unpredictable and can change suddenly from 0 requests to over 500 per second. The total size of the data that needs to be persisted in a backend database is currently less than 1 GB with unpredictable future growth. Data can be queried using simple key-value requests.  
  
Which combination ofAWS services would meet these requirements? (Choose two.)

- A. AWS Fargate
- B. AWS Lambda
- C. Amazon DynamoDB
- D. Amazon EC2 Auto Scaling
- E. MySQL-compatible Amazon Aurora
```

정답 : `B, C`

- 서버리스 조합인 API Gateway → Lambda → DynamoDB는 0에서 수백 RPS로 급변하는 트래픽을 자동으로 흡수
- 키-값 질의에 최적화된 DynamoDB가 밀리초 지연으로 확장성 있게 응답
- 데이터 크기가 현재 1GB 미만이고 향후 성장도 유연히 대응 가능해 운영 오버헤드와 비용을 최소화

오답 이유

- A. Fargate는 컨테이너 운영에 적합하지만, 본 요구(간단한 키-값 + 급변 트래픽)에 서버리스 Lambda가 더 단순하고 비용 효율적입니다.

- D. EC2 Auto Scaling은 서버 프로비저닝/용량 계획/패치 관리 등 운영 부담이 큽니다. 0→500 RPS 급변 시에도 서버리스가 더 적합합니다.

- E. Aurora(MySQL 호환)는 관계형에 적합하며 키-값 단순 조회/급격한 스파이크 대응에 DynamoDB 대비 비용·운영 효율이 떨어집니다.


## #638
한 회사가 전 세계의 직원들과 연구 데이터를 수집·공유합니다. 회사는 데이터를 Amazon S3 버킷에 수집·저장하고 AWS 클라우드에서 처리하려고 합니다. 또한 이 데이터를 회사의 직원들과 공유하려고 합니다. 회사는 운영 오버헤드를 최소화하면서 보안적인 AWS 클라우드 기반 솔루션이 필요합니다.

다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. AWS Lambda 함수를 사용해 S3 프리사인드 URL을 생성합니다. 직원들에게 이 URL을 사용하도록 안내합니다.
B. 각 직원에 대해 IAM 사용자를 생성합니다. 각 직원에게 S3 액세스를 허용하는 IAM 정책을 생성합니다. 직원들에게 AWS Management Console을 사용하도록 안내합니다.
C. S3 File Gateway를 생성합니다. 업로드용 공유와 다운로드용 공유를 만듭니다. 직원들이 로컬 컴퓨터에 공유를 마운트하여 S3 File Gateway를 사용하도록 허용합니다.
D. AWS Transfer Family SFTP 엔드포인트를 구성합니다. 사용자 지정 ID 공급자 옵션을 선택합니다. 사용자 자격 증명 관리를 위해 AWS Secrets Manager를 사용합니다. 직원들에게 Transfer Family를 사용하도록 안내합니다.

```
A company collects and shares research data with the company's employees all over the world. The company wants to collect and store the data in an Amazon S3 bucket and process the data in the AWS Cloud. The company will share the data with the company's employees. The company needs a secure solution in the AWS Cloud that minimizes operational overhead.  
  
Which solution will meet these requirements?

- A. Use an AWS Lambda function to create an S3 presigned URL. Instruct employees to use the URL.
- B. Create an IAM user for each employee. Create an IAM policy for each employee to allow S3 access. Instruct employees to use the AWS Management Console.
- C. Create an S3 File Gateway. Create a share for uploading and a share for downloading. Allow employees to mount shares on their local computers to use S3 File Gateway.
- D. Configure AWS Transfer Family SFTP endpoints. Select the custom identity provider options. Use AWS Secrets Manager to manage the user credentials Instruct employees to use Transfer Family.
```

정답 : `A`

- S3 프리사인드 URL은 객체(업로드/다운로드)에 대해 시간 제한･권한 제한이 가능한 임시 보안 액세스를 제공
- 대규모 사용자에 대한 계정･자격 증명 관리 없이 안전하게 데이터를 공유/수집 가능
- 프런트엔드나 간단한 API에서 Lambda로 URL을 생성하면 운영 오버헤드가 매우 낮음

오답 이유

- B. 직원 수만큼 IAM 사용자/자격 증명을 생성·회전·폐기해야 하므로 계정 관리 오버헤드와 보안 위험(장기 자격 증명)이 큽니다.

- C. S3 File Gateway는 사이트/애플라이언스 단위 배치와 지속 운영이 필요합니다. 전 세계 개인 단말(직원)별로 마운트시키는 것은 운영 복잡도가 큽니다.

- D. Transfer Family(SFTP)는 사용자 계정/암호(또는 키) 관리를 요구하고, 대규모 직원에게는 Secrets Manager/IdP 연동 운영 부담이 큽니다. 단순 공유·수집에는 프리사인드 URL이 더 간단하고 경제적입니다.


## #639
한 회사가 새로운 가구 재고 관리 애플리케이션을 구축하고 있습니다. 회사는 이 애플리케이션을 여러 가용 영역(Availability Zone)에 걸쳐 배포된 Amazon EC2 인스턴스 플릿에 배포했습니다. EC2 인스턴스들은 VPC 내에서 Application Load Balancer(ALB) 뒤에 있습니다.

솔루션스 아키텍트는 들어오는 트래픽이 한 EC2 인스턴스로 편향되어 일부 요청에서 지연(latency)이 발생하는 것을 관찰했습니다.

이 문제를 해결하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?

A. ALB에서 세션 어피니티(스티키 세션)를 비활성화합니다.  
B. ALB를 Network Load Balancer로 교체합니다.  
C. 각 가용 영역의 EC2 인스턴스 수를 늘립니다.  
D. ALB의 대상 그룹에 대한 상태 검사(health check) 빈도를 조정합니다.

```
A company is building a new furniture inventory application. The company has deployed the application on a fleet ofAmazon EC2 instances across multiple Availability Zones. The EC2 instances run behind an Application Load Balancer (ALB) in their VPC.  
  
A solutions architect has observed that incoming traffic seems to favor one EC2 instance, resulting in latency for some requests.  
  
What should the solutions architect do to resolve this issue?

- A. Disable session affinity (sticky sessions) on the ALB
- B. Replace the ALB with a Network Load Balancer
- C. Increase the number of EC2 instances in each Availability Zone
- D. Adjust the frequency of the health checks on the ALB's target group
```

정답 : `A`

- ALB에서 세션 어피니티(Sticky Session)가 활성화되어 있으면, 동일한 클라이언트의 요청이 항상 같은 EC2 인스턴스로 전달되므로 특정 인스턴스에 트래픽이 몰릴 수 있음
- 세션 어피니티를 비활성화하면 ALB가 요청을 라운드 로빈으로 고르게 분산시켜 부하 불균형 문제를 해소

오답 이유

- B. Network Load Balancer는 TCP/UDP 레벨의 고성능 트래픽 처리용이며 HTTP 계층 로드 밸런싱 기능(예: 라운드 로빈, 경로 기반 라우팅 등)이 없습니다. 이 문제의 원인과는 관련이 없습니다.

- C. 인스턴스 수를 늘려도 세션 어피니티가 활성화되어 있으면 트래픽은 여전히 한 인스턴스로 몰릴 수 있습니다. 근본적인 원인 해결이 아닙니다.

- D. 상태 검사 빈도를 조정해도 트래픽 분산에는 영향을 주지 않습니다. 이는 인스턴스의 가용성 확인 목적이지 로드 밸런싱 알고리즘과 무관합니다.


## #640
한 회사의 애플리케이션 워크플로는 AWS Lambda 함수를 사용하여 Amazon S3에서 파일을 다운로드하고 복호화(decrypt)합니다. 이 파일들은 AWS Key Management Service(AWS KMS) 키로 암호화되어 있습니다.  
솔루션스 아키텍트는 필요한 권한이 올바르게 설정되도록 보장하는 솔루션을 설계해야 합니다.

다음 중 어떤 조합의 작업이 이를 달성합니까? (두 가지 선택)

A. kms:decrypt 권한을 Lambda 함수의 리소스 정책(resource policy)에 추가합니다.  
B. KMS 키의 정책에서 Lambda IAM 역할에 대해 decrypt 권한을 부여합니다.  
C. KMS 키의 정책에서 Lambda 리소스 정책에 대해 decrypt 권한을 부여합니다.  
D. kms:decrypt 권한을 가진 새 IAM 정책을 생성하고 이를 Lambda 함수에 연결합니다.  
E. kms:decrypt 권한을 가진 새 IAM 역할을 생성하고 이 실행 역할을 Lambda 함수에 연결합니다.

```
A company has an application workflow that uses an AWS Lambda function to download and decrypt files from Amazon S3. These files are encrypted using AWS Key Management Service (AWS KMS) keys. A solutions architect needs to design a solution that will ensure the required permissions are set correctly.  
  
Which combination of actions accomplish this? (Choose two.)

- A. Attach the kms:decrypt permission to the Lambda function’s resource policy
- B. Grant the decrypt permission for the Lambda IAM role in the KMS key's policy
- C. Grant the decrypt permission for the Lambda resource policy in the KMS key's policy.
- D. Create a new IAM policy with the kms:decrypt permission and attach the policy to the Lambda function.
- E. Create a new IAM role with the kms:decrypt permission and attach the execution role to the Lambda function.
```

정답 : `B, E`

- AWS KMS로 암호화된 데이터를 복호화하려면, Lambda 함수의 실행 역할이 kms:Decrypt 권한을 가져야 하며, 해당 역할이 KMS 키 정책에서 명시적으로 허용되어야 함

오답 이유

- A. Lambda 함수 자체에는 리소스 정책(resource policy)이 없습니다. (Lambda는 리소스 정책을 통해 외부 서비스 접근을 허용할 수 있지만, KMS 복호화와는 무관합니다.)

- C. KMS 키 정책은 IAM 주체(principal, 즉 IAM 역할/사용자)에 권한을 부여할 수는 있지만 Lambda 리소스 정책에 직접 권한을 부여할 수는 없습니다.

- D. Lambda 함수에 직접 IAM 정책을 붙일 수는 없습니다. Lambda는 실행 역할(Execution Role)에 권한을 부여해야 하며, 함수 자체에 정책을 부착하는 개념은 없습니다.