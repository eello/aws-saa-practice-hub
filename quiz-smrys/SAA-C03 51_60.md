---
created: 2025-09-25 16:03:59
last_modified: 2025-09-25 17:08:09
---
## #51
한 회사는 REST API를 통해 조회할 수 있는 주문 배송 통계를 제공하는 애플리케이션을 개발하고 있습니다.  
회사는 배송 통계를 추출하고, 데이터를 읽기 쉬운 HTML 형식으로 정리하며, 매일 아침 같은 시간에 여러 이메일 주소로 보고서를 전송하고 싶습니다.

이 요구사항을 충족하기 위해 솔루션 설계자가 수행해야 하는 조합은 무엇입니까? (2개 선택)

A. 애플리케이션이 데이터를 Amazon Kinesis Data Firehose로 전송하도록 구성합니다. 

B. Amazon Simple Email Service(Amazon SES)를 사용하여 데이터를 형식화하고 이메일로 보고서를 전송합니다.  

C. Amazon EventBridge(CloudWatch Events) 예약 이벤트를 생성하여 AWS Glue 작업을 호출하고 애플리케이션의 API에서 데이터를 쿼리하도록 합니다.  

D. Amazon EventBridge(CloudWatch Events) 예약 이벤트를 생성하여 AWS Lambda 함수를 호출하고 애플리케이션의 API에서 데이터를 쿼리하도록 합니다.  

E. 애플리케이션 데이터를 Amazon S3에 저장합니다. S3 이벤트 대상으로 Amazon Simple Notification Service(Amazon SNS) 주제를 생성하여 이메일로 보고서를 전송합니다.

```
A company is developing an application that provides order shipping statistics for retrieval by a REST API. The company wants to extract the shipping statistics, organize the data into an easy-to-read HTML format, and send the report to several email addresses at the same time every morning.  
Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Configure the application to send the data to Amazon Kinesis Data Firehose.
- B. Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.
- C. Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue job to query the application's API for the data.
- D. Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data.
- E. Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS) topic as an S3 event destination to send the report by email.
```

정답 : `B, D`

- 요구사항
	- 매일 아침 같은 시간에 보고서 전송 -> 예약 이벤트 필요
	- API에서 데이터를 조회하고 HTML 형식으로 정리 -> 데이터 처리 필요
	- 여러 이메일 주소로 전송 -> 이메일 발송 서비스 필요
- EventBridge 예약 이벤트
	- 매일 같은 시간에 람다 함수를 트리거
	- 람다가 애플리케이션 API에서 데이터 조회
- 람다 + SES
	- 조회한 데이터를 HTML 형식으로 변환
	- SES를 사용해 여러 이메일 주소로 전송

오답 이유

- **A. Kinesis Data Firehose — 오답**
    - Firehose는 스트리밍 데이터 수집 및 변환, S3/Redshift/Splunk 전달용
    - **정기 보고서 생성 및 이메일 전송 목적에는 불필요하고 과도한 솔루션**
    
- **C. Glue Job — 오답**
    - Glue는 ETL용으로 적합
    - 단순히 매일 API 호출 후 HTML 이메일 보고서를 보내는 목적에는 **운영 오버헤드가 크고 불필요**
    
- **E. S3 + SNS — 오답**
    - S3 이벤트는 객체 생성/수정 이벤트 기반
    - **정해진 시간에 이메일 전송** 요구에는 부적합
    - 이메일 발송 시 HTML 변환 처리 어려움

## #52
한 회사는 온프레미스 애플리케이션을 AWS로 마이그레이션하려고 합니다.  
애플리케이션은 수십 기가바이트에서 수백 테라바이트에 이르는 출력 파일을 생성합니다.  
애플리케이션 데이터는 표준 파일 시스템 구조로 저장되어야 합니다.  
회사는 **자동으로 확장 가능하고, 고가용성이며, 최소 운영 오버헤드**가 필요한 솔루션을 원합니다.

이 요구사항을 충족할 솔루션은 무엇입니까?

A. 애플리케이션을 Amazon Elastic Container Service(Amazon ECS)에서 컨테이너로 실행하도록 마이그레이션합니다. 저장소로 Amazon S3를 사용합니다.  
B. 애플리케이션을 Amazon Elastic Kubernetes Service(Amazon EKS)에서 컨테이너로 실행하도록 마이그레이션합니다. 저장소로 Amazon Elastic Block Store(Amazon EBS)를 사용합니다.  
C. 애플리케이션을 Multi-AZ Auto Scaling 그룹에 있는 Amazon EC2 인스턴스로 마이그레이션합니다. 저장소로 Amazon Elastic File System(Amazon EFS)을 사용합니다.  
D. 애플리케이션을 Multi-AZ Auto Scaling 그룹에 있는 Amazon EC2 인스턴스로 마이그레이션합니다. 저장소로 Amazon Elastic Block Store(Amazon EBS)를 사용합니다.

```
A company wants to migrate its on-premises application to AWS. The application produces output files that vary in size from tens of gigabytes to hundreds of terabytes. The application data must be stored in a standard file system structure. The company wants a solution that scales automatically. is highly available, and requires minimum operational overhead.  
Which solution will meet these requirements?

- A. Migrate the application to run as containers on Amazon Elastic Container Service (Amazon ECS). Use Amazon S3 for storage.
- B. Migrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon Elastic Block Store (Amazon EBS) for storage.
- C. Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.
- D. Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic Block Store (Amazon EBS) for storage.
```

정답 : `C`

- 요구사항
	- 표준 파일 시스템 구조 필요 -> POSIX 호환 파일 시스템
	- 자동 확장, 고가용성, 최소 운영 오버헤드
	- 수십 GB ~ 수백 TB 파일 처리 가능
- Amazon EFS
	- POSIX 호환 파일 시스템
	- 자동 확장(Auto Scaling)
	- 멀티 AZ 고가용성
	- 수많은 EC2 인스턴스에서 동시에 접근 가능

오답 이유

- **A. ECS + S3 — 오답**
    - S3는 객체 스토리지 → **표준 파일 시스템 구조(POSIX) 지원 불가**
    - 대용량 파일 처리 가능하지만, 애플리케이션이 POSIX API 사용 시 코드 수정 필요
    
- **B. EKS + EBS — 오답**
    - EBS는 **단일 AZ에 종속** → Multi-AZ 고가용성 요구 불충족
    - 자동 확장에 따라 스케일 아웃 시 새로운 인스턴스에서 EBS 공유 불가 → 추가 운영 필요
    
- **D. EC2 + EBS — 오답**
    - EBS는 **AZ 단위** → Multi-AZ EC2 인스턴스 간 공유 불가
    - 표준 파일 시스템 구조 공유 불가 → 운영 부담 발생

## #53
한 회사는 회계 기록을 Amazon S3에 저장해야 합니다.  
기록은 **처음 1년 동안 즉시 액세스 가능**해야 하며, 이후 추가 **9년 동안 아카이브**되어야 합니다.  
회사의 누구도, 관리자 사용자나 루트 사용자도 **전체 10년 동안 기록을 삭제할 수 없어야 합니다.**  
기록은 **최대 내구성**으로 저장되어야 합니다.  

이 요구사항을 충족할 솔루션은 무엇입니까?

A. 기록을 10년 동안 S3 Glacier에 저장합니다. 접근 제어 정책을 사용하여 10년 동안 기록 삭제를 거부합니다.  

B. 기록을 S3 Intelligent-Tiering을 사용하여 저장합니다. IAM 정책을 사용하여 기록 삭제를 거부합니다. 10년 후 IAM 정책을 변경하여 삭제를 허용합니다.  

C. S3 Lifecycle 정책을 사용하여 기록을 처음 1년은 S3 Standard에 저장하고, 이후 S3 Glacier Deep Archive로 전환합니다. S3 Object Lock을 준수 모드(compliance mode)로 10년 동안 적용합니다.  

D. S3 Lifecycle 정책을 사용하여 기록을 처음 1년은 S3 Standard에 저장하고, 이후 S3 One Zone-Infrequent Access(S3 One Zone-IA)로 전환합니다. S3 Object Lock을 거버넌스 모드(governance mode)로 10년 동안 적용합니다.

```
A company needs to store its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then must be archived for an additional 9 years. No one at the company, including administrative users and root users, can be able to delete the records during the entire 10-year period. The records must be stored with maximum resiliency.  
Which solution will meet these requirements?

- A. Store the records in S3 Glacier for the entire 10-year period. Use an access control policy to deny deletion of the records for a period of 10 years.
- B. Store the records by using S3 Intelligent-Tiering. Use an IAM policy to deny deletion of the records. After 10 years, change the IAM policy to allow deletion.
- C. Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for a period of 10 years.
- D. Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 year. Use S3 Object Lock in governance mode for a period of 10 years.
```

정답 : `C

- 요구사항
	- 처음 1년 동안 즉시 액세스 가능 -> S3 Standard 사용
	- 추가 9년 아카이브 -> S3 Glacier Deep Archive 사용
	- 전체 10년 동안 삭제 불가 -> S3 Object Lock 준수 모드
	- 최대 내구성 -> S3 Standard + Glacier Deep Archive
- S3 Object Lock Compliance Mode
	- 정책 적용 기간 동안 누구도 삭제 불가, 루트 사용자 포함
	- 장기 보관 및 법규 준수 요구사항에 적합

오답 이유

- **A. S3 Glacier + 접근 제어 정책 — 오답**
    - S3 Glacier는 내구성 보장하지만, 단순 **IAM 정책만으로 삭제를 완전히 차단 불가**
    - 루트 사용자나 권한이 있는 사용자가 정책을 변경하면 삭제 가능
    
- **B. S3 Intelligent-Tiering + IAM 정책 — 오답**
    - IAM 정책으로 삭제 거부 가능하지만 **루트 사용자 또는 관리자 권한 사용 시 우회 가능**
    - 준수 모드 수준의 **법적 삭제 방지(Compliance)** 요구사항 충족 불가
    
- **D. S3 One Zone-IA + Object Lock 거버넌스 모드 — 오답**
    - 거버넌스 모드는 **권한 있는 사용자(root 포함) 우회 가능**
    - One Zone-IA는 **단일 AZ 저장**, 최대 내구성/고가용성 요구 사항 불충족

## #54
한 회사는 AWS에서 여러 Windows 워크로드를 실행합니다.  
회사의 직원들은 두 개의 Amazon EC2 인스턴스에서 호스팅되는 Windows 파일 공유를 사용합니다.  
파일 공유는 서로 데이터를 동기화하고 복제본을 유지합니다.  
회사는 **현재 사용자가 파일에 접근하는 방식을 그대로 유지하면서, 고가용성과 내구성을 갖춘 스토리지 솔루션**을 원합니다.  

이 요구사항을 충족할 솔루션은 무엇입니까?

A. 모든 데이터를 Amazon S3로 마이그레이션합니다. 사용자가 파일에 접근할 수 있도록 IAM 인증을 설정합니다.  
B. Amazon S3 File Gateway를 설정합니다. 기존 EC2 인스턴스에서 S3 File Gateway를 마운트합니다.  
C. Multi-AZ 구성을 갖춘 Amazon FSx for Windows File Server로 파일 공유 환경을 확장합니다. 모든 데이터를 FSx for Windows File Server로 마이그레이션합니다.  
D. Multi-AZ 구성을 갖춘 Amazon Elastic File System(Amazon EFS)로 파일 공유 환경을 확장합니다. 모든 데이터를 Amazon EFS로 마이그레이션합니다.

```
A company runs multiple Windows workloads on AWS. The company's employees use Windows file shares that are hosted on two Amazon EC2 instances. The file shares synchronize data between themselves and maintain duplicate copies. The company wants a highly available and durable storage solution that preserves how users currently access the files.  
What should a solutions architect do to meet these requirements?

- A. Migrate all the data to Amazon S3. Set up IAM authentication for users to access files.
- B. Set up an Amazon S3 File Gateway. Mount the S3 File Gateway on the existing EC2 instances.
- C. Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server.
- D. Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Migrate all the data to Amazon EFS.
```

정답 : `C`

- 요구사항
	- Windows 파일 공유 호환성 유지 -> SMB 프로토콜 지원 필요
	- 고가용성, 내구성 확보 -> Multi-AZ 구성
	- 현재 접근 방식 그대로 유지 -> 사용자 경험 최소 변경
- Amazon FSx for Windows File Server
	- 윈도우즈 네이티브 파일 시스템 지원(NTFS, SMB)
	- Active Director 통합 가능
	- 멀티 AZ 고가용성 및 내구성 제공
	- 기존 윈도우즈 클라이언트에서 파일 접근 방식 그대로 사용 가능

오답 이유

- **A. S3로 마이그레이션 + IAM 인증 — 오답**한
    - S3는 **객체 스토리지**, NTFS/SMB 지원 불가
    - 기존 Windows 파일 공유 접근 방식(드라이브 매핑, SMB) 불가 → 사용자 경험 변화
    
- **B. S3 File Gateway — 오답**
    - File Gateway는 SMB/NFS 지원 → 일부 호환성 제공
    - 하지만 **Multi-AZ 내구성 및 고가용성 제한**, 대규모 Windows 워크로드에는 적합하지 않음
    - 성능 및 동시 액세스 제한
    
- **D. Amazon EFS — 오답**
    - EFS는 **Linux 기반 NFS 최적화**, Windows SMB 지원 제한
    - Windows 클라이언트에서 NTFS 속성, ACL 등 호환성 문제 발생


## #55
솔루션 아키텍트가 여러 서브넷을 포함하는 VPC 아키텍처를 설계하고 있습니다.  
이 아키텍처는 Amazon EC2 인스턴스와 Amazon RDS DB 인스턴스를 사용하는 애플리케이션을 호스팅합니다.  
아키텍처에는 두 개의 가용 영역(AZ)에 걸쳐 총 6개의 서브넷이 있습니다.  
각 가용 영역에는 퍼블릭 서브넷, 프라이빗 서브넷, 데이터베이스 전용 서브넷이 포함되어 있습니다.  
**RDS 데이터베이스에 접근할 수 있는 것은 프라이빗 서브넷에서 실행되는 EC2 인스턴스뿐입니다.**

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. 퍼블릭 서브넷의 CIDR 블록에 대한 경로를 제외한 새로운 라우트 테이블을 생성합니다. DB 서브넷과 라우트 테이블을 연결합니다.  
B. 퍼블릭 서브넷에 할당된 보안 그룹에서 오는 인바운드 트래픽을 거부하는 보안 그룹을 생성합니다. DB 인스턴스에 이 보안 그룹을 연결합니다.  
C. 프라이빗 서브넷에 할당된 보안 그룹에서 오는 인바운드 트래픽을 허용하는 보안 그룹을 생성합니다. DB 인스턴스에 이 보안 그룹을 연결합니다.  
D. 퍼블릭 서브넷과 프라이빗 서브넷 간에 새로운 피어링 연결을 생성합니다. 프라이빗 서브넷과 데이터베이스 서브넷 간에 다른 피어링 연결을 생성합니다.

```
A solutions architect is developing a VPC architecture that includes multiple subnets. The architecture will host applications that use Amazon EC2 instances and Amazon RDS DB instances. The architecture consists of six subnets in two Availability Zones. Each Availability Zone includes a public subnet, a private subnet, and a dedicated subnet for databases. Only EC2 instances that run in the private subnets can have access to the RDS databases.  
Which solution will meet these requirements?

- A. Create a new route table that excludes the route to the public subnets' CIDR blocks. Associate the route table with the database subnets.
- B. Create a security group that denies inbound traffic from the security group that is assigned to instances in the public subnets. Attach the security group to the DB instances.
- C. Create a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances.
- D. Create a new peering connection between the public subnets and the private subnets. Create a different peering connection between the private subnets and the database subnets.
```

정답 : `C`

- 요구사항
	- DB 접근 제한 : 데이터베이스는 프라이빗 서브넷인 EC2 인스턴스에서만 접근 가능
	- VPC 서브넷 구성 : 퍼블릭, 프라이빗, DB 전용 서브넷
- 보안 그룹은 상태 저장형(Stateful) 방화벽으로, DB 인스턴스에 접근 가능한 소스를 정의할 수 있음
- DB 인스턴스 보안 그룹을 프라이빗 서브넷 EC2 인스턴스의 보안 그룹만 허용하도록 설정하면, 퍼브릵 서브넷 인스턴스나 외부에서는 접근 불가

오답 이유

- **A. 라우트 테이블 사용 — 오답**
    - 라우트 테이블은 서브넷 간 네트워크 경로를 정의하지만, **접근 제어를 위해 라우트 테이블만으로는 인스턴스 접근을 제한할 수 없음**
    - 보안 그룹이나 네트워크 ACL 없이 경로만 삭제하면 여전히 다른 방법으로 접근 가능
    
- **B. 퍼블릭 서브넷 보안 그룹 차단 — 오답**
    - 보안 그룹은 “허용” 규칙 중심이며, “거부(Deny)” 규칙을 지원하지 않음
    - AWS 보안 그룹은 **거부 규칙을 제공하지 않기 때문에**, 이 방식은 불가능
    
- **D. 피어링 연결 — 오답**
    - VPC 피어링은 서로 다른 VPC 간 트래픽을 허용할 때 사용
    - 같은 VPC 내 서브넷에서는 불필요하고, 접근 제어 문제를 해결하지 못함


## #56
한 회사가 Amazon Route 53에 도메인 이름을 등록했습니다.  
회사는 캐나다 중부(ca-central-1) 리전에서 Amazon API Gateway를 백엔드 마이크로서비스 API의 퍼블릭 인터페이스로 사용하고 있습니다.  
서드파티 서비스들이 API를 안전하게 사용합니다.  
회사는 API Gateway URL을 회사 도메인 이름과 해당 인증서를 사용하도록 설계하여 서드파티 서비스가 HTTPS를 사용할 수 있도록 하고 싶습니다.  

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. API Gateway에서 stage 변수를 Name="Endpoint-URL" 및 Value="Company Domain Name"으로 생성하여 기본 URL을 덮어씁니다. 회사 도메인 이름과 연결된 공개 인증서를 AWS Certificate Manager(ACM)에 가져옵니다.  

B. Route 53 DNS 레코드를 회사 도메인 이름으로 생성합니다. 별칭(Alias) 레코드를 Regional API Gateway stage 엔드포인트로 지정합니다. 회사 도메인 이름과 연결된 공개 인증서를 AWS Certificate Manager(ACM)에 us-east-1 리전에서 가져옵니다.  

C. Regional API Gateway 엔드포인트를 생성합니다. API Gateway 엔드포인트를 회사 도메인 이름과 연결합니다. 회사 도메인 이름과 연결된 공개 인증서를 AWS Certificate Manager(ACM)에 같은 리전에서 가져옵니다. 인증서를 API Gateway 엔드포인트에 연결합니다. Route 53을 구성하여 트래픽을 API Gateway 엔드포인트로 라우팅합니다.  

D. Regional API Gateway 엔드포인트를 생성합니다. API Gateway 엔드포인트를 회사 도메인 이름과 연결합니다. 회사 도메인 이름과 연결된 공개 인증서를 AWS Certificate Manager(ACM)에 us-east-1 리전에서 가져옵니다. 인증서를 API Gateway API에 연결합니다. Route 53 DNS 레코드를 회사 도메인 이름으로 생성합니다. A 레코드를 회사 도메인 이름으로 지정합니다.

```
A company has registered its domain name with Amazon Route 53. The company uses Amazon API Gateway in the ca-central-1 Region as a public interface for its backend microservice APIs. Third-party services consume the APIs securely. The company wants to design its API Gateway URL with the company's domain name and corresponding certificate so that the third-party services can use HTTPS.  
Which solution will meet these requirements?

- A. Create stage variables in API Gateway with Name="Endpoint-URL" and Value="Company Domain Name" to overwrite the default URL. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM).
- B. Create Route 53 DNS records with the company's domain name. Point the alias record to the Regional API Gateway stage endpoint. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region.
- C. Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.
- D. Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region. Attach the certificate to the API Gateway APIs. Create Route 53 DNS records with the company's domain name. Point an A record to the company's domain name.
```

정답 : `C`

- 요구사항
	- Regional API Gateway 사용
	- 회사 도메인 이름과 HTTPS 인증서 연결
	- Route 53으로 트래픽 라우팅
- Regional API Gateway는 해당 리전에서 ACM 인증서를 발급받아 직접 연결 가능
- 같은 리전에서 발급된 ACM 인증서를 사용해야 Regional API Gateway에서 HTTPS 지원
- Route 53 Alias 레코드를 이용해 API Gateway 엔드포인트로 트래픽 라우팅 가능

오답 이유

- **A. Stage 변수 사용 — 오답**
    - Stage 변수는 **엔드포인트 URL을 변경하지 못함**
    - API Gateway 커스텀 도메인 연결과 HTTPS 인증서 연결이 필요함
    
- **B. ACM 인증서를 us-east-1 리전에서 가져옴 — 오답**
    - Regional API Gateway는 **동일 리전에서 발급된 ACM 인증서만 지원**
    - us-east-1 인증서는 Global CloudFront 배포와 연동되는 Edge-optimized API Gateway에서만 사용 가능
    
- **D. ACM 인증서를 us-east-1 리전에서 가져오고 A 레코드 사용 — 오답**
    - Regional API Gateway는 **같은 리전 ACM 인증서 필요**
    - A 레코드를 직접 도메인 이름으로 연결하는 방식은 Route 53에서 지원하지 않음; Alias 사용해야 함


## #57
한 회사가 인기 있는 소셜 미디어 웹사이트를 운영하고 있습니다.  
웹사이트는 사용자가 이미지를 업로드하여 다른 사용자와 공유할 수 있는 기능을 제공합니다.  
회사는 업로드된 이미지가 부적절한 콘텐츠를 포함하지 않도록 하고자 합니다.  
회사는 개발 노력을 최소화하는 솔루션이 필요합니다.  

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. Amazon Comprehend를 사용하여 부적절한 콘텐츠를 감지합니다. 낮은 신뢰도의 예측은 사람(Human Review)에게 검토하도록 합니다.  

B. Amazon Rekognition을 사용하여 부적절한 콘텐츠를 감지합니다. 낮은 신뢰도의 예측은 사람(Human Review)에게 검토하도록 합니다.  

C. Amazon SageMaker를 사용하여 부적절한 콘텐츠를 감지합니다. 낮은 신뢰도의 예측은 Ground Truth를 사용하여 라벨링합니다.  

D. AWS Fargate를 사용하여 부적절한 콘텐츠를 감지하는 커스텀 머신러닝 모델을 배포합니다. 낮은 신뢰도의 예측은 Ground Truth를 사용하여 라벨링합니다.

```
A company is running a popular social media website. The website gives users the ability to upload images to share with other users. The company wants to make sure that the images do not contain inappropriate content. The company needs a solution that minimizes development effort.  
What should a solutions architect do to meet these requirements?

- A. Use Amazon Comprehend to detect inappropriate content. Use human review for low-confidence predictions.
- B. Use Amazon Rekognition to detect inappropriate content. Use human review for low-confidence predictions.
- C. Use Amazon SageMaker to detect inappropriate content. Use ground truth to label low-confidence predictions.
- D. Use AWS Fargate to deploy a custom machine learning model to detect inappropriate content. Use ground truth to label low-confidence predictions.
```

정답 : `B`

- 요구사항
	- 이미지 콘텐츠 분석
	- 부적절한 콘텐츠 감지
	- 개발 노력 최소화
- Amazon Rekognition은 이미지와 동영상 분석을 위한 완전관리형 서비스
	- NSFW 또는 부적절 콘텐츠 감지 기능 제공
- Human Review 기능과 통합하여 낮은 신뢰도 예측 처리 가능
- SageMaker, Fargate 등은 커스텀 모델 구축이 필요해 개발 부담 높음

오답 이유

- **A. Amazon Comprehend — 오답**
    - Comprehend는 **텍스트 분석 서비스**로, 이미지 분석 기능이 없음
    - 이미지 내 부적절 콘텐츠 검출 불가
    
- **C. Amazon SageMaker — 오답**
    - SageMaker는 커스텀 ML 모델 개발 필요
    - 요구사항의 **개발 노력 최소화** 조건을 충족하지 못함
    
- **D. AWS Fargate + 커스텀 모델 — 오답**
    - 컨테이너화된 커스텀 ML 모델 배포 필요
    - 개발, 배포, 관리 부담이 매우 높음

## #58
한 회사가 확장성과 가용성 요구사항을 충족하기 위해 중요 애플리케이션을 컨테이너로 실행하고자 합니다.  
회사는 중요 애플리케이션의 유지 관리에 집중하기를 원합니다.  
회사는 컨테이너화된 워크로드를 실행하는 **기반 인프라를 프로비저닝하거나 관리하는 책임**을 지고 싶지 않습니다.  

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. Amazon EC2 인스턴스를 사용하고, 인스턴스에 Docker를 설치합니다.  
B. Amazon Elastic Container Service(Amazon ECS)를 Amazon EC2 워커 노드에서 실행합니다.  
C. Amazon Elastic Container Service(Amazon ECS)를 AWS Fargate에서 실행합니다.  
D. Amazon EC2 인스턴스를 Amazon ECS 최적화 AMI로 실행합니다.

```
A company wants to run its critical applications in containers to meet requirements for scalability and availability. The company prefers to focus on maintenance of the critical applications. The company does not want to be responsible for provisioning and managing the underlying infrastructure that runs the containerized workload.  
What should a solutions architect do to meet these requirements?

- A. Use Amazon EC2 instances, and install Docker on the instances.
- B. Use Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 worker nodes.
- C. Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate.
- D. Use Amazon EC2 instances from an Amazon Elastic Container Service (Amazon ECS)-optimized Amazon Machine Image (AMI).
```

정답 : `C`

- 요구사항
	- 컨테이너화된 중요 애플리케이션 실행
	- 확장성 및 가용성 보장
	- 기반 인프라 관리 책임 없음
	- 애플리케이션 유지보수에 집중
- AWS Fargate는 서버리스 컨테이너 실행환경으로 사용자는 컨테이너 정의와 리소스 요구 사항만 지정하면 기반 이프라(EC2, OS, 패치 등)를 관리할 필요 없음
- ECS와 Fargate를 함께 사용하면 클러스터 관리 없이 컨테이너 단위로 확장 가능
- 

오답 이유

- **A. EC2 + Docker 설치 — 오답**
    - 사용자가 EC2 인스턴스와 Docker 환경을 직접 관리해야 함
    - 기반 인프라 관리 책임이 요구사항과 불일치
    
- **B. ECS on EC2 — 오답**
    - ECS 클러스터를 EC2에서 실행하면 EC2 인스턴스와 OS, 패치, 스케일링 등을 직접 관리해야 함
    - 완전 관리형 서버리스 요구사항 불충족
    
- **D. EC2 + ECS 최적화 AMI — 오답**
    - ECS 최적화 AMI는 인프라 설정을 단순화하지만, **EC2 인스턴스 관리 책임은 여전히 사용자에게 있음**
    - 기반 인프라 관리 부담 제거되지 않음


## #59
한 회사가 전 세계 300개 이상의 웹사이트와 애플리케이션을 호스팅하고 있습니다.  
회사는 하루에 30TB 이상의 클릭스트림 데이터를 분석할 수 있는 플랫폼이 필요합니다.  

이 클릭스트림 데이터를 전송하고 처리하기 위해 솔루션 설계자는 무엇을 해야 합니까?

A. AWS Data Pipeline을 설계하여 데이터를 Amazon S3 버킷에 아카이브하고, Amazon EMR 클러스터를 실행하여 데이터를 분석합니다.  

B. Auto Scaling 그룹으로 Amazon EC2 인스턴스를 생성하여 데이터를 처리하고 Amazon Redshift에서 분석할 수 있도록 Amazon S3 데이터 레이크에 전송합니다.  

C. 데이터를 Amazon CloudFront에 캐시합니다. 데이터를 Amazon S3 버킷에 저장합니다. 객체가 S3 버킷에 추가될 때마다 AWS Lambda 함수를 실행하여 데이터를 분석합니다.  

D. Amazon Kinesis Data Streams에서 데이터를 수집합니다. Amazon Kinesis Data Firehose를 사용하여 데이터를 Amazon S3 데이터 레이크로 전송합니다. 분석을 위해 Amazon Redshift에 데이터를 로드합니다.

```
A company hosts more than 300 global websites and applications. The company requires a platform to analyze more than 30 TB of clickstream data each day.  
What should a solutions architect do to transmit and process the clickstream data?

- A. Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon EMR cluster with the data to generate analytics.
- B. Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an Amazon S3 data lake for Amazon Redshift to use for analysis.
- C. Cache the data to Amazon CloudFront. Store the data in an Amazon S3 bucket. When an object is added to the S3 bucket. run an AWS Lambda function to process the data for analysis.
- D. Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit the data to an Amazon S3 data lake. Load the data in Amazon Redshift for analysis.
```

정답 : `D`

- 요구사항
	- 매일 30TB 이상의 대규모 클릭스트림 데이터 처리
	- 실시간 또는 near-real-time 데이터 전송 가능
	- 대규모 분석 가능
- Kinesis Data Streams : 대규모 실시간 스트리밍 데이터를 수집 가능
- Kinesis Data Firehos : 스트리밍 데이터를 S3 데이터 레이크로 안정적으로 전달
- S3 데이터 레이크 + Redshift : S3에서 데이터를 저장하고 Redshift에서 분석 가능

오답 이유

- **A. Data Pipeline + EMR** — 오답
    - Data Pipeline은 배치 기반 데이터 전송에 적합
    - 실시간 수집과 매일 30TB 이상 데이터 처리에는 적합하지 않음
    - EMR을 직접 관리해야 하므로 운영 부담 증가
    
- **B. EC2 Auto Scaling + S3 + Redshift** — 오답
    - EC2로 스트리밍 데이터를 처리하려면 직접 인프라와 스케일링 관리 필요
    - 운영 부담이 높음
    
- **C. CloudFront 캐시 + Lambda** — 오답
    - Lambda는 15분 실행 제한, 초당 호출 제한 존재
    - 하루 30TB 대규모 데이터 처리에는 부적합

## #60
한 회사가 AWS에 호스팅된 웹사이트를 운영하고 있습니다.  
웹사이트는 HTTP와 HTTPS를 각각 처리하도록 구성된 Application Load Balancer(ALB) 뒤에 있습니다.  
회사는 웹사이트에 대한 모든 요청이 HTTPS를 사용하도록 전달되기를 원합니다.  

이 요구사항을 충족하기 위해 솔루션 설계자는 무엇을 해야 합니까?

A. ALB의 네트워크 ACL을 업데이트하여 HTTPS 트래픽만 허용합니다.  
B. URL에서 HTTP를 HTTPS로 바꾸는 규칙을 생성합니다.  
C. ALB에서 리스너 규칙을 생성하여 HTTP 트래픽을 HTTPS로 리디렉션합니다.  
D. SNI(Server Name Indication)를 사용하도록 구성된 Network Load Balancer로 ALB를 교체합니다.

```
A company has a website hosted on AWS. The website is behind an Application Load Balancer (ALB) that is configured to handle HTTP and HTTPS separately. The company wants to forward all requests to the website so that the requests will use HTTPS.  
What should a solutions architect do to meet this requirement?

- A. Update the ALB's network ACL to accept only HTTPS traffic.
- B. Create a rule that replaces the HTTP in the URL with HTTPS.
- C. Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.
- D. Replace the ALB with a Network Load Balancer configured to use Server Name Indication (SNI).
```


정답 : `C`

- ALB는 HTTP와 HTTPS를 동시에 처리 가능하며 리스너 규칙을 통해 HTTP 요청을 HTTPS로 리디렉션 가능
- ALB 자체에서 URL 리디렉션 기능을 제공하므로 추가 서버 구성을 하지 않고 HTTPS 강제 적용 가능

오답 이유

- **A. 네트워크 ACL을 업데이트하여 HTTPS만 허용** — 오답
    - 단순히 HTTP 트래픽을 차단하면 브라우저에서 오류 발생
    - 기존 HTTP 요청을 HTTPS로 리디렉션하지 않음
    
- **B. URL에서 HTTP를 HTTPS로 바꾸는 규칙 생성** — 오답
    - URL 문자열 자체를 바꾸는 방식은 ALB에서 직접 지원하지 않음
    - 애플리케이션 레벨에서 처리해야 하므로 운영 부담 증가
    
- **D. NLB로 교체** — 오답
    - Network Load Balancer는 L4 계층으로 HTTP 리디렉션 기능 제공하지 않음
    - SNI는 TLS 호스트 이름 식별용으로 HTTPS 강제와 관련 없음