---
created: 2025-10-15 15:22:25
last_modified: 2025-10-15 18:29:58
---
## #481
한 회사가 AWS 클라우드에 3계층 웹 애플리케이션을 호스팅하고 있습니다.  
Multi-AZ 구성을 사용한 Amazon RDS for MySQL 서버가 데이터베이스 계층을 구성하고,  
Amazon ElastiCache가 캐시 계층을 구성합니다.  

회사는 고객이 데이터베이스에 항목을 추가할 때 캐시에 데이터를 추가하거나 업데이트하는 캐싱 전략을 원합니다.  
캐시에 있는 데이터는 항상 데이터베이스의 데이터와 일치해야 합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. Lazy loading 캐싱 전략을 구현합니다.  
B. Write-through 캐싱 전략을 구현합니다.  
C. TTL(Time To Live) 캐싱 전략을 구현합니다.  
D. AWS AppConfig 캐싱 전략을 구현합니다.

```
A company hosts a three-tier web application in the AWS Cloud. A Multi-AZAmazon RDS for MySQL server forms the database layer Amazon ElastiCache forms the cache layer. The company wants a caching strategy that adds or updates data in the cache when a customer adds an item to the database. The data in the cache must always match the data in the database.  
  
Which solution will meet these requirements?

- A. Implement the lazy loading caching strategy
- B. Implement the write-through caching strategy
- C. Implement the adding TTL caching strategy
- D. Implement the AWS AppConfig caching strategy
```

정답 : `B`

- Write-thropugh 캐싱은 데이터베이스에 쓰는 시점에 캐시도 동시에 갱신하는 전략
	- 데이터가 추가/수정될 때 DB와 캐시가 항상 동기화되어 일관성 유지

오답 이유

- **A. Lazy loading caching strategy (지연 로딩)**
    - 캐시 미스(cache miss)가 발생했을 때 **DB에서 데이터를 가져와 캐시에 저장**하는 전략.
    - DB 데이터가 변경되더라도 캐시는 즉시 갱신되지 않기 때문에, **DB와 캐시 간 불일치(inconsistency)** 가 발생할 수 있습니다.
    
- **C. TTL(Time To Live) caching strategy**
    - 캐시 항목이 일정 시간(TTL) 후 만료되도록 설정하는 방식.
    - 자동 만료를 통해 데이터 최신성을 높일 수 있지만, DB 변경 시 **즉각적인 캐시 갱신은 보장하지 않음**.
    - 문제의 “항상 일치해야 한다”는 요건에는 부적합합니다.
    
- **D. AWS AppConfig caching strategy**
    - AppConfig는 **애플리케이션 설정(Configuration) 관리 서비스**입니다.
    - 캐시 전략과 무관하며, 데이터베이스나 ElastiCache 데이터 일관성 문제를 해결할 수 없습니다.


## #482
한 회사가 온프레미스 위치에 있는 100GB의 과거 데이터를 Amazon S3 버킷으로 마이그레이션하려고 합니다.  
이 회사는 온프레미스에 100Mbps 인터넷 연결을 보유하고 있습니다.  
데이터는 S3 버킷으로 전송되는 동안 암호화되어야 합니다.  
회사는 새로운 데이터를 직접 Amazon S3에 저장할 예정입니다.

운영 오버헤드가 가장 적은 솔루션은 무엇입니까?

A. AWS CLI의 s3 sync 명령어를 사용하여 데이터를 S3 버킷으로 직접 이동합니다.  
B. AWS DataSync를 사용하여 온프레미스 위치의 데이터를 S3 버킷으로 마이그레이션합니다.  
C. AWS Snowball을 사용하여 데이터를 S3 버킷으로 이동합니다.  
D. 온프레미스 위치에서 AWS로 IPsec VPN을 설정합니다. AWS CLI의 s3 cp 명령어를 사용하여 데이터를 S3 버킷으로 직접 이동합니다.

```
A company wants to migrate 100 GB of historical data from an on-premises location to an Amazon S3 bucket. The company has a 100 megabits per second (Mbps) internet connection on premises. The company needs to encrypt the data in transit to the S3 bucket. The company will store new data directly in Amazon S3.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Use the s3 sync command in the AWS CLI to move the data directly to an S3 bucket
- B. Use AWS DataSync to migrate the data from the on-premises location to an S3 bucket
- C. Use AWS Snowball to move the data to an S3 bucket
- D. Set up an IPsec VPN from the on-premises location to AWS. Use the s3 cp command in the AWS CLI to move the data directly to an S3 bucket
```

정답 : `B`

- AWS DataSync는 온프레미스 스토리지와 S3 간 데이터 전송을 자동화하는 관리형 서비스
- DataSync는 TLS(전송 중 암호화)를 기본적으로 사용하여 데이터를 S3로 안전하게 전송
- 100GB 데이터는 100Mbps 속도로 충분히 전송가능하므로 물리적 장비나 수동 VPN 설정이 필요 없음

오답 이유

- **A. s3 sync 명령어 (AWS CLI)**
    - s3 sync는 HTTPS를 통해 전송하므로 암호화는 충족하지만, **오류 처리·재시도·속도 조정·병렬화·모니터링 등 자동화 기능이 부족**합니다.
    - 운영자가 직접 명령 실행, 중단 시 재시작 처리 등을 해야 하므로 **운영 오버헤드가 높습니다**.
    
- **C. AWS Snowball**
    - 오프라인 데이터 전송 장비로 **수 TB~PB 규모의 대용량 전송용**입니다.
    - 100GB는 네트워크로 전송하는 것이 더 빠르고 간단하므로, **비효율적이고 불필요한 물리 장비 조작 오버헤드가 발생**합니다.
    
- **D. IPsec VPN + s3 cp 명령어**
    - VPN 구성은 **복잡한 네트워크 설정 및 유지 관리**가 필요합니다.
    - S3 CLI 전송은 암호화를 자체적으로 처리할 수 있는데, VPN은 중복된 보안 계층입니다.
    - 결국 **구현 복잡도와 운영 부담이 증가**합니다.


## #483
한 회사가 Windows 컨테이너에서 .NET 6 Framework로 실행되는 Windows 잡을 컨테이너화했습니다. 회사는 이 잡을 AWS 클라우드에서 실행하고자 합니다. 이 잡은 10분마다 실행됩니다. 잡의 실행 시간은 1분에서 3분 사이로 변동됩니다.

이 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?

A. 잡의 컨테이너 이미지 기반으로 AWS Lambda 함수를 생성합니다. Amazon EventBridge를 구성하여 함수를 10분마다 호출합니다.
B. AWS Batch를 사용하여 AWS Fargate 리소스를 사용하는 잡을 생성합니다. 잡 스케줄링을 구성하여 10분마다 실행되도록 합니다.
C. Amazon Elastic Container Service (Amazon ECS) on AWS Fargate를 사용하여 잡을 실행합니다. 잡의 컨테이너 이미지를 기반으로 스케줄된 태스크를 생성하여 10분마다 실행되도록 합니다.
D. Amazon Elastic Container Service (Amazon ECS) on AWS Fargate를 사용하여 잡을 실행합니다. 잡의 컨테이너 이미지를 기반으로 독립형 태스크를 생성합니다. Windows 작업 스케줄러를 사용하여 잡을 10분마다 실행합니다.

```
A company containerized a Windows job that runs on .NET 6 Framework under a Windows container. The company wants to run this job in the AWS Cloud. The job runs every 10 minutes. The job’s runtime varies between 1 minute and 3 minutes.  
  
Which solution will meet these requirements MOST cost-effectively?

- A. Create an AWS Lambda function based on the container image of the job. Configure Amazon EventBridge to invoke the function every 10 minutes.
- B. Use AWS Batch to create a job that uses AWS Fargate resources. Configure the job scheduling to run every 10 minutes.
- C. Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a scheduled task based on the container image of the job to run every 10 minutes.
- D. Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a standalone task based on the container image of the job. Use Windows task scheduler to run the job every  
    10 minutes.
```

정답 : `C`

- Fargate는 ECS에서 Windows 컨테이너를 지원하므로 서버 관리 없이 컨테이너를 필요한 때만 기동해 과금되어 주기적 단발성 작업에 가장 비용 효율적
- EventBridge 세쿠젤 + ECS 스케줄드 태스크 조합으로 10분마다 태스크를 시작하고, 태스크가 1~3분 내 종료되면 비용도 그만큼만 발생

오답 이유

- **A. Lambda(컨테이너 이미지 기반)**
    - Lambda는 **Linux 기반 런타임/이미지만 지원**하며 Windows 컨테이너 이미지를 실행할 수 없습니다. 따라서 전제(Windows 컨테이너)에 부합하지 않습니다. 
    
- **B. AWS Batch on Fargate**
    - Batch도 Fargate에서 **Windows 컨테이너를 지원**하지만, 이 사용례(짧은 주기의 단일 잡 스케줄링)는 **ECS 스케줄드 태스크가 더 단순하고 관리 오버헤드/비용이 적음**. Batch 큐/잡 정의/컴퓨트 환경 등 추가 구성이 불필요합니다. 
    
- **D. ECS Fargate + Windows 작업 스케줄러**
    - 컨테이너 내부의 작업 스케줄러에 의존하려면 컨테이너를 **상시 실행**해야 하거나 자체 재기동 로직이 필요해 **비용과 운영 복잡도 증가**. EventBridge+스케줄드 태스크가 표준적이고 더 저렴합니다.


## #484
한 회사가 여러 개의 독립형 AWS 계정에서 통합된 멀티 계정 아키텍처로 전환하려고 합니다. 회사는 서로 다른 비즈니스 유닛을 위해 많은 새로운 AWS 계정을 만들 계획입니다. 회사는 중앙 집중식 기업 디렉터리 서비스를 사용하여 이러한 AWS 계정에 대한 액세스를 인증해야 합니다.

이 요구 사항을 충족하기 위해 솔루션스 아키텍트가 권장해야 하는 작업 조합은 무엇입니까? (두 가지를 선택하십시오.)

A. 모든 기능이 켜진 상태로 AWS Organizations에서 새 조직을 생성합니다. 조직에서 새로운 AWS 계정을 생성합니다.
B. Amazon Cognito 자격 증명 풀을 설정합니다. AWS IAM Identity Center(AWS Single Sign-On)가 Amazon Cognito 인증을 허용하도록 구성합니다.
C. 서비스 제어 정책(SCP)을 구성하여 AWS 계정을 관리합니다. AWS IAM Identity Center(AWS Single Sign-On)를 AWS Directory Service에 추가합니다.
D. AWS Organizations에서 새 조직을 생성합니다. 조직의 인증 메커니즘이 AWS Directory Service를 직접 사용하도록 구성합니다.
E. 조직에서 AWS IAM Identity Center(AWS Single Sign-On)를 설정합니다. IAM Identity Center를 구성하고 회사의 기업 디렉터리 서비스와 통합합니다.

```
A company wants to move from many standalone AWS accounts to a consolidated, multi-account architecture. The company plans to create many new AWS accounts for different business units. The company needs to authenticate access to these AWS accounts by using a centralized corporate directory service.  
  
Which combination of actions should a solutions architect recommend to meet these requirements? (Choose two.)

- A. Create a new organization in AWS Organizations with all features turned on. Create the new AWS accounts in the organization.
- B. Set up an Amazon Cognito identity pool. Configure AWS IAM Identity Center (AWS Single Sign-On) to accept Amazon Cognito authentication.
- C. Configure a service control policy (SCP) to manage the AWS accounts. Add AWS IAM Identity Center (AWS Single Sign-On) to AWS Directory Service.
- D. Create a new organization in AWS Organizations. Configure the organization's authentication mechanism to use AWS Directory Service directly.
- E. Set up AWS IAM Identity Center (AWS Single Sign-On) in the organization. Configure IAM Identity Center, and integrate it with the company's corporate directory service.
```

정답 : `A, E`

- A: 멀티 계정 전략의 기본은 AWS Organizations(모든 기능 활성화)를 사용해 계정을 중앙에서 생성･관리하는 것
- E: IAM Identity Center(구 SSO)를 조직에 설정하고, 기업 디렉터리(예: AD, Azure AD, Okta 등)와 연동하면 중앙 집중식 인증/SSO로 각 AWS 계정에 대한 액세스를 제공할 수 있음

오답 이유

- **B (Cognito identity pool)**: Cognito는 주로 **애플리케이션 사용자(앱 클라이언트)** 연동을 위한 서비스이며, **AWS 콘솔/CLI에 대한 계정 접근 SSO** 용도와는 부합하지 않습니다.
    
- **C (SCP + Identity Center를 Directory Service에 추가)**: **SCP는 권한 경계/거버넌스**를 위한 정책이지 **인증을 제공하지 않습니다**. 또한 “Identity Center를 Directory Service에 추가”라는 표현은 부정확하며, 인증 연동의 핵심은 **Identity Center와 기업 디렉터리의 통합**입니다.
    
- **D (Organizations가 Directory Service를 직접 인증 메커니즘으로 사용)**: **Organizations는 인증 제공자가 아닙니다**. 사용자 인증/SSO는 **IAM Identity Center**가 담당합니다.


## #485
한 회사가 오래된 뉴스 영상을 AWS에 저장할 수 있는 솔루션을 찾고 있습니다.  
회사는 비용을 최소화해야 하며, 이러한 파일을 복원할 일은 거의 없습니다.  
하지만 파일이 필요할 때는 최대 5분 이내에 사용할 수 있어야 합니다.

가장 비용 효율적인 솔루션은 무엇입니까?

A. Amazon S3 Glacier에 비디오 아카이브를 저장하고, Expedited 복구를 사용합니다.  
B. Amazon S3 Glacier에 비디오 아카이브를 저장하고, Standard 복구를 사용합니다.  
C. Amazon S3 Standard-Infrequent Access (S3 Standard-IA)에 비디오 아카이브를 저장합니다.  
D. Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)에 비디오 아카이브를 저장합니다.

```
A company is looking for a solution that can store video archives in AWS from old news footage. The company needs to minimize costs and will rarely need to restore these files. When the files are needed, they must be available in a maximum of five minutes.  
  
What is the MOST cost-effective solution?

- A. Store the video archives in Amazon S3 Glacier and use Expedited retrievals.
- B. Store the video archives in Amazon S3 Glacier and use Standard retrievals.
- C. Store the video archives in Amazon S3 Standard-Infrequent Access (S3 Standard-IA).
- D. Store the video archives in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA).
```

정답 : `A`

- Amazon S3 Glacier는 아카이브 데이터 저장용 최저 비용 스토리지 클래스
	- "Expedited retrieval" 옵션을 사용하면 1~5분 내에 데이터 복원
- 다른 Glacier 복구 옵션은 요구사항의 "5분 내 접근"을 충족하지 못함

오답 이유

- **B. S3 Glacier + Standard retrievals**
    - Standard 복구는 **3~5시간**이 걸리므로 “최대 5분 내” 조건을 충족하지 못합니다.
    
- **C. S3 Standard-IA**
    - 즉시 접근은 가능하지만, **저장 비용이 Glacier보다 훨씬 비쌉니다**.
    - “거의 접근하지 않는” 아카이브 용도로는 **비용 비효율적**입니다.
    
- **D. S3 One Zone-IA**
    - 비용은 낮지만, **단일 AZ** 저장으로 내구성이 낮습니다.
    - 또한 **즉시 접근 가능하므로 비용이 Glacier보다 여전히 높습니다**, 즉, “5분 내 접근”을 만족하긴 하지만 비용 최소화 측면에서는 Glacier보다 비쌈.


## #486
한 회사가 AWS에서 3계층 애플리케이션을 구축하고 있습니다. 프레젠테이션 계층은 정적 웹사이트를 제공할 것입니다. 로직 계층은 컨테이너화된 애플리케이션입니다. 이 애플리케이션은 관계형 데이터베이스에 데이터를 저장할 것입니다. 회사는 배포를 단순화하고 운영 비용을 줄이기를 원합니다.

이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 정적 콘텐츠를 호스팅하기 위해 Amazon S3를 사용합니다. 컴퓨팅 파워를 위해 AWS Fargate가 있는 Amazon Elastic Container Service (Amazon ECS)를 사용합니다. 데이터베이스에는 관리형 Amazon RDS 클러스터를 사용합니다.
B. 정적 콘텐츠를 호스팅하기 위해 Amazon CloudFront를 사용합니다. 컴퓨팅 파워를 위해 Amazon EC2가 있는 Amazon Elastic Container Service (Amazon ECS)를 사용합니다. 데이터베이스에는 관리형 Amazon RDS 클러스터를 사용합니다.
C. 정적 콘텐츠를 호스팅하기 위해 Amazon S3를 사용합니다. 컴퓨팅 파워를 위해 AWS Fargate가 있는 Amazon Elastic Kubernetes Service (Amazon EKS)를 사용합니다. 데이터베이스에는 관리형 Amazon RDS 클러스터를 사용합니다.
D. 정적 콘텐츠를 호스팅하기 위해 Amazon EC2 예약 인스턴스를 사용합니다. 컴퓨팅 파워를 위해 Amazon EC2가 있는 Amazon Elastic Kubernetes Service (Amazon EKS)를 사용합니다. 데이터베이스에는 관리형 Amazon RDS 클러스터를 사용합니다.

```
A company is building a three-tier application on AWS. The presentation tier will serve a static website The logic tier is a containerized application. This application will store data in a relational database. The company wants to simplify deployment and to reduce operational costs.  
  
Which solution will meet these requirements?

- A. Use Amazon S3 to host static content. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.
- B. Use Amazon CloudFront to host static content. Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for compute power. Use a managed Amazon RDS cluster for the database.
- C. Use Amazon S3 to host static content. Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.
- D. Use Amazon EC2 Reserved Instances to host static content. Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 for compute power. Use a managed Amazon RDS cluster for the database.
```

정답 : `A`

- S3 정적 웹 호스팅은 서버 관리가 필요 없고 비용이 매우 낮음
- ECS on Fargate는 서버리스 컨테이너 실행으로 클러스터/노드 관리 불필요, 사용량 기반 과금
- 관리형 RDS는 패치/백업/복구를 관리형으로 제공

오답 이유

- **B. CloudFront로 정적 콘텐츠 ‘호스팅’ + ECS on EC2**
    - CloudFront는 **호스팅 서비스가 아니라 CDN**이므로 원본(S3 등)이 필요합니다.
    - ECS on EC2는 **EC2 클러스터/용량 관리**가 필요해 Fargate 대비 **운영 부담↑**.
    
- **C. S3 + EKS on Fargate**
    - EKS는 **제어-plane과 쿠버네티스 구성/운영 지식**이 필요해 **배포/운영 복잡도↑**. 단순 웹/배치에는 ECS가 더 간단하고 비용 효율적입니다.
    
- **D. EC2로 정적 사이트 호스팅 + EKS on EC2**
    - 정적 사이트에 EC2는 **과도한 관리/비용**.
    - EKS on EC2는 **노드 관리 + 쿠버네티스 운영**이 모두 필요해 요구사항(단순화/비용 절감)에 반합니다.


## #487
한 회사가 애플리케이션을 위한 스토리지 솔루션을 찾고 있습니다. 이 솔루션은 고가용성과 확장성을 가져야 합니다. 또한 파일 시스템으로 동작해야 하며, AWS의 여러 Linux 인스턴스와 온프레미스에서 네이티브 프로토콜을 통해 마운트 가능해야 하고, 최소 크기 요구사항이 없어야 합니다. 회사는 온프레미스 네트워크에서 VPC로 액세스하기 위해 Site-to-Site VPN을 설정했습니다.

어떤 스토리지 솔루션이 이러한 요구사항을 충족합니까?

A. Amazon FSx 멀티 AZ 배포
B. Amazon Elastic Block Store (Amazon EBS) Multi-Attach 볼륨
C. 다중 마운트 타겟을 사용하는 Amazon Elastic File System (Amazon EFS)
D. 단일 마운트 타겟과 여러 액세스 포인트를 사용하는 Amazon Elastic File System (Amazon EFS)

```
A company seeks a storage solution for its application. The solution must be highly available and scalable. The solution also must function as a file system be mountable by multiple Linux instances in AWS and on premises through native protocols, and have no minimum size requirements. The company has set up a Site-to-Site VPN for access from its on-premises network to its VPC.  
  
Which storage solution meets these requirements?

- A. Amazon FSx Multi-AZ deployments
- B. Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes
- C. Amazon Elastic File System (Amazon EFS) with multiple mount targets
- D. Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points
```

정답 : `C`

- EFS는 POSIX 호환 NFS 파일 시스템으로, 용량 사전 할당/최소 크기 없음(자동 확장/축소)
- 다중 AZ 마운트 타겟으로 고가용성 제공, 여러 EC2 인스턴스 동시 마운트 가능
- Site-to-Site VPN(또는 Direct Connect)을 통해 온프레미스에서도 네이티브 NFS로 마운트 가능

오답 이유

- **A. Amazon FSx 멀티 AZ 배포**
    - FSx(Windows/ONTAP/OpenZFS 등)는 성능/기능이 풍부하지만 **용량을 사전 프로비저닝**해야 하는 등 **최소 크기/관리 복잡도**가 존재할 수 있음. 문제의 “최소 크기 요구사항 없음”과 단순 확장성 측면에서 **EFS가 더 적합**.
    
- **B. Amazon EBS Multi-Attach**
    - EBS는 **블록 스토리지**로 **동시 다중 인스턴스 마운트는 제한적(io1/io2, 같은 AZ)** 이며 **파일 시스템 공유 세마antics 보장 안 됨**. 또한 **온프레미스 마운트 불가**.
    
- **D. EFS 단일 마운트 타겟 + 여러 액세스 포인트**
    - **단일 마운트 타겟**은 AZ 장애 시 **가용성 저하**. 문제는 고가용성을 요구하므로 **각 AZ에 마운트 타겟을 배치**해야 함(액세스 포인트는 네임스페이스/권한 분리에 유용하지만 HA 대체 아님).


## #488
한 미디어 회사(설립 4년 차)는 AWS Organizations의 모든 기능(All features) 기능 세트를 사용하여 여러 AWS 계정을 구성하고 있습니다.  
회사의 재무팀에 따르면, 멤버 계정의 결제(billing) 정보는 멤버 계정의 루트 사용자(root user)조차도 접근할 수 없어야 합니다.

이 요구 사항을 충족할 수 있는 솔루션은 무엇입니까?

A. 모든 재무팀 사용자를 IAM 그룹에 추가합니다. Billing이라는 AWS 관리형 정책을 그룹에 연결합니다.  
B. 모든 사용자(루트 사용자 포함)가 결제 정보에 접근하지 못하도록 ID 기반 정책(identity-based policy)을 연결합니다.  
C. 결제 정보에 대한 접근을 거부하는 서비스 제어 정책(SCP)을 생성합니다. 이 SCP를 루트 조직 단위(OU)에 연결합니다.  
D. Organizations의 모든 기능(All features) 기능 세트에서 통합 결제(Consolidated billing) 기능 세트로 전환합니다.

```
A 4-year-old media company is using the AWS Organizations all features feature set to organize its AWS accounts. According to the company's finance team, the billing information on the member accounts must not be accessible to anyone, including the root user of the member accounts.  
  
Which solution will meet these requirements?

- A. Add all finance team users to an IAM group. Attach an AWS managed policy named Billing to the group.
- B. Attach an identity-based policy to deny access to the billing information to all users, including the root user.
- C. Create a service control policy (SCP) to deny access to the billing information. Attach the SCP to the root organizational unit (OU).
- D. Convert from the Organizations all features feature set to the Organizations consolidated billing feature set.
```

정답 : `C`

- SCP는 AWS Organizations의 모든 기능 모드에서 사용할 수 있는 상위 수준의 권한 제어 정책
- SCP는 IAM 정책보다 상위 레벨에서 동작하며, 루트 사용자 포함 모든 주체의 API 호출을 제한할 수 있음
- 따라서 결제 관련 API에 대해 명시적 Deny를 적용하면, 멤버 계정의 루트 사용자조차 결제 정보에 접근 불가

오답 이유

- **A. IAM 그룹 + Billing 정책 부여**
    - Billing 관리 정책은 **접근 허용** 정책이며, 접근 제한이 아닙니다.
    - 또한 **루트 사용자(root user)** 에게는 IAM 정책이 적용되지 않으므로 문제의 요구사항을 충족하지 않습니다.
    
- **B. ID 기반 정책으로 접근 거부**
    - ID 기반 정책은 IAM 사용자/역할에만 적용되며, **루트 사용자에는 영향을 미치지 않습니다.**
    - 따라서 루트 사용자 접근 제한 불가 → 요구 조건 불충족.
    
- **D. All features → Consolidated billing 전환**
    - Consolidated billing 모드에서는 **SCP를 사용할 수 없습니다.**
    - 즉, 루트 사용자 제한 같은 보안 제어 불가 → 반대로 기능이 축소됩니다.


## #489
한 전자상거래 회사는 온프레미스 창고 솔루션과 통합된 애플리케이션을 AWS 클라우드에서 운영하고 있습니다. 회사는 Amazon Simple Notification Service (Amazon SNS)를 사용하여 주문 메시지를 온프레미스 HTTPS 엔드포인트로 전송하여 창고 애플리케이션이 주문을 처리할 수 있도록 합니다. 로컬 데이터 센터 팀은 일부 주문 메시지가 수신되지 않았음을 감지했습니다.

솔루션스 아키텍트는 전달되지 않은 메시지를 보관하고 최대 14일 동안 메시지를 분석해야 합니다.

가장 적은 개발 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 보존 기간이 14일인 Amazon Kinesis Data Stream 대상을 갖는 Amazon SNS 데드 레터 큐를 구성합니다.
B. 애플리케이션과 Amazon SNS 사이에 보존 기간이 14일인 Amazon Simple Queue Service (Amazon SQS) 큐를 추가합니다.
C. 보존 기간이 14일인 Amazon Simple Queue Service (Amazon SQS) 대상을 갖는 Amazon SNS 데드 레터 큐를 구성합니다.
D. 보존 기간이 14일인 TTL 속성이 설정된 Amazon DynamoDB 대상을 갖는 Amazon SNS 데드 레터 큐를 구성합니다.

```
An ecommerce company runs an application in the AWS Cloud that is integrated with an on-premises warehouse solution. The company uses Amazon Simple Notification Service (Amazon SNS) to send order messages to an on-premises HTTPS endpoint so the warehouse application can process the orders. The local data center team has detected that some of the order messages were not received.  
  
A solutions architect needs to retain messages that are not delivered and analyze the messages for up to 14 days.  
  
Which solution will meet these requirements with the LEAST development effort?

- A. Configure an Amazon SNS dead letter queue that has an Amazon Kinesis Data Stream target with a retention period of 14 days.
- B. Add an Amazon Simple Queue Service (Amazon SQS) queue with a retention period of 14 days between the application and Amazon SNS.
- C. Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days.
- D. Configure an Amazon SNS dead letter queue that has an Amazon DynamoDB target with a TTL attribute set for a retention period of 14 days.
```

정답 : `C`

- SNS는 구독 단위 데드 레터 큐로 SQS 큐를 지정할 수 있어, 전달 실패한 메시지를 자동으로 SQS에 보관
- SQS 보존 기간 최대 14일을 설정하면 요구된 분석 기간을 충족
- 기존 게시/구독 구조를 바꾸지 않고 콘솔 설정만으로 구현 가능해 개발 노력 최소

오답 이유

- **A. SNS DLQ로 Kinesis Data Streams 지정**
    - SNS의 구독 DLQ 대상은 **SQS**가 표준이며, **Kinesis Data Streams는 DLQ 대상이 아님**. 또한 Kinesis 기본 보존은 24시간(옵션 비용 증가)으로 전제와도 맞지 않습니다.
    
- **B. 애플리케이션과 SNS 사이에 SQS 추가**
    - 애플리케이션이 **SQS→(별도 소비자 구현)→SNS** 로 전달하도록 아키텍처와 코드를 변경해야 해 **개발/운영 오버헤드 증가**. 요구사항은 “전달 실패 메시지 보관·분석”이므로 **구독 DLQ**가 더 직접적입니다.
    
- **D. SNS DLQ로 DynamoDB 지정**
    - SNS는 **DynamoDB를 DLQ 대상으로 직접 지원하지 않음**. TTL은 만료 삭제일 뿐 DLQ 기능 대체가 아닙니다.


## #490
한 게임 회사가 Amazon DynamoDB를 사용하여 지리적 위치, 플레이어 데이터, 리더보드와 같은 사용자 정보를 저장합니다. 회사는 최소한의 코딩으로 Amazon S3 버킷에 연속(continuous) 백업을 구성해야 합니다. 백업은 애플리케이션의 가용성에 영향을 주어서는 안 되며, 테이블에 정의된 읽기 용량 단위(RCU)에도 영향을 주어서는 안 됩니다.

이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. Amazon EMR 클러스터를 사용합니다. 데이터를 Amazon S3로 백업하기 위해 Apache Hive 작업을 생성합니다.
B. 연속 백업을 사용하여 DynamoDB에서 데이터를 Amazon S3로 직접 내보냅니다. 테이블에 대해 시점 복구(point-in-time recovery)를 활성화합니다.
C. Amazon DynamoDB Streams를 구성합니다. 스트림을 소비하여 데이터를 Amazon S3 버킷으로 내보내는 AWS Lambda 함수를 생성합니다.
D. 정기적으로 데이터베이스 테이블에서 데이터를 Amazon S3로 내보내는 AWS Lambda 함수를 생성합니다. 테이블에 대해 시점 복구(point-in-time recovery)를 활성화합니다.

```
A gaming company uses Amazon DynamoDB to store user information such as geographic location, player data, and leaderboards. The company needs to configure continuous backups to an Amazon S3 bucket with a minimal amount of coding. The backups must not affect availability of the application and must not affect the read capacity units (RCUs) that are defined for the table.  
  
Which solution meets these requirements?

- A. Use an Amazon EMR cluster. Create an Apache Hive job to back up the data to Amazon S3.
- B. Export the data directly from DynamoDB to Amazon S3 with continuous backups. Turn on point-in-time recovery for the table.
- C. Configure Amazon DynamoDB Streams. Create an AWS Lambda function to consume the stream and export the data to an Amazon S3 bucket.
- D. Create an AWS Lambda function to export the data from the database tables to Amazon S3 on a regular basis. Turn on point-in-time recovery for the table.
```

정답 : `B`

- PITR(연속 백업)은 최대 35일까지 연속적으로 변경 이력을 보존하며 가용성 및 RCU에 영향을 주지 않음
- Point-in-time export to S3 기능은 PITR 스냅샷에서 직접 S3로 내보내며 테이블 읽기 용량을 소모하지 않고(백그라운드 복제본 기반) 애플리케이션에 영향이 없음
- 콘솔/CLI로 구성 가능해 코드 작성이 최소

오답 이유

- **A. EMR + Hive 작업**
    - 별도 클러스터 운영/코드/스케줄링 필요 → **운영/개발 오버헤드 큼**. 또한 테이블을 스캔해 읽으면 **RCU 소비**가 발생할 수 있습니다.
    
- **C. DynamoDB Streams + Lambda → S3**
    - CDC 파이프라인 구현으로 **코딩 및 운영 복잡도 큼**. 과거 전체 스냅샷/백업 보장을 위해 추가 로직 필요. 스트림 처리 자체는 테이블 RCU와 별개지만, **연속 백업 요건을 간편하게 충족하는 표준 해법이 아님**.
    
- **D. 정기 Lambda 내보내기 + PITR**
    - 정기 풀 스캔 내보내기는 **RCU를 소비**하고 실행/재시도/증분 관리 등 **코드와 운영 부담**이 증가합니다. PITR만으로는 S3 백업이 자동 생성되지 않으므로 **요건 충족이 불완전**합니다.


## #491
한 솔루션스 아키텍트가 은행의 신용카드 데이터 유효성 검사 요청을 비동기적으로 처리하는 애플리케이션을 설계하고 있습니다. 애플리케이션은 보안이 보장되어야 하며, 각 요청을 최소 한 번 이상(at least once) 처리할 수 있어야 합니다.

이 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?

A. AWS Lambda 이벤트 소스 매핑을 사용합니다. Amazon Simple Queue Service (Amazon SQS) 표준 큐를 이벤트 소스로 설정합니다. 암호화에는 AWS Key Management Service (SSE-KMS)를 사용합니다. Lambda 실행 역할에 kms:Decrypt 권한을 추가합니다.
B. AWS Lambda 이벤트 소스 매핑을 사용합니다. Amazon Simple Queue Service (Amazon SQS) FIFO 큐를 이벤트 소스로 사용합니다. 암호화에는 SQS 관리형 키(SSE-SQS)를 사용합니다. Lambda 함수에 암호화 키 호출 권한을 추가합니다.
C. AWS Lambda 이벤트 소스 매핑을 사용합니다. Amazon Simple Queue Service (Amazon SQS) FIFO 큐를 이벤트 소스로 설정합니다. 암호화에는 AWS KMS 키(SSE-KMS)를 사용합니다. Lambda 실행 역할에 kms:Decrypt 권한을 추가합니다.
D. AWS Lambda 이벤트 소스 매핑을 사용합니다. Amazon Simple Queue Service (Amazon SQS) 표준 큐를 이벤트 소스로 설정합니다. 암호화에는 AWS KMS 키(SSE-KMS)를 사용합니다. Lambda 함수에 암호화 키 호출 권한을 추가합니다.

```
A solutions architect is designing an asynchronous application to process credit card data validation requests for a bank. The application must be secure and be able to process each request at least once.  
  
Which solution will meet these requirements MOST cost-effectively?

- A. Use AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS Key Management Service (SSE-KMS) for encryption. Add the kms:Decrypt permission for the Lambda execution role.
- B. Use AWS Lambda event source mapping. Use Amazon Simple Queue Service (Amazon SQS) FIFO queues as the event source. Use SQS managed encryption keys (SSE-SQS) for encryption. Add the encryption key invocation permission for the Lambda function.
- C. Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) FIFO queues as the event source. Use AWS KMS keys (SSE-KMS). Add the kms:Decrypt permission for the Lambda execution role.
- D. Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS KMS keys (SSE-KMS) for encryption. Add the encryption key invocation permission for the Lambda function.
```

정답 : `A`

- SQS 표준 큐 + Lambda 이벤트 소스 매핑은 at-least-once 처리 보장을 기본으로 제공하며, 실패 시 재시도/가시성 타임아웃으로 안정적
- 보안을 위해 전송/저장 시 암호화가 필요하므로 SSE-KMS 사용이 타당하며, Lambda 실행 역할에 kms:Decrypt 권한을 부여하면 KMS로 암호화된 메시지를 읽을 수 있음
- FIFO(순서/중복 제거)는 요구되지 않았고 비용이 더 높으므로 불필요

오답 이유

- **B. SQS FIFO + SSE-SQS**
    - FIFO는 **정확한 순서/중복 제거**가 필요할 때 사용하며 **비용이 더 높음**. 문제는 **at-least-once**만 요구합니다. 또한 SSE-SQS는 KMS 호출 비용을 줄일 수 있지만, **불필요한 FIFO 비용**이 추가됩니다.
    
- **C. SQS FIFO + SSE-KMS**
    - B보다 더 비용이 높습니다(FIFO 비용 + KMS 요청 비용). 요구사항에 없는 순서 보장/중복 제거를 제공하므로 **과도한 선택**입니다.
    
- **D. SQS 표준 + SSE-KMS(“키 호출 권한”)**
    - 표현이 모호하며 보통 **Lambda 실행 역할에 kms:Decrypt** 등 명시적 권한이 필요합니다. A가 **정확한 권한 부여 방식**을 제시합니다(표현의 정확성 측면에서 D보다 A가 적합).


## #492
한 회사는 개발 작업을 위해 여러 AWS 계정을 사용하고 있습니다. 일부 직원이 지속적으로 과도한 크기의 Amazon EC2 인스턴스를 사용하여 회사가 개발 계정의 연간 예산을 초과하고 있습니다. 회사는 이러한 계정에서 AWS 리소스 생성을 중앙에서 제한하고자 합니다.

이 요구 사항을 가장 적은 개발 노력으로 충족하는 솔루션은 무엇입니까?

A. 승인된 EC2 생성 프로세스를 사용하는 AWS Systems Manager 템플릿을 개발합니다. 승인된 Systems Manager 템플릿을 사용하여 EC2 인스턴스를 프로비저닝합니다.
B. AWS Organizations를 사용하여 계정을 조직 단위(OU)로 구성합니다. EC2 인스턴스 유형 사용을 제어하는 서비스 제어 정책(SCP)을 정의하여 연결합니다.
C. EC2 인스턴스가 생성될 때 AWS Lambda 함수를 호출하는 Amazon EventBridge 규칙을 구성합니다. 허용되지 않는 EC2 인스턴스 유형을 중지합니다.
D. 직원이 허용된 EC2 인스턴스 유형을 생성할 수 있도록 AWS Service Catalog 제품을 설정합니다. 직원이 Service Catalog 제품을 사용해서만 EC2 인스턴스를 배포할 수 있도록 보장합니다.

```
A company has multiple AWS accounts for development work. Some staff consistently use oversized Amazon EC2 instances, which causes the company to exceed the yearly budget for the development accounts. The company wants to centrally restrict the creation of AWS resources in these accounts.  
  
Which solution will meet these requirements with the LEAST development effort?

- A. Develop AWS Systems Manager templates that use an approved EC2 creation process. Use the approved Systems Manager templates to provision EC2 instances.
- B. Use AWS Organizations to organize the accounts into organizational units (OUs). Define and attach a service control policy (SCP) to control the usage of EC2 instance types.
- C. Configure an Amazon EventBridge rule that invokes an AWS Lambda function when an EC2 instance is created. Stop disallowed EC2 instance types.
- D. Set up AWS Service Catalog products for the staff to create the allowed EC2 instance types. Ensure that staff can deploy EC2 instances only by using the Service Catalog products.
```

정답 : `B`

- SCP는 여러 계정에 걸쳐 '최대 권한 경계'를 중앙에서 강제할 수 있어, ec2:RunInstances에 대해 특정 인스턴스 유형을 명시적 Deny로 일괄 차단 가능
- OU 단위로 한 번 작성해 연결하면 모든 하위 계정에 즉시 적용되며, 추가 코드/런타임 구성 없이 콘솔･CLI･SDK 어디서 시도해도 생성 자체가 차단

오답 이유

- **A. Systems Manager 템플릿(Automation/Runbook) 강제**
    - 승인된 템플릿을 만들어도 **사용자가 콘솔/CLI로 직접 EC2를 생성하는 것을 기술적으로 차단하지 못함**. 프로세스 준수 의존이라 우회가 가능하며, 중앙 강제력이 약함.
    
- **C. EventBridge + Lambda로 사후 중지**
    - 인스턴스가 **이미 생성된 후** 중지합니다. 짧은 시간이라도 **비용 발생/보안·태깅 일탈**이 생기며, **함수 개발·운영(권한, 재시도, 예외 처리)** 오버헤드가 큼. 근본적 예방이 아님.
    
- **D. Service Catalog 제품만 사용하도록**
    - 제품 설계/포트폴리오 공유/권한 설정 등 **구성·운영 복잡도**가 큼. 또한 사용자가 **Service Catalog를 우회해 직접 생성**하지 못하도록 하려면 **결국 SCP 같은 중앙 통제**가 추가로 필요.


## #493
한 회사가 인공지능(AI)을 사용하여 고객 서비스 통화의 품질을 평가하려고 합니다.  
회사는 현재 영어를 포함한 4개의 다른 언어로 통화를 관리하고 있으며, 향후 새로운 언어도 추가할 예정입니다.  
회사는 머신러닝(ML) 모델을 정기적으로 유지 관리할 리소스가 없습니다.  

회사는 고객 서비스 통화 녹음으로부터 **문서 형태의 감정 분석 보고서**를 생성해야 합니다.  
또한 고객 서비스 통화 녹음의 텍스트는 **영어로 번역**되어야 합니다.  

이 요구 사항을 충족할 수 있는 단계 조합은 무엇입니까? (세 가지를 선택하십시오.)

A. Amazon Comprehend를 사용하여 오디오 녹음을 영어로 번역합니다.  
B. Amazon Lex를 사용하여 문서 형태의 감정 분석 보고서를 생성합니다.  
C. Amazon Polly를 사용하여 오디오 녹음을 텍스트로 변환합니다.  
D. Amazon Transcribe를 사용하여 모든 언어의 오디오 녹음을 텍스트로 변환합니다.  
E. Amazon Translate를 사용하여 모든 언어의 텍스트를 영어로 번역합니다.  
F. Amazon Comprehend를 사용하여 감정 분석 보고서를 생성합니다.

```
A company wants to use artificial intelligence (AI) to determine the quality of its customer service calls. The company currently manages calls in four different languages, including English. The company will offer new languages in the future. The company does not have the resources to regularly maintain machine learning (ML) models.  
  
The company needs to create written sentiment analysis reports from the customer service call recordings. The customer service call recording text must be translated into English.  
  
Which combination of steps will meet these requirements? (Choose three.)

- A. Use Amazon Comprehend to translate the audio recordings into English.
- B. Use Amazon Lex to create the written sentiment analysis reports.
- C. Use Amazon Polly to convert the audio recordings into text.
- D. Use Amazon Transcribe to convert the audio recordings in any language into text.
- E. Use Amazon Translate to translate text in any language to English.
- F. Use Amazon Comprehend to create the sentiment analysis reports.
```

정답 : `D, E, F`

- D: Amazon Transcribe: 여러 언어의 음성(오디오)를 자동으로 텍스트로 변환
- E: Amazon Translate: Tanscribe로 얻은 비영어 텍스트를 영어로 번역
- F: Amazon Comprehend: 영어 텍스트에 대해 감정 분석(sentiment analysis) 및 요약 보고서 생성
- 세 서비스 모두 완전관리형으로 ML 모델 유지보수 필요 없음

오답 이유

- **A. Amazon Comprehend로 오디오 번역**
    - Comprehend는 **텍스트 분석 서비스**이며, **오디오 처리 기능 없음**. 오디오 번역 불가.
    
- **B. Amazon Lex**
    - 대화형 챗봇/음성 인터페이스용 서비스로, **감정 분석 보고서 생성 기능 없음**.
    
- **C. Amazon Polly**
    - **텍스트를 음성으로 변환(TTS)** 하는 서비스로, 반대 방향(음성→텍스트 변환) 아님.


## #494
한 회사가 Amazon EC2 인스턴스를 사용하여 내부 시스템을 호스팅하고 있습니다.  
배포 작업의 일환으로 관리자가 AWS CLI를 사용해 EC2 인스턴스를 종료(terminate)하려 했지만,  
403 (Access Denied) 오류 메시지를 받았습니다.

관리자는 다음 IAM 정책이 연결된 IAM 역할을 사용하고 있습니다:

```
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Effect": "Allow",
			"Action": ["ec2:TerminateInstances"],
			"Resource": ["*"]
		},
		{
			"Effect": "Deny",
			"Action": ["ec2:TerminateInstances"],
			"Condition": {
				"NotIpAddress": {
					"aws:SourceIp": [
						"192.0.2.0/24",
						"203.0.113.0/24"
					]
				}
			},
			"Resource": ["*"]
		}
	]
}
```

관리자가 인스턴스 종료 요청을 수행하지 못한 원인은 무엇입니까?

A. EC2 인스턴스에 Deny 문이 포함된 리소스 기반 정책이 있습니다.

B. 정책 문에 Principal(주체)이 지정되지 않았습니다.

C. “Action” 필드가 인스턴스 종료에 필요한 작업 권한을 부여하지 않습니다.

D. EC2 인스턴스 종료 요청이 192.0.2.0/24 또는 203.0.113.0/24 CIDR 블록에서 발생하지 않았습니다.

```
A company uses Amazon EC2 instances to host its internal systems. As part of a deployment operation, an administrator tries to use the AWS CLI to terminate an EC2 instance. However, the administrator receives a 403 (Access Denied) error message.  
  
The administrator is using an IAM role that has the following IAM policy attached:  
  
```
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Effect": "Allow",
			"Action": ["ec2:TerminateInstances"],
			"Resource": ["*"]
		},
		{
			"Effect": "Deny",
			"Action": ["ec2:TerminateInstances"],
			"Condition": {
				"NotIpAddress": {
					"aws:SourceIp": [
						"192.0.2.0/24",
						"203.0.113.0/24"
					]
				}
			},
			"Resource": ["*"]
		}
	]
}
```
  
What is the cause of the unsuccessful request?

- A. The EC2 instance has a resource-based policy with a Deny statement.
- B. The principal has not been specified in the policy statement.
- C. The "Action" field does not grant the actions that are required to terminate the EC2 instance.
- D. The request to terminate the EC2 instance does not originate from the CIDR blocks 192.0.2.0/24 or 203.0.113.0/24.
```

정답 : `D`

- Deny 문에는 "aws:SourceIp"가 "192.0.2.0/24" 또는 "203.0.113.0/24"가 아닌 경우에 ec2 정지를 못하도록 적용되어 있기 때문에 403 응답을 받았다는 것은 지정된 IP 범위가 아닌 외부에서 발생

오답 이유

- **A. EC2 인스턴스의 리소스 기반 정책(Deny)**  
	- EC2 인스턴스에는 **리소스 기반 정책이 존재하지 않습니다.** (S3, Lambda, KMS 등에서만 적용 가능)  
	- 따라서 해당 이유는 불가능합니다.

- **B. Principal 누락**  
	- IAM 역할에 연결된 정책에서는 **Principal을 명시하지 않아도** 됩니다. Principal은 역할에 연결된 엔터티(사용자, 서비스 등)로 자동 정의됩니다.

- **C. Action 불충분**  
	- `ec2:TerminateInstances` 는 인스턴스 종료에 필요한 올바른 작업(Action)입니다. 
	- 권한 자체는 허용되어 있으므로 원인이 아닙니다.


## #495
한 회사가 내부 감사를 수행하고 있습니다.  
회사는 AWS Lake Formation 데이터 레이크와 연결된 Amazon S3 버킷의 데이터에  
민감한 고객 또는 직원 데이터가 포함되어 있지 않은지 확인하려고 합니다.  

회사는 **여권 번호, 신용카드 번호 등 개인 식별 정보(PII)** 또는 **금융 정보**를 탐지하고자 합니다.  

이 요구 사항을 충족할 수 있는 솔루션은 무엇입니까?

A. 계정에서 AWS Audit Manager를 구성합니다. 감사를 위해 Payment Card Industry Data Security Standards (PCI DSS)를 선택합니다.  
B. S3 버킷에서 Amazon S3 Inventory를 구성하고, Amazon Athena를 사용하여 인벤토리를 쿼리합니다.  
C. Amazon Macie를 구성하여 필요한 데이터 유형에 대한 관리형 식별자(managed identifiers)를 사용하는 데이터 검색 작업을 실행합니다.  
D. Amazon S3 Select를 사용하여 S3 버킷 전체에 걸친 보고서를 실행합니다.

```
A company is conducting an internal audit. The company wants to ensure that the data in an Amazon S3 bucket that is associated with the company’s AWS Lake Formation data lake does not contain sensitive customer or employee data. The company wants to discover personally identifiable information (PII) or financial information, including passport numbers and credit card numbers.  
  
Which solution will meet these requirements?

- A. Configure AWS Audit Manager on the account. Select the Payment Card Industry Data Security Standards (PCI DSS) for auditing.
- B. Configure Amazon S3 Inventory on the S3 bucket Configure Amazon Athena to query the inventory.
- C. Configure Amazon Macie to run a data discovery job that uses managed identifiers for the required data types.
- D. Use Amazon S3 Select to run a report across the S3 bucket.
```

정답 : `C`

- Amazon Macie는 S3 내 데이터 분류 및 민감 데이터 탐지 서비스
	- 개인 식별 정보(PII), 금융 데이터(신용카드, 여권번호 등)를 자동으로 식별
- Macie의 Managed Data Identifier는 신용카드 번호, 이름, 주소, 여권 번호, 은행 계좌번호 등 표준 PII/PCI 패턴을 사전에 정의해 추가 코딩 없이 스캔 가능
- Macie는 S3 객체를 직접 분석하며 Lake Formation, Athena와 별개로 동작

오답 이유

- **A. AWS Audit Manager (PCI DSS 템플릿)**
    - Audit Manager는 **컴플라이언스 보고서 자동 수집 및 평가** 도구로, 실제 **데이터 내용(Payload)** 을 검사하지 않습니다.
    - 즉, S3 내 파일의 **PII/금융 데이터 탐지 기능이 없음**.
    
- **B. Amazon S3 Inventory + Athena**
    - S3 Inventory는 **객체 메타데이터(예: 키, 버전, 암호화 상태 등)** 목록만 제공합니다.
    - 객체 내용(Content)을 스캔하지 않으므로 **민감 정보 탐지 불가**.
    
- **D. Amazon S3 Select**
    - 특정 객체 내의 데이터 일부를 SQL로 조회할 수 있지만, **PII 자동 식별 기능이 없고** 모든 객체를 일괄 스캔하려면 **대규모 수작업 쿼리 필요**.
    - 비용과 관리 오버헤드가 커서 비효율적입니다.


## #496
한 회사는 온프레미스 서버로 애플리케이션을 호스팅하고 있습니다. 회사는 스토리지 용량이 부족해지고 있습니다. 애플리케이션은 블록 스토리지와 NFS 스토리지를 모두 사용합니다. 회사는 기존 애플리케이션을 재설계하지 않고 로컬 캐싱을 지원하는 고성능 솔루션이 필요합니다.

이 요구 사항을 충족하기 위해 솔루션스 아키텍트가 수행해야 할 작업 조합은 무엇입니까? (두 가지를 선택하십시오.)

A. Amazon S3를 파일 시스템으로 온프레미스 서버에 마운트합니다.
B. NFS 스토리지를 대체하기 위해 AWS Storage Gateway 파일 게이트웨이를 배포합니다.
C. 온프레미스 서버에 NFS 마운트를 프로비저닝하기 위해 AWS Snowball Edge를 배포합니다.
D. 블록 스토리지를 대체하기 위해 AWS Storage Gateway 볼륨 게이트웨이를 배포합니다.
E. Amazon Elastic File System (Amazon EFS) 볼륨을 배포하고 이를 온프레미스 서버에 마운트합니다.

```
A company uses on-premises servers to host its applications. The company is running out of storage capacity. The applications use both block storage and NFS storage. The company needs a high-performing solution that supports local caching without re-architecting its existing applications.  
  
Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)

- A. Mount Amazon S3 as a file system to the on-premises servers.
- B. Deploy an AWS Storage Gateway file gateway to replace NFS storage.
- C. Deploy AWS Snowball Edge to provision NFS mounts to on-premises servers.
- D. Deploy an AWS Storage Gateway volume gateway to replace the block storage.
- E. Deploy Amazon Elastic File System (Amazon EFS) volumes and mount them to on-premises servers.
```

정답 : `B, D`

- 파일 게이트웨이는 온프레미스에서 NFS/SMB를 그대로 제공하면서 로컬 캐시 디스크로 성능을 높이고 백엔드는 S3에 저장
- 볼륨 게이트웨이는 iSCSI 블록 디바이스를 온프레미스에 제공하며 캐시드/스토어드 볼륨으로 로컬 캐싱을 활용할 수 있어 기존 블록스토리지 워크로드를 재설계 없이 대체 가능

오답 이유

- **A. S3를 파일 시스템으로 마운트**
    - S3는 오브젝트 스토리지로, 네이티브 POSIX 파일 시스템이 아닙니다. 공식적으로 일반 목적의 “직접 마운트”를 지원하지 않으며, 요구한 **로컬 캐싱/고성능/재설계 없음**을 만족하기 어렵습니다.
    
- **C. Snowball Edge 배포**
    - 주로 **오프라인 데이터 이관/엣지 일시 처리** 용도입니다. 상시 운영형 NFS 스토리지 대체 및 지속 캐싱 솔루션이 아닙니다.
    
- **E. EFS를 온프레미스에서 마운트**
    - VPN/Direct Connect를 통해 온프레미스에서 NFS로 접근은 가능하나, **로컬 캐시를 제공하지 않으며** WAN 지연의 영향을 받습니다. 또한 블록 스토리지 대체가 되지 않습니다.

## #497
한 회사는 동일한 AWS 리전에 있는 Amazon S3 버킷에서 대량의 데이터를 읽고 쓰는 서비스를 보유하고 있습니다. 이 서비스는 VPC의 프라이빗 서브넷 내 Amazon EC2 인스턴스에 배포되어 있습니다. 서비스는 퍼블릭 서브넷의 NAT 게이트웨이를 통해 Amazon S3와 통신합니다. 그러나 회사는 데이터 아웃바운드 비용을 줄일 수 있는 솔루션을 원합니다.

가장 비용 효율적으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 퍼블릭 서브넷에 전용 EC2 NAT 인스턴스를 프로비저닝합니다. 프라이빗 서브넷의 라우트 테이블을 이 인스턴스의 탄력적 네트워크 인터페이스를 모든 S3 트래픽의 대상지로 사용하도록 구성합니다.
B. 프라이빗 서브넷에 전용 EC2 NAT 인스턴스를 프로비저닝합니다. 퍼블릭 서브넷의 라우트 테이블을 이 인스턴스의 탄력적 네트워크 인터페이스를 모든 S3 트래픽의 대상지로 사용하도록 구성합니다.
C. VPC 게이트웨이 엔드포인트를 프로비저닝합니다. 프라이빗 서브넷의 라우트 테이블을 게이트웨이 엔드포인트가 모든 S3 트래픽의 경로가 되도록 구성합니다.
D. 두 번째 NAT 게이트웨이를 프로비저닝합니다. 프라이빗 서브넷의 라우트 테이블을 이 NAT 게이트웨이가 모든 S3 트래픽의 대상지가 되도록 구성합니다.

```
A company has a service that reads and writes large amounts of data from an Amazon S3 bucket in the same AWS Region. The service is deployed on Amazon EC2 instances within the private subnet of a VPC. The service communicates with Amazon S3 over a NAT gateway in the public subnet. However, the company wants a solution that will reduce the data output costs.  
  
Which solution will meet these requirements MOST cost-effectively?

- A. Provision a dedicated EC2 NAT instance in the public subnet. Configure the route table for the private subnet to use the elastic network interface of this instance as the destination for all S3 traffic.
- B. Provision a dedicated EC2 NAT instance in the private subnet. Configure the route table for the public subnet to use the elastic network interface of this instance as the destination for all S3 traffic.
- C. Provision a VPC gateway endpoint. Configure the route table for the private subnet to use the gateway endpoint as the route for all S3 traffic.
- D. Provision a second NAT gateway. Configure the route table for the private subnet to use this NAT gateway as the destination for all S3 traffic.
```

정답 : `C`

- S3용 VPC 게이트웨이 엔드포인트를 사용하면 트래픽이 NAT 게이트웨이를 우회하여 AWS 백본 내부로 직접 라우팅
- 게이트웨이 엔드포인트는 시간/처리 요금이 없고(무료) NAT 게이트웨이의 Data processing 비용/시간당 비용을 제거하므로 가장 비용 효율적
- 동일 리전 내 S3 통신 요구를 기능적으로 그대로 충족

오답 이유

- **A. 퍼블릭 서브넷에 EC2 NAT 인스턴스**
    - NAT 게이트웨이 대신 NAT 인스턴스를 써도 **데이터 처리 비용/운영 오버헤드**가 남고, 스케일/가용성 관리가 필요합니다. **게이트웨이 엔드포인트보다 비효율**.
    
- **B. 프라이빗 서브넷에 NAT 인스턴스**
    - NAT 인스턴스는 **퍼블릭 서브넷에 배치**되어야 하며, 구성 자체가 부적절합니다. 또한 비용/운영 측면에서도 비효율.
    
- **D. NAT 게이트웨이 추가**
    - 두 번째 NAT GW를 추가해도 **총 비용 증가**이며, S3 트래픽 비용 절감에 도움이 되지 않습니다. 근본 원인( NAT 경유 비용 )을 해결하지 못합니다.


## #498
한 회사가 Amazon S3를 사용하여 고해상도 이미지를 S3 버킷에 저장하고 있습니다.  
애플리케이션 변경을 최소화하기 위해 회사는 이미지를 항상 S3 객체의 최신 버전으로 저장합니다.  
회사는 각 이미지의 **가장 최근 두 개의 버전만 보존**해야 합니다.

회사는 비용 절감을 원하며, S3 버킷이 주요 비용 요인으로 확인되었습니다.

운영 오버헤드를 최소화하면서 S3 비용을 줄일 수 있는 솔루션은 무엇입니까?

A. S3 Lifecycle을 사용하여 만료된 객체 버전을 삭제하고 가장 최근 두 버전만 유지합니다.  
B. AWS Lambda 함수를 사용하여 오래된 버전을 확인하고 가장 최근 두 버전을 제외한 나머지를 삭제합니다.  
C. S3 Batch Operations를 사용하여 비현재(noncurrent) 객체 버전을 삭제하고 두 개의 최신 버전만 유지합니다.  
D. S3 버전 관리를 비활성화하고 두 개의 최신 버전만 유지합니다.

```
A company uses Amazon S3 to store high-resolution pictures in an S3 bucket. To minimize application changes, the company stores the pictures as the latest version of an S3 object. The company needs to retain only the two most recent versions of the pictures.  
  
The company wants to reduce costs. The company has identified the S3 bucket as a large expense.  
  
Which solution will reduce the S3 costs with the LEAST operational overhead?

- A. Use S3 Lifecycle to delete expired object versions and retain the two most recent versions.
- B. Use an AWS Lambda function to check for older versions and delete all but the two most recent versions.
- C. Use S3 Batch Operations to delete noncurrent object versions and retain only the two most recent versions.
- D. Deactivate versioning on the S3 bucket and retain the two most recent versions.
```

정답 : `A`

- S3 라이프사이클 규칙은 버전 관리가 활성화된 버킷에서 비현재(noncurrent) 객체 버전의 보존 개수를 자동으로 관리 가능
- 이 방식은 자동화된 관리형 기능으로 Lambda, Batch 작업, 수동 스크립트가 불필요해 운영 오버헤드가 가장 낮음

오답 이유

- **B. Lambda 함수로 오래된 버전 삭제**
    - 가능은 하지만 **버전 목록 조회 및 삭제 로직 구현 필요**, 코드 유지보수/오류처리/비용 발생 등 **운영 오버헤드 증가**.
    
- **C. S3 Batch Operations**
    - 대규모 일회성 정리에 유용하지만 **주기적 관리 자동화가 불가능**하며, **Batch Job 구성 및 유지보수**가 필요. Lifecycle Rule보다 복잡.
    
- **D. 버전 관리 비활성화**
    - 버전 관리를 비활성화하면 **모든 기존 비현재 버전은 그대로 남아 있음**, 자동 삭제되지 않으며 “최근 2개 유지” 제어 불가능.
    - 또한 이후 새 버전 생성 시 **이전 버전이 완전히 덮어쓰기 되어 복구 불가**, 요구사항(두 버전 유지) 불충족.


## #499
한 회사가 1 Gbps AWS Direct Connect 연결의 비용을 최소화해야 합니다.  
회사의 평균 연결 사용률은 10% 미만입니다.  
솔루션스 아키텍트는 보안을 손상시키지 않으면서 비용을 줄일 수 있는 솔루션을 추천해야 합니다.

이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 새 1 Gbps Direct Connect 연결을 설정하고, 다른 AWS 계정과 연결을 공유합니다.  
B. AWS Management Console에서 새 200 Mbps Direct Connect 연결을 설정합니다.  
C. AWS Direct Connect 파트너에게 문의하여 1 Gbps 연결을 주문하고, 다른 AWS 계정과 연결을 공유합니다.  
D. 기존 AWS 계정에 대해 AWS Direct Connect 파트너에게 문의하여 200 Mbps 호스티드(Hosted) 연결을 주문합니다.

```
A company needs to minimize the cost of its 1 Gbps AWS Direct Connect connection. The company's average connection utilization is less than 10%. A solutions architect must recommend a solution that will reduce the cost without compromising security.  
  
Which solution will meet these requirements?

- A. Set up a new 1 Gbps Direct Connect connection. Share the connection with another AWS account.
- B. Set up a new 200 Mbps Direct Connect connection in the AWS Management Console.
- C. Contact an AWS Direct Connect Partner to order a 1 Gbps connection. Share the connection with another AWS account.
- D. Contact an AWS Direct Connect Partner to order a 200 Mbps hosted connection for an existing AWS account.
```

정답 : `D`

- AWS Direct Connect는 콘솔에서 직접 200Mbps 물리적 연결을 생성할 수 없음
	- 콘솔에서 직접 요청 가능한 전용 연결은 1Gbps, 10Gbps, 100Gbps 중 선택
- 200Mbps, 500Mbps 등 세분화도니 속도는 AWS Direct Connect Partner를 통해 제공되는 Hosted Connection을 통해서만 주문 가능
- 회사의 평균 사용률이 10% 미만이므로 1Gbps 전용 연결은 낭비이며, 200Mbps 호스티드 연결로 전환하면 비용을 대폭 절감하면서 보안은 유지

오답 이유

- **A. 1 Gbps Direct Connect 연결을 새로 설정하고 다른 계정과 공유**
    - 여전히 1 Gbps 전용 연결을 유지하므로 **비용 절감 효과 없음**.
    - 공유 기능(Direct Connect Gateway/Resource Access Manager)은 다중 계정에 유용하지만, 문제의 핵심은 **비용 절감**이므로 부적합.
    
- **B. 콘솔에서 200 Mbps Direct Connect 연결 생성**
    - AWS 콘솔에서는 **1, 10, 100 Gbps 전용 연결만 지원**하므로 불가능.
    - 200 Mbps는 **호스티드 연결만 가능**.
    
- **C. Direct Connect 파트너를 통해 1 Gbps 연결 주문 및 공유**
    - 여전히 1 Gbps 회선이므로 **비용 절감 불충분**.
    - 공유는 다계정용일 뿐, 과잉 프로비저닝 문제 해결이 아님.


## #500
한 회사는 온프레미스에 여러 대의 Windows 파일 서버를 보유하고 있습니다. 회사는 파일을 Amazon FSx for Windows File Server 파일 시스템으로 마이그레이션하여 통합하려고 합니다. 액세스 권한이 변경되지 않도록 파일 권한을 보존해야 합니다.

이 요구 사항을 충족하는 솔루션은 무엇입니까? (두 가지 선택)

A. 온프레미스에 AWS DataSync 에이전트를 배포합니다. DataSync 작업을 예약하여 데이터를 FSx for Windows File Server 파일 시스템으로 전송합니다.
B. 각 파일 서버의 공유를 AWS CLI를 사용해 Amazon S3 버킷으로 복사합니다. DataSync 작업을 예약하여 데이터를 FSx for Windows File Server 파일 시스템으로 전송합니다.
C. 각 파일 서버에서 드라이브를 제거합니다. 드라이브를 AWS로 배송하여 Amazon S3로 가져옵니다. DataSync 작업을 예약하여 데이터를 FSx for Windows File Server 파일 시스템으로 전송합니다.
D. AWS Snowcone 디바이스를 주문합니다. 디바이스를 온프레미스 네트워크에 연결합니다. 디바이스에서 AWS DataSync 에이전트를 실행합니다. DataSync 작업을 예약하여 데이터를 FSx for Windows File Server 파일 시스템으로 전송합니다.
E. AWS Snowball Edge Storage Optimized 디바이스를 주문합니다. 디바이스를 온프레미스 네트워크에 연결합니다. AWS CLI를 사용하여 데이터를 디바이스로 복사합니다. 디바이스를 AWS로 배송하여 Amazon S3로 가져옵니다. DataSync 작업을 예약하여 데이터를 FSx for Windows File Server 파일 시스템으로 전송합니다.

```
A company has multiple Windows file servers on premises. The company wants to migrate and consolidate its files into an Amazon FSx for Windows File Server file system. File permissions must be preserved to ensure that access rights do not change.  
  
Which solutions will meet these requirements? (Choose two.)

- A. Deploy AWS DataSync agents on premises. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.
- B. Copy the shares on each file server into Amazon S3 buckets by using the AWS CLI. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.
- C. Remove the drives from each file server. Ship the drives to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.
- D. Order an AWS Snowcone device. Connect the device to the on-premises network. Launch AWS DataSync agents on the device. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.
- E. Order an AWS Snowball Edge Storage Optimized device. Connect the device to the on-premises network. Copy data to the device by using the AWS CLI. Ship the device back to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.
```

정답 : `A, D`

- AWS DataSync는 SMB 소스 ↔ FSx for Windows 대상 전송을 지원하며 NTFS/SMB ACL(권한), 소유권, 타임스탬프 등 메타데이터 보존 가능
- A: 표준 패턴으로 온프레미스에 DataSync 에이전트를 배포해 기존 Windows 파일 공유(SMB) 에서 FSx for Windows로 그대로 마이그레이션하면서 권한을 유지
- D: 네트워크 여건에 따라 Snowcone에 DataSync 에이전트를 구동하여 온프레미스 SMB 공유를 대상으로 DataSync 작업을 수행 가능
	- 연결 품질 제약 시 유용

오답 이유

- **B**: 먼저 **S3**로 복사하면 S3는 오브젝트 스토리지로 **NTFS/SMB ACL을 보존하지 못함**. 이후 DataSync로 FSx로 옮겨도 원래의 파일 권한 정보를 잃습니다.
    
- **C**: 드라이브를 **S3로 임포트**하는 경로 역시 동일하게 **NTFS/SMB 권한 손실**. 권한 보존 요건 불충족.
    
- **E**: Snowball Edge로 **S3로 임포트**한 뒤 DataSync로 FSx로 옮겨도 **S3 단계에서 권한이 소실**됩니다. 권한 보존 요구에 부적합.