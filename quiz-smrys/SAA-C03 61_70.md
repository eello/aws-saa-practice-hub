---
created: 2025-09-25 17:08:09
last_modified: 2025-09-26 18:37:02
---
## #61
한 회사가 AWS에서 2계층 웹 애플리케이션을 개발하고 있습니다.  
회사의 개발자는 애플리케이션을 Amazon EC2 인스턴스에 배포했으며, 이 인스턴스는 백엔드 Amazon RDS 데이터베이스에 직접 연결됩니다.  

회사는 애플리케이션에 데이터베이스 자격 증명을 하드코딩하지 않아야 합니다.  
또한 데이터베이스 자격 증명을 정기적으로 자동으로 교체하는 솔루션을 구현해야 합니다.  

운영 부담이 가장 적은 솔루션은 무엇입니까?

A. 인스턴스 메타데이터에 데이터베이스 자격 증명을 저장합니다. Amazon EventBridge(CloudWatch Events) 규칙을 사용하여 예약된 AWS Lambda 함수를 실행하여 RDS 자격 증명과 인스턴스 메타데이터를 동시에 업데이트합니다.  

B. 암호화된 Amazon S3 버킷에 구성 파일로 데이터베이스 자격 증명을 저장합니다. Amazon EventBridge(CloudWatch Events) 규칙을 사용하여 예약된 AWS Lambda 함수를 실행하여 RDS 자격 증명과 구성 파일의 자격 증명을 동시에 업데이트합니다. S3 버전 관리를 사용하여 이전 값으로 롤백할 수 있도록 합니다.  

C. AWS Secrets Manager에 데이터베이스 자격 증명을 비밀로 저장합니다. 비밀의 자동 교체를 활성화합니다. EC2 역할에 비밀에 접근할 수 있는 권한을 부여합니다.  

D. AWS Systems Manager Parameter Store에 암호화된 매개변수로 데이터베이스 자격 증명을 저장합니다. 암호화된 매개변수의 자동 교체를 활성화합니다. EC2 역할에 암호화된 매개변수에 접근할 수 있는 권한을 부여합니다.

```
A company is developing a two-tier web application on AWS. The company's developers have deployed the application on an Amazon EC2 instance that connects directly to a backend Amazon RDS database. The company must not hardcode database credentials in the application. The company must also implement a solution to automatically rotate the database credentials on a regular basis.  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Store the database credentials in the instance metadata. Use Amazon EventBridge (Amazon CloudWatch Events) rules to run a scheduled AWS Lambda function that updates the RDS credentials and instance metadata at the same time.
- B. Store the database credentials in a configuration file in an encrypted Amazon S3 bucket. Use Amazon EventBridge (Amazon CloudWatch Events) rules to run a scheduled AWS Lambda function that updates the RDS credentials and the credentials in the configuration file at the same time. Use S3 Versioning to ensure the ability to fall back to previous values.
- C. Store the database credentials as a secret in AWS Secrets Manager. Turn on automatic rotation for the secret. Attach the required permission to the EC2 role to grant access to the secret.
- D. Store the database credentials as encrypted parameters in AWS Systems Manager Parameter Store. Turn on automatic rotation for the encrypted parameters. Attach the required permission to the EC2 role to grant access to the encrypted parameters.
```

정답 : `C`

- AWS Secrets Manager는 데이터베이스 자격 증명 자동 회전을 기본적으로 지원
- 람다 등 추가 코드 작성 없이 RDS와 통합하여 자격 증명을 주기적으로 교체 가능
- EC2 역할에 권한만 부여하면 애플리케이션이 Secrets Manager API를 통해 안전하게 자격 증명을 조회 가능

오답 이유

- **A. 인스턴스 메타데이터 사용** — 오답
    - 인스턴스 메타데이터는 민감 정보 저장용이 아님
    - 자격 증명 회전 자동화 및 보안 측면에서 부적합
    
- **B. S3 구성 파일 사용** — 오답
    - 파일 기반 저장은 보안 및 회전 관리가 복잡
    - Lambda를 이용한 스크립트 작성 필요 → 운영 부담 증가
    
- **D. Parameter Store 사용** — 부분적으로 가능하지만 부적합
    - Parameter Store는 암호화된 매개변수 저장 가능
    - 하지만 **자동 회전 기능이 Secrets Manager만큼 완전하게 지원되지 않음**
    - RDS 통합 회전도 별도로 구현 필요 → 운영 부담 증가

## #62
한 회사가 AWS에 새로운 퍼블릭 웹 애플리케이션을 배포하고 있습니다.  
애플리케이션은 Application Load Balancer(ALB) 뒤에서 실행됩니다.  

애플리케이션은 외부 인증 기관(CA)이 발급한 SSL/TLS 인증서를 사용하여 엣지에서 암호화되어야 합니다.  
인증서는 만료되기 전에 매년 교체해야 합니다.  

이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. AWS Certificate Manager(ACM)를 사용하여 SSL/TLS 인증서를 발급합니다. 인증서를 ALB에 적용합니다. 관리형 갱신 기능을 사용하여 인증서를 자동으로 교체합니다.  

B. AWS Certificate Manager(ACM)를 사용하여 SSL/TLS 인증서를 발급합니다. 인증서에서 키 자료를 가져옵니다. 인증서를 ALB에 적용합니다. 관리형 갱신 기능을 사용하여 인증서를 자동으로 교체합니다.  

C. AWS Certificate Manager(ACM) Private Certificate Authority를 사용하여 루트 CA에서 SSL/TLS 인증서를 발급합니다. 인증서를 ALB에 적용합니다. 관리형 갱신 기능을 사용하여 인증서를 자동으로 교체합니다.  

D. AWS Certificate Manager(ACM)를 사용하여 SSL/TLS 인증서를 가져옵니다. 인증서를 ALB에 적용합니다. Amazon EventBridge(CloudWatch Events)를 사용하여 인증서 만료 시 알림을 전송합니다. 인증서를 수동으로 교체합니다.

```
A company is deploying a new public web application to AWS. The application will run behind an Application Load Balancer (ALB). The application needs to be encrypted at the edge with an SSL/TLS certificate that is issued by an external certificate authority (CA). The certificate must be rotated each year before the certificate expires.  
What should a solutions architect do to meet these requirements?

- A. Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.
- B. Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Import the key material from the certificate. Apply the certificate to the ALUse the managed renewal feature to automatically rotate the certificate.
- C. Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS certificate from the root CA. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.
- D. Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate. Apply the certificate to the ALB. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification when the certificate is nearing expiration. Rotate the certificate manually.
```

정답 : `D`

- 외부 CA에서 발급한 인증서 사용
- ACM 관리형 인증서 자동 갱신은 ACM이 발급한 인증서에만 적용
- 따라서 외부 CA 인증서는 ACM에 수동으로 가져오기(import) 해야 하며 자동 갱신 불가
- EventBridge(CloudWatch Events)를 통해 만료 전 알림을 받고, 수동으로 인증서 교체

오답 이유

- **A. ACM 발급 + 자동 갱신** — 오답
    - ACM 관리형 자동 갱신은 **ACM 자체 발급 인증서만 지원**
    - 외부 CA 인증서는 자동 갱신 불가
    
- **B. ACM 발급 후 키 자료 가져오기 + 자동 갱신** — 오답
    - ACM이 발급한 인증서를 가져오는 개념은 불필요
    - 외부 CA 인증서 요구 조건과 맞지 않음
    
- **C. ACM Private CA + 자동 갱신** — 오답
    - Private CA는 내부에서 자체 CA 발급용
    - 문제에서 요구하는 **외부 CA 발급** 조건과 맞지 않음

## #63
한 회사가 AWS에서 인프라를 운영하고 있으며, 문서 관리 애플리케이션을 위한 70만 명의 등록 사용자를 보유하고 있습니다.  
회사는 대용량 .pdf 파일을 .jpg 이미지 파일로 변환하는 제품을 만들고자 합니다.  

.pdf 파일 평균 크기는 5MB입니다.  
회사는 원본 파일과 변환된 파일을 모두 저장해야 합니다.  
솔루션 아키텍트는 시간이 지남에 따라 급격히 증가할 수 있는 수요를 수용할 수 있는 확장 가능한 솔루션을 설계해야 합니다.  

이 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?

A. .pdf 파일을 Amazon S3에 저장합니다. S3 PUT 이벤트를 구성하여 AWS Lambda 함수를 호출하고, 파일을 .jpg 형식으로 변환한 후 Amazon S3에 다시 저장합니다.  

B. .pdf 파일을 Amazon DynamoDB에 저장합니다. DynamoDB Streams 기능을 사용하여 AWS Lambda 함수를 호출하고, 파일을 .jpg 형식으로 변환한 후 DynamoDB에 다시 저장합니다.  

C. .pdf 파일을 Amazon EC2 인스턴스, Amazon Elastic Block Store(EBS) 스토리지 및 Auto Scaling 그룹을 포함하는 AWS Elastic Beanstalk 애플리케이션에 업로드합니다. EC2 인스턴스에서 프로그램을 사용하여 파일을 .jpg 형식으로 변환합니다. .pdf 파일과 .jpg 파일을 EBS 스토리지에 저장합니다.  

D. .pdf 파일을 Amazon EC2 인스턴스, Amazon Elastic File System(EFS) 스토리지 및 Auto Scaling 그룹을 포함하는 AWS Elastic Beanstalk 애플리케이션에 업로드합니다. EC2 인스턴스에서 프로그램을 사용하여 파일을 .jpg 형식으로 변환합니다. .pdf 파일과 .jpg 파일을 EBS 스토리지에 저장합니다.

```
A company runs its infrastructure on AWS and has a registered base of 700,000 users for its document management application. The company intends to create a product that converts large .pdf files to .jpg image files. The .pdf files average 5 MB in size. The company needs to store the original files and the converted files. A solutions architect must design a scalable solution to accommodate demand that will grow rapidly over time.  
Which solution meets these requirements MOST cost-effectively?

- A. Save the .pdf files to Amazon S3. Configure an S3 PUT event to invoke an AWS Lambda function to convert the files to .jpg format and store them back in Amazon S3.
- B. Save the .pdf files to Amazon DynamoDUse the DynamoDB Streams feature to invoke an AWS Lambda function to convert the files to .jpg format and store them back in DynamoDB.
- C. Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic Block Store (Amazon EBS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the files to .jpg format. Save the .pdf files and the .jpg files in the EBS store.
- D. Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic File System (Amazon EFS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the file to .jpg format. Save the .pdf files and the .jpg files in the EBS store.
```

정답 : `A`

- S3 + Lambda 조합은 서버리스 아키텍처로, 확장성 자동 확보 및 관리 부담 최소화
- Lambda S3 PUT 이벤트 기반으로 파일 변환을 자동 처리
- 비용 효율적: 서버리스이므로 사용한 만큼만 비용 발생, EC2/EBS/Elastic Beanstalk 대비 초기 및 운영 비용 절감
- 대용량 파일 변환, 확장성, 원본과 변환본 저장 모두 만족

오답 이유

- **B. DynamoDB + Lambda** — 오답
    - DynamoDB는 **문서 저장용이 아닌 NoSQL 키-값/문서 DB**
    - 5MB PDF를 DynamoDB에 저장하면 **아이템 크기 제한(400KB)**에 걸림
    - 대용량 파일 처리 불가
    
- **C. EC2 + EBS + Beanstalk** — 오답
    - EC2와 Beanstalk 기반이면 서버 프로비저닝 및 관리 필요
    - EBS는 **EC2 단일 AZ 종속** → 고가용성 확보 어렵고 스토리지 비용 상승
    - 서버리스보다 운영 오버헤드 큼
    
- **D. EC2 + EFS + Beanstalk** — 오답
    - EFS는 공유 스토리지로 좋지만 EC2 + Beanstalk와 결합하면 **서버 관리 필요**
    - 비용과 운영 부담이 증가
    - Lambda + S3 대비 불필요하게 복잡


## #64
한 회사는 온프레미스에서 실행되는 Windows 파일 서버에 5TB 이상의 파일 데이터를 보유하고 있습니다.  
사용자와 애플리케이션은 매일 이 데이터와 상호작용합니다.  

회사는 Windows 워크로드를 AWS로 이전하고 있습니다. 이전 과정에서도 회사는 AWS와 온프레미스 파일 스토리지 모두에 **최소 지연(latency)으로 접근**해야 합니다.  
회사는 **운영 오버헤드를 최소화**하고 기존 파일 접근 패턴을 크게 변경하지 않는 솔루션이 필요합니다.  
회사는 AWS와의 연결을 위해 **AWS Site-to-Site VPN**을 사용합니다.  

이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. AWS에서 Amazon FSx for Windows File Server를 배포하고 구성합니다. 온프레미스 파일 데이터를 FSx for Windows File Server로 이동합니다. 워크로드를 AWS FSx for Windows File Server를 사용하도록 재구성합니다.  

B. 온프레미스에 Amazon S3 File Gateway를 배포하고 구성합니다. 온프레미스 파일 데이터를 S3 File Gateway로 이동합니다. 온프레미스 워크로드와 클라우드 워크로드를 S3 File Gateway를 사용하도록 재구성합니다.  

C. 온프레미스에 Amazon S3 File Gateway를 배포하고 구성합니다. 온프레미스 파일 데이터를 Amazon S3로 이동합니다. 각 워크로드 위치에 따라 워크로드가 Amazon S3 또는 S3 File Gateway를 사용하도록 재구성합니다.  

D. AWS에서 Amazon FSx for Windows File Server를 배포하고 구성합니다. 온프레미스에 Amazon FSx File Gateway를 배포하고 구성합니다. 온프레미스 파일 데이터를 FSx File Gateway로 이동합니다. 클라우드 워크로드는 AWS FSx for Windows File Server를 사용하도록, 온프레미스 워크로드는 FSx File Gateway를 사용하도록 구성합니다.

```
A company has more than 5 TB of file data on Windows file servers that run on premises. Users and applications interact with the data each day.  
The company is moving its Windows workloads to AWS. As the company continues this process, the company requires access to AWS and on-premises file storage with minimum latency. The company needs a solution that minimizes operational overhead and requires no significant changes to the existing file access patterns. The company uses an AWS Site-to-Site VPN connection for connectivity to AWS.  
What should a solutions architect do to meet these requirements?

- A. Deploy and configure Amazon FSx for Windows File Server on AWS. Move the on-premises file data to FSx for Windows File Server. Reconfigure the workloads to use FSx for Windows File Server on AWS.
- B. Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to the S3 File Gateway. Reconfigure the on-premises workloads and the cloud workloads to use the S3 File Gateway.
- C. Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to Amazon S3. Reconfigure the workloads to use either Amazon S3 directly or the S3 File Gateway. depending on each workload's location.
- D. Deploy and configure Amazon FSx for Windows File Server on AWS. Deploy and configure an Amazon FSx File Gateway on premises. Move the on-premises file data to the FSx File Gateway. Configure the cloud workloads to use FSx for Windows File Server on AWS. Configure the on-premises workloads to use the FSx File Gateway.
```

정답 : `D`

- FSx for Windows File Server는 윈도우즈 환경과 완벽 호환되는 관리형 파일 스토리지
- FSx File Gateway를 온프레미스에 배포하면 기존 윈도우즈 파일 공유 접근 패턴을 그대로 유지 가능
- Site-to-Site VPN을 통해 온프레미스와 AWS 간 접근이 가능하며 최소 지연과 고가용성 제공
- 관리형 FSx와 File Gateway로 서버 프로비저닝/패치 필요 없음

오답 이유

- **A. FSx 단독 사용** — 오답
    - 클라우드에만 FSx를 배포하면 온프레미스 접근 시 VPN 지연 발생 가능
    - “최소 지연” 요구사항을 충족하지 못함
    
- **B. S3 File Gateway 단독 사용** — 오답
    - S3는 객체 스토리지이며 Windows 파일 서버 호환이 제한적
    - 기존 애플리케이션의 파일 접근 패턴(파일 공유, SMB/NFS)과 호환되지 않을 수 있음
    
- **C. S3 File Gateway + S3** — 오답
    - 온프레미스 애플리케이션이 파일 공유를 바로 사용하기 어렵고, 객체 스토리지 특성상 파일 수정 시 지연 발생
    - 최소 지연 요구사항 및 운영 효율성 미충족


## #65
한 병원이 최근 Amazon API Gateway와 AWS Lambda를 사용하여 RESTful API를 배포했습니다.  
병원은 API Gateway와 Lambda를 사용하여 PDF 형식 및 JPEG 형식의 보고서를 업로드합니다.  

병원은 Lambda 코드를 수정하여 보고서에서 **보호 건강 정보(PHI)**를 식별할 수 있어야 합니다.  

이 요구 사항을 가장 **운영 오버헤드가 적게** 충족하는 솔루션은 무엇입니까?

A. 기존 Python 라이브러리를 사용하여 보고서에서 텍스트를 추출하고, 추출한 텍스트에서 PHI를 식별합니다.  
B. Amazon Textract를 사용하여 보고서에서 텍스트를 추출합니다. Amazon SageMaker를 사용하여 추출된 텍스트에서 PHI를 식별합니다.  
C. Amazon Textract를 사용하여 보고서에서 텍스트를 추출합니다. Amazon Comprehend Medical을 사용하여 추출된 텍스트에서 PHI를 식별합니다.  
D. Amazon Rekognition을 사용하여 보고서에서 텍스트를 추출합니다. Amazon Comprehend Medical을 사용하여 추출된 텍스트에서 PHI를 식별합니다.

```
A hospital recently deployed a RESTful API with Amazon API Gateway and AWS Lambda. The hospital uses API Gateway and Lambda to upload reports that are in PDF format and JPEG format. The hospital needs to modify the Lambda code to identify protected health information (PHI) in the reports.  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Use existing Python libraries to extract the text from the reports and to identify the PHI from the extracted text.
- B. Use Amazon Textract to extract the text from the reports. Use Amazon SageMaker to identify the PHI from the extracted text.
- C. Use Amazon Textract to extract the text from the reports. Use Amazon Comprehend Medical to identify the PHI from the extracted text.
- D. Use Amazon Rekognition to extract the text from the reports. Use Amazon Comprehend Medical to identify the PHI from the extracted text.
```

정답 : `C`

- Amazon Textract: PDF 및 JPEG 문서에서 구조화된 텍스트를 자동으로 추출
- Amazon Comprehend Medical : 추출된 텍스트에서 의료 관련 PHI(환자 이름, 진단, 처방 등)를 자동 식별
- 서버리스 환경(AWS 람다)과 완전관리형 서비스 활용으로 운영 오버헤드 최소화
- 기존 파이썬 라이브러리를 사용한 수동 처리(A)는 직접 구현과 유지보수가 필요하므로 오버헤드가 높음


오답 이유

- **A. 기존 Python 라이브러리 사용** — 오답
    - 텍스트 추출과 PHI 식별 알고리즘을 직접 구현해야 하므로 개발 및 운영 오버헤드 큼
    
- **B. Textract + SageMaker** — 오답
    - SageMaker로 PHI 모델을 직접 학습시켜야 하므로 개발·학습 비용 및 관리 오버헤드가 큼
    - 요구사항에서 “운영 오버헤드 최소화” 조건 미충족
    
- **D. Rekognition + Comprehend Medical** — 오답
    - Rekognition은 주로 이미지/비디오 분석용이며, 텍스트 추출 정확도가 Textract보다 낮음        
    - PDF 지원이 제한적이므로 문서 처리에는 적합하지 않음

## #66
한 회사는 애플리케이션에서 많은 파일을 생성하며, 각 파일 크기는 약 5MB입니다.  
파일은 Amazon S3에 저장됩니다. 회사 정책상 파일은 삭제되기 전에 4년 동안 저장되어야 합니다.  
파일은 중요한 비즈니스 데이터를 포함하고 있으며, 재생산이 쉽지 않으므로 항상 즉시 접근 가능해야 합니다.  

파일은 객체 생성 후 처음 30일 동안 자주 접근되지만, 이후에는 거의 접근되지 않습니다.  

가장 **비용 효율적인 스토리지 솔루션**은 무엇입니까?

A. S3 버킷 수명 주기 정책을 생성하여 객체 생성 후 30일이 지나면 파일을 S3 Standard에서 S3 Glacier로 이동합니다. 객체 생성 후 4년 후 파일을 삭제합니다.  

B. S3 버킷 수명 주기 정책을 생성하여 객체 생성 후 30일이 지나면 파일을 S3 Standard에서 S3 One Zone-Infrequent Access(S3 One Zone-IA)로 이동합니다. 객체 생성 후 4년 후 파일을 삭제합니다.  

C. S3 버킷 수명 주기 정책을 생성하여 객체 생성 후 30일이 지나면 파일을 S3 Standard에서 S3 Standard-Infrequent Access(S3 Standard-IA)로 이동합니다. 객체 생성 후 4년 후 파일을 삭제합니다.  

D. S3 버킷 수명 주기 정책을 생성하여 객체 생성 후 30일이 지나면 파일을 S3 Standard에서 S3 Standard-Infrequent Access(S3 Standard-IA)로 이동합니다. 객체 생성 후 4년 후 파일을 S3 Glacier로 이동합니다.

```
A company has an application that generates a large number of files, each approximately 5 MB in size. The files are stored in Amazon S3. Company policy requires the files to be stored for 4 years before they can be deleted. Immediate accessibility is always required as the files contain critical business data that is not easy to reproduce. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days.  
Which storage solution is MOST cost-effective?

- A. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Glacier 30 days from object creation. Delete the files 4 years after object creation.
- B. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days from object creation. Delete the files 4 years after object creation.
- C. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Delete the files 4 years after object creation.
- D. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Move the files to S3 Glacier 4 years after object creation.
```

정답 : `C`

- 객체는 항상 접근 가능해야하고 생성 후 30일 동안 자주 접근 -> S3 Standard 사용
- 이후로는 드물게 접근 -> S3 Standard-IA로 이동
- Standard-IA는 접근 빈도가 낮은 객체에 대해 비용 효율적이면서 즉시 접근 가능
- 4년 후 삭제 정책을 적용하면 규정 준수 가능
- Glacier는 즉시 접근 불가이므로 정책상 요구사항과 맞지 않음

오답 이유

- **A. Standard → Glacier** — 오답
    - Glacier는 **즉시 접근 불가**, 객체가 중요하고 즉시 접근 필요하므로 부적합
    
- **B. Standard → One Zone-IA** — 오답
    - One Zone-IA는 단일 AZ에 저장 → 내구성은 낮음(99.5%)
    - 중요한 비즈니스 데이터이고 재생산이 어려움 → 내구성 우선시
    
- **D. Standard → Standard-IA → Glacier** — 오답
    - 4년 후 Glacier로 이동하면 즉시 접근 불가
    - 정책상 요구사항(항상 즉시 접근 가능)에 맞지 않음


## #67
한 회사는 여러 Amazon EC2 인스턴스에서 애플리케이션을 호스팅합니다.  
이 애플리케이션은 Amazon SQS 큐에서 메시지를 처리하고, Amazon RDS 테이블에 기록하며, 큐에서 메시지를 삭제합니다.  

가끔 RDS 테이블에 **중복 레코드**가 발견됩니다.  
SQS 큐에는 중복 메시지가 없습니다.  

메시지가 **한 번만 처리되도록** 보장하려면, 솔루션 아키텍트는 무엇을 해야 합니까?

A. CreateQueue API 호출을 사용하여 새 큐를 생성합니다.  
B. AddPermission API 호출을 사용하여 적절한 권한을 추가합니다.  
C. ReceiveMessage API 호출을 사용하여 적절한 대기 시간을 설정합니다.  
D. ChangeMessageVisibility API 호출을 사용하여 가시성 타임아웃(visibility timeout)을 늘립니다.

```
A company hosts an application on multiple Amazon EC2 instances. The application processes messages from an Amazon SQS queue, writes to an Amazon RDS table, and deletes the message from the queue. Occasional duplicate records are found in the RDS table. The SQS queue does not contain any duplicate messages.  
What should a solutions architect do to ensure messages are being processed once only?

- A. Use the CreateQueue API call to create a new queue.
- B. Use the AddPermission API call to add appropriate permissions.
- C. Use the ReceiveMessage API call to set an appropriate wait time.
- D. Use the ChangeMessageVisibility API call to increase the visibility timeout.
```

정답 : `D`

- SQS 메시지를 EC2 인스턴스가 읽으면 가시성 타임아웃(visibility timeout) 동안 다른 소비자가 해당 메시지를 처리하지 못하게 막음
- RDS에 메시지를 기록하는 동안 처리가 완료되기 전에 가시성 타임아웃이 짧으면 SQS는 메시지를 다시 보낼 수 있음
- 이로 인해 중복 레코드가 발생
- 가시성 타임아웃을 충분히 늘려 처리 완료 후 메시지를 삭제하도록 하면 메시지의 중복 처리 방지 가능


오답 이유

- **A. CreateQueue API 호출** — 오답
    - 새 큐를 만드는 것은 중복 처리 문제와 관련 없음.
    
- **B. AddPermission API 호출** — 오답
    - 권한 설정은 메시지 처리 실패나 중복 문제와 관련이 없음.
    
- **C. ReceiveMessage API 호출** — 오답
    - 대기 시간을 설정하는 것은 **폴링 지연(long polling)**과 관련 있음
    - 중복 레코드 문제 해결에는 직접적인 영향 없음

## #68
솔루션 아키텍트가 회사의 온프레미스 인프라를 AWS로 확장하는 새로운 하이브리드 아키텍처를 설계하고 있습니다.  
회사는 AWS 리전과 **일관된 낮은 지연 시간**을 가진 **고가용성 연결**이 필요합니다.  
회사는 비용을 최소화하고 싶으며, **주 연결이 실패할 경우 트래픽 속도가 느려지는 것을 허용**할 의향이 있습니다.  

솔루션 아키텍트는 이러한 요구사항을 충족하기 위해 무엇을 해야 합니까?

A. AWS Direct Connect 연결을 리전에 프로비저닝합니다. 주 Direct Connect 연결이 실패할 경우 백업으로 VPN 연결을 프로비저닝합니다.  

B. 프라이빗 연결을 위해 리전에 VPN 터널 연결을 프로비저닝합니다. 주 VPN 연결이 실패할 경우 백업으로 두 번째 VPN 터널을 프로비저닝합니다.  

C. AWS Direct Connect 연결을 리전에 프로비저닝합니다. 주 Direct Connect 연결이 실패할 경우 백업으로 동일 리전에 두 번째 Direct Connect 연결을 프로비저닝합니다.  

D. AWS Direct Connect 연결을 리전에 프로비저닝합니다. AWS CLI의 Direct Connect failover 속성을 사용하여 주 Direct Connect 연결이 실패하면 자동으로 백업 연결을 생성합니다.

```
A solutions architect is designing a new hybrid architecture to extend a company's on-premises infrastructure to AWS. The company requires a highly available connection with consistent low latency to an AWS Region. The company needs to minimize costs and is willing to accept slower traffic if the primary connection fails.  
What should the solutions architect do to meet these requirements?

- A. Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup if the primary Direct Connect connection fails.
- B. Provision a VPN tunnel connection to a Region for private connectivity. Provision a second VPN tunnel for private connectivity and as a backup if the primary VPN connection fails.
- C. Provision an AWS Direct Connect connection to a Region. Provision a second Direct Connect connection to the same Region as a backup if the primary Direct Connect connection fails.
- D. Provision an AWS Direct Connect connection to a Region. Use the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection if the primary Direct Connect connection fails.
```

정답 : `A`

- 일관된 낮은 지연 시간 필요 -> AWS Direct Connect가 기본 연결로 적합
- 비용 최소화와 주 연결 실패 시 느린 연결 허용 -> 백연 연결로는 저비용의 VPN 연결 설정
- 이러한 구성 = Direct Connect + VPN failover 아키텍처 -> 일반적인 하이브리드 환경에서 비용 효율적이고 고가용성을 제공


## #69
회사는 Amazon EC2 인스턴스 뒤에서 Application Load Balancer를 사용하는 비즈니스 핵심 웹 애플리케이션을 실행하고 있습니다.  
EC2 인스턴스는 Auto Scaling 그룹에 속해 있습니다.  
애플리케이션은 단일 가용 영역에 배포된 Amazon Aurora PostgreSQL 데이터베이스를 사용합니다.  

회사는 애플리케이션을 **최소 다운타임과 최소 데이터 손실**로 **고가용성**으로 만들고자 합니다.  

이 요구사항을 **가장 적은 운영 노력**으로 충족하려면 어떤 솔루션을 선택해야 합니까?

A. EC2 인스턴스를 서로 다른 AWS 리전에 배치합니다. Amazon Route 53 헬스 체크를 사용하여 트래픽을 리디렉션합니다. Aurora PostgreSQL 교차 리전 복제를 사용합니다.  

B. Auto Scaling 그룹이 여러 가용 영역을 사용하도록 구성합니다. 데이터베이스를 Multi-AZ로 구성합니다. 데이터베이스에 Amazon RDS Proxy 인스턴스를 구성합니다.  

C. Auto Scaling 그룹이 한 가용 영역만 사용하도록 구성합니다. 데이터베이스의 스냅샷을 매시간 생성합니다. 장애 발생 시 스냅샷에서 데이터베이스를 복구합니다.  

D. Auto Scaling 그룹이 여러 AWS 리전을 사용하도록 구성합니다. 애플리케이션에서 Amazon S3로 데이터를 기록합니다. S3 이벤트 알림을 사용하여 데이터를 데이터베이스에 기록하는 AWS Lambda 함수를 실행합니다.

```
A company is running a business-critical web application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances are in an Auto Scaling group. The application uses an Amazon Aurora PostgreSQL database that is deployed in a single Availability Zone. The company wants the application to be highly available with minimum downtime and minimum loss of data.  
Which solution will meet these requirements with the LEAST operational effort?

- A. Place the EC2 instances in different AWS Regions. Use Amazon Route 53 health checks to redirect traffic. Use Aurora PostgreSQL Cross-Region Replication.
- B. Configure the Auto Scaling group to use multiple Availability Zones. Configure the database as Multi-AZ. Configure an Amazon RDS Proxy instance for the database.
- C. Configure the Auto Scaling group to use one Availability Zone. Generate hourly snapshots of the database. Recover the database from the snapshots in the event of a failure.
- D. Configure the Auto Scaling group to use multiple AWS Regions. Write the data from the application to Amazon S3. Use S3 Event Notifications to launch an AWS Lambda function to write the data to the database.
```

정답 : `B`

- 웹 서버 계층 : 오토 스케일링 그룹을 여러 가용 영역(AZ)에 배포하면 특정 AZ 장애 시에도 트래픽이 다른 AZ로 라우팅되어 최소 다운타임 달성 가능
- 데이터베이스 계층 : 오로라 데이터베이스를 멀티 AZ 배포로 구성하면 기본 인스턴스 장애 시 자동으로 장애 조치가 발생해 데이터 손실 최소화
- RDS Proxy : 데이터베이스 연결 관리와 장애 조치 시 애플리케이션 연결 재사용을 지원하여 운영 부담 감소

오답 이유

- **A. 서로 다른 AWS 리전 + Cross-Region Replication**
    - 장점: 리전 장애 대비 가능
    - 단점: 복잡한 아키텍처, Route 53 헬스 체크 및 트래픽 관리 필요, 지연 시간 증가 가능, 운영 노력 큼
    
- **C. 한 AZ + 매시간 스냅샷**
    - 장점: 단순
    - 단점: AZ 장애 시 서비스 중단 발생, 데이터 손실 가능 (최대 1시간치), 다운타임 길어짐 → 요구사항 충족 못함
    
- **D. 여러 리전 + S3 이벤트 → Lambda → DB 기록**
    - 장점: 글로벌 확장 가능
    - 단점: 애플리케이션 로직 변경 필요, 지연 시간 증가, 복잡한 이벤트 기반 처리 → 최소 운영 노력 아님


## #70
회사의 HTTP 애플리케이션이 Network Load Balancer(NLB) 뒤에서 실행되고 있습니다.  
NLB의 대상 그룹은 웹 서비스를 실행하는 여러 EC2 인스턴스로 구성된 Amazon EC2 Auto Scaling 그룹을 사용하도록 설정되어 있습니다.  

회사는 NLB가 애플리케이션의 HTTP 오류를 감지하지 못하는 것을 발견했습니다.  
이러한 오류가 발생하면 웹 서비스를 실행하는 EC2 인스턴스를 수동으로 재시작해야 합니다.  

회사는 **커스텀 스크립트나 코드를 작성하지 않고** 애플리케이션의 가용성을 개선하고자 합니다.  
이 요구사항을 충족하려면 어떤 솔루션을 선택해야 합니까?

A. NLB에서 HTTP 헬스 체크를 활성화하고 회사 애플리케이션의 URL을 제공합니다.  

B. EC2 인스턴스에 크론 작업을 추가하여 로컬 애플리케이션 로그를 매분 확인합니다. HTTP 오류가 감지되면 애플리케이션을 재시작합니다.  

C. NLB를 Application Load Balancer(ALB)로 교체합니다. 회사 애플리케이션의 URL을 제공하여 HTTP 헬스 체크를 활성화합니다. Auto Scaling 동작을 구성하여 비정상 인스턴스를 교체합니다.  

D. NLB의 UnhealthyHostCount 지표를 모니터링하는 Amazon CloudWatch 알람을 생성합니다. 알람 상태가 ALARM일 때 비정상 인스턴스를 교체하도록 Auto Scaling 동작을 구성합니다.

```
A company's HTTP application is behind a Network Load Balancer (NLB). The NLB's target group is configured to use an Amazon EC2 Auto Scaling group with multiple EC2 instances that run the web service.  
The company notices that the NLB is not detecting HTTP errors for the application. These errors require a manual restart of the EC2 instances that run the web service. The company needs to improve the application's availability without writing custom scripts or code.  
What should a solutions architect do to meet these requirements?

- A. Enable HTTP health checks on the NLB, supplying the URL of the company's application.
- B. Add a cron job to the EC2 instances to check the local application's logs once each minute. If HTTP errors are detected. the application will restart.
- C. Replace the NLB with an Application Load Balancer. Enable HTTP health checks by supplying the URL of the company's application. Configure an Auto Scaling action to replace unhealthy instances.
- D. Create an Amazon Cloud Watch alarm that monitors the UnhealthyHostCount metric for the NLB. Configure an Auto Scaling action to replace unhealthy instances when the alarm is in the ALARM state.
```

정답 : `C`

- NLB는 TCP 수준의 헬스 체크만 지원하므로 HTTP 상태 코드 기반의 오류 감지가 불가능 -> 웹 애플리케이션의 HTTP 오류를 자동 감지 불가능
- ALB는 HTTP/HTTPS 기반 헬스 체크를 지원하며, 지정한 URL을 통해 실제 애플리케이션 상태를 확인
- ALB와 오토스케일링을 함께 사용하면 비정상 인스턴스가 감지되면 자동으로 교체되므로 운영자가 수동으로 재시작할 필요 없음

오답 이유

- **A. NLB에서 HTTP 헬스 체크 활성화**
    - NLB는 **TCP/UDP 수준 헬스 체크만 지원**하며, HTTP 상태 코드 기반 오류 감지는 불가 → 요구사항 불충분
    
- **B. 크론 작업으로 로그 확인 후 재시작**
    - 수동 스크립트 작성 필요, 운영 부담 큼 → “커스텀 스크립트 없이” 조건 불충족
    
- **D. CloudWatch 알람 + Auto Scaling**
    - NLB UnhealthyHostCount는 **TCP 헬스 체크 실패만 감지** 가능
    - HTTP 오류는 감지 불가 → 애플리케이션 상태를 정확히 반영하지 못함