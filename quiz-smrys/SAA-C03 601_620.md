---
created: 2025-10-17 17:02:34
last_modified: 2025-10-18 11:08:46
---
## #601
한 회사가 중요한 데이터베이스를 Amazon RDS for PostgreSQL DB 인스턴스에서 운영하고 있습니다. 회사는 최소한의 다운타임과 데이터 손실로 Amazon Aurora PostgreSQL로 마이그레이션하고자 합니다.

다음 중 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. RDS for PostgreSQL DB 인스턴스의 DB 스냅샷을 생성하여 새로운 Aurora PostgreSQL DB 클러스터를 채웁니다.
B. RDS for PostgreSQL DB 인스턴스의 Aurora 읽기 전용 복제본을 생성합니다. Aurora 읽기 전용 복제본을 새로운 Aurora PostgreSQL DB 클러스터로 승격합니다.
C. Amazon S3에서 데이터 가져오기를 사용하여 데이터베이스를 Aurora PostgreSQL DB 클러스터로 마이그레이션합니다.
D. pg_dump 유틸리티를 사용하여 RDS for PostgreSQL 데이터베이스를 백업합니다. 백업을 새로운 Aurora PostgreSQL DB 클러스터로 복원합니다.

```
A company runs its critical database on an Amazon RDS for PostgreSQL DB instance. The company wants to migrate to Amazon Aurora PostgreSQL with minimal downtime and data loss.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Create a DB snapshot of the RDS for PostgreSQL DB instance to populate a new Aurora PostgreSQL DB cluster.
- B. Create an Aurora read replica of the RDS for PostgreSQL DB instance. Promote the Aurora read replicate to a new Aurora PostgreSQL DB cluster.
- C. Use data import from Amazon S3 to migrate the database to an Aurora PostgreSQL DB cluster.
- D. Use the pg_dump utility to back up the RDS for PostgreSQL database. Restore the backup to a new Aurora PostgreSQL DB cluster.
```

정답 : `B`

- RDS for PostgreSQL로부터 Aurora 읽기 전용 복제본을 생성하면 지속적인 복제를 통해 최신 상태를 유지하다가 전환 시점에 복제본을 승격(Promote)하여 Aurora 클러스터로 전환 가능

오답 이유

- A. 스냅샷으로 복원하는 방식은 정적 시점 복사이므로 최신 변경 내용이 반영되지 않으며, 복원 및 전환 동안 다운타임이 발생합니다.

- C. S3 가져오기는 대량 적재용 경로일 뿐, 운영 중인 트랜잭션 DB의 최소 다운타임 마이그레이션에는 적합하지 않습니다. 증분 동기화가 없어 데이터 손실/정합성 이슈가 생길 수 있습니다.

- D. pg_dump/pg_restore는 논리적 내보내기/가져오기 방식으로 시간이 오래 걸리고, 대규모/트래픽 중 DB에서는 다운타임이 커집니다. 또한 증분 동기화가 없어 데이터 손실을 최소화하기 어렵습니다.


## #602
한 회사의 인프라는 Amazon Elastic Block Store(Amazon EBS) 스토리지를 사용하는 수백 개의 Amazon EC2 인스턴스로 구성되어 있습니다. 솔루션스 아키텍트는 모든 EC2 인스턴스가 재해 이후 복구될 수 있도록 보장해야 합니다.

가장 적은 노력으로 이 요구 사항을 충족하려면 무엇을 해야 합니까?

A. 각 EC2 인스턴스에 연결된 EBS 스토리지의 스냅샷을 생성합니다. EBS 스토리지에서 새 EC2 인스턴스를 시작하기 위해 AWS CloudFormation 템플릿을 생성합니다.
B. 각 EC2 인스턴스에 연결된 EBS 스토리지의 스냅샷을 생성합니다. EC2 템플릿을 기반으로 환경을 설정하고 EBS 스토리지를 연결하기 위해 AWS Elastic Beanstalk을 사용합니다.
C. 모든 EC2 인스턴스 그룹에 대한 백업 계획을 설정하기 위해 AWS Backup을 사용합니다. 여러 EC2 인스턴스의 복구 프로세스를 가속화하기 위해 AWS Backup API 또는 AWS CLI를 사용합니다.
D. 각 EC2 인스턴스에 연결된 EBS 스토리지의 스냅샷을 생성하고 Amazon Machine Image(AMI)를 복사하는 AWS Lambda 함수를 생성합니다. 복사된 AMI로 복구를 수행하고 EBS 스토리지를 연결하는 또 다른 Lambda 함수를 생성합니다.

```
A company's infrastructure consists of hundreds of Amazon EC2 instances that use Amazon Elastic Block Store (Amazon EBS) storage. A solutions architect must ensure that every EC2 instance can be recovered after a disaster.  
  
What should the solutions architect do to meet this requirement with the LEAST amount of effort?

- A. Take a snapshot of the EBS storage that is attached to each EC2 instance. Create an AWS CloudFormation template to launch new EC2 instances from the EBS storage.
- B. Take a snapshot of the EBS storage that is attached to each EC2 instance. Use AWS Elastic Beanstalk to set the environment based on the EC2 template and attach the EBS storage.
- C. Use AWS Backup to set up a backup plan for the entire group of EC2 instances. Use the AWS Backup API or the AWS CLI to speed up the restore process for multiple EC2 instances.
- D. Create an AWS Lambda function to take a snapshot of the EBS storage that is attached to each EC2 instance and copy the Amazon Machine Images (AMIs). Create another Lambda function to perform the restores with the copied AMIs and attach the EBS storage.
```

정답 : `C`

- AWS Backup은 EC2/EBS에 대해 중앙집중식으로 백업 정책을 정의하고 대규모로 일관 관리･복구할 수 있는 완전관리형 서비스
- 수백 대 인스턴스를 대상으로 최소환의 운영 노력으로 자동 백업과 대량 복구를 수행할 수 있어 요구사항에 가장 적합

오답 이유

- A. 스냅샷과 개별 CloudFormation 템플릿으로 복구 절차를 구성하면 인스턴스 수가 많을수록 템플릿 관리/갱신이 복잡해집니다. 대량 복구 자동화에 비효율적입니다.

- B. Elastic Beanstalk은 애플리케이션 배포 플랫폼으로, 임의의 기존 EC2/EBS 조합 복구를 표준화해 주지 않습니다. 또한 Beanstalk 환경으로의 전환이 전제되어야 해 부적합합니다.

- D. 스냅샷/AMI 복사 및 복구 오케스트레이션을 Lambda로 직접 구현하는 것은 유지보수와 오류 처리, 권한/예외 케이스 대응 등 운영 오버헤드가 큽니다. 동일 목적을 AWS Backup이 네이티브로 제공합니다.


## #603
한 회사가 최근 AWS 클라우드로 마이그레이션했습니다. 회사는 반정형(semistructured) 데이터셋에 대해 대규모 병렬 온디맨드 처리(serverless)를 원합니다. 데이터는 로그, 미디어 파일, 판매 트랜잭션, IoT 센서 데이터로 구성되며 Amazon S3에 저장되어 있습니다. 회사는 이 솔루션이 데이터셋의 수천 개 항목을 병렬로 처리하길 원합니다.

다음 중 가장 높은 운영 효율성으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. AWS Step Functions의 Map 상태를 Inline 모드로 사용하여 병렬로 데이터를 처리합니다.
B. AWS Step Functions의 Map 상태를 Distributed 모드로 사용하여 병렬로 데이터를 처리합니다.
C. AWS Glue를 사용하여 병렬로 데이터를 처리합니다.
D. 여러 AWS Lambda 함수를 사용하여 병렬로 데이터를 처리합니다.

```
A company recently migrated to the AWS Cloud. The company wants a serverless solution for large-scale parallel on-demand processing of a semistructured dataset. The data consists of logs, media files, sales transactions, and IoT sensor data that is stored in Amazon S3. The company wants the solution to process thousands of items in the dataset in parallel.  
  
Which solution will meet these requirements with the MOST operational efficiency?

- A. Use the AWS Step Functions Map state in Inline mode to process the data in parallel.
- B. Use the AWS Step Functions Map state in Distributed mode to process the data in parallel.
- C. Use AWS Glue to process the data in parallel.
- D. Use several AWS Lambda functions to process the data in parallel.
```

정답 : `B`

- Step Functions Distributed Map은 S3 객체 목록 등을 직접 입력으로 받아 수천~수백만 건 규모의 고도 병렬 처리를 서버리스로 오케스트레이션
- 분산 맵은 워크로드를 자동으로 샤딩해 병렬 실행을 확장하며, 로그/미디어/트랜잭션/센서 데이터처럼 항목 단위 처리에 최적화

오답 이유

- A. Inline 모드 Map은 표준 상태 머신 내부 병렬 처리로 규모와 동시성에 상대적 제약이 큽니다. 대규모(수천~수백만 건) 병렬 처리에는 Distributed 모드가 더 적합합니다.

- C. AWS Glue는 ETL/데이터 엔지니어링에 최적화되어 있고 스키마 중심 처리에 강점이 있으나, 미디어 파일 등 임의 항목 처리 오케스트레이션에는 과하고 설계가 번거로울 수 있습니다.

- D. Lambda를 여러 개 배치하는 것만으로는 **대규모 병렬 오케스트레이션/재시도/에러 처리/추적**을 관리하기 어렵습니다. 별도의 큐/조정 로직이 필요해 운영 오버헤드가 커집니다.


## #604
회사는 6주 안에 10PB의 데이터를 Amazon S3로 마이그레이션할 예정입니다. 현재 데이터 센터의 인터넷 업링크는 500Mbps입니다. 다른 온프레미스 애플리케이션들이 이 업링크를 공유합니다. 회사는 이 일회성 마이그레이션 작업을 위해 인터넷 대역폭의 80%를 사용할 수 있습니다.

다음 중 어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. AWS DataSync을 구성하여 데이터를 Amazon S3로 마이그레이션하고 데이터를 자동으로 검증합니다.
B. rsync를 사용하여 데이터를 직접 Amazon S3로 전송합니다.
C. AWS CLI와 여러 복사 프로세스를 사용하여 데이터를 직접 Amazon S3로 보냅니다.
D. 여러 AWS Snowball 디바이스를 주문합니다. 데이터를 디바이스에 복사합니다. 디바이스를 AWS로 보내 AWS가 데이터를 Amazon S3로 복사하게 합니다.

```
A company will migrate 10 PB of data to Amazon S3 in 6 weeks. The current data center has a 500 Mbps uplink to the internet. Other on-premises applications share the uplink. The company can use 80% of the internet bandwidth for this one-time migration task.  
  
Which solution will meet these requirements?

- A. Configure AWS DataSync to migrate the data to Amazon S3 and to automatically verify the data.
- B. Use rsync to transfer the data directly to Amazon S3.
- C. Use the AWS CLI and multiple copy processes to send the data directly to Amazon S3.
- D. Order multiple AWS Snowball devices. Copy the data to the devices. Send the devices to AWS to copy the data to Amazon S3.
```

정답 : `D`

- 사용할 수 있는 네트워크 대욕폭은 500Mbps x 80% = 400Mbps(= 약 50MB/s)
- 6주(3,628,800초)동안 전송 가능한 최대 데이터는 약 181TB 수준에 불과해 10TB를 마감 내 전송 불가
- 오프라인 대용량 전송인 AWS Snowball을 여러 대 병렬로 사용하면 네트워크 병목 없이 PB 규모 데이터를 6주 내 S3로 이관 가능

오답 이유

- A. DataSync은 전송 자동화와 무결성 검증에 유리하지만, 가용 대역폭(약 400Mbps) 제약 때문에 6주 내 10PB 전송이 물리적으로 불가능합니다.

- B. rsync로 직접 업로드해도 네트워크 한계는 동일합니다. 전송 시간 요건을 충족하지 못합니다.

- C. AWS CLI로 병렬 복사해도 링크 용량을 넘을 수 없으므로 총 소요 시간 문제를 해결하지 못합니다.


## #605
한 회사에 여러 온프레미스 iSCSI(Internet Small Computer Systems Interface) 네트워크 스토리지 서버가 있습니다. 회사는 AWS 클라우드로 이전하여 이러한 서버 수를 줄이려고 합니다. 솔루션스 아키텍트는 자주 사용되는 데이터에 대해 낮은 지연 시간 액세스를 제공하고, 최소한의 인프라 변경으로 온프레미스 서버에 대한 의존도를 줄여야 합니다.

다음 중 어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. Amazon S3 File Gateway를 배포합니다.
B. Amazon Elastic Block Store(Amazon EBS) 스토리지를 배포하고 Amazon S3로 백업합니다.
C. 저장형(Stored) 볼륨으로 구성된 AWS Storage Gateway 볼륨 게이트웨이를 배포합니다.
D. 캐시형(Cached) 볼륨으로 구성된 AWS Storage Gateway 볼륨 게이트웨이를 배포합니다.

```
A company has several on-premises Internet Small Computer Systems Interface (ISCSI) network storage servers. The company wants to reduce the number of these servers by moving to the AWS Cloud. A solutions architect must provide low-latency access to frequently used data and reduce the dependency on on-premises servers with a minimal number of infrastructure changes.  
  
Which solution will meet these requirements?

- A. Deploy an Amazon S3 File Gateway.
- B. Deploy Amazon Elastic Block Store (Amazon EBS) storage with backups to Amazon S3.
- C. Deploy an AWS Storage Gateway volume gateway that is configured with stored volumes.
- D. Deploy an AWS Storage Gateway volume gateway that is configured with cached volumes.
```

정답 : `D`

- Storage Gateway Volume Gateway(캐시형, Cached Volume)는 주 데이터(Pirmary)를 S3에 보관
- 로컬에 자주 쓰는 데이터만 캐시하여 iSCSI로 제공하므로 빈번 접근 데이터에 대해 저지연을 제공하면서 온프레미스 스토리지 의존도를 줄임
- iSCSI 클라이언트(서버) 입장에서는 LUN을 그대로 마운트하듯 사용하므로 인프라 변경이 최소화

오답 이유

- A. S3 File Gateway는 SMB/NFS 파일 공유를 제공하는 게이트웨이로, iSCSI 블록 스토리지 대체가 아닙니다. 기존 iSCSI 기반 워크로드에 적합하지 않습니다.

- B. EBS는 EC2에 연결되는 블록 스토리지로 온프레미스 서버에서 직접 사용할 수 없습니다. 온프레미스 의존도를 줄이거나 낮은 지연 액세스 요구를 충족하지 못합니다.

- C. Stored Volumes는 주 데이터를 온프레미스에 유지하고 S3로 비동기 복제합니다. 로컬 저지연은 가능하지만 **온프레미스 스토리지 의존도를 줄이지 못해** 문제 요구사항과 상충합니다.


## #606
솔루션스 아키텍트가 비즈니스 사용자가 객체를 Amazon S3에 업로드할 수 있는 애플리케이션을 설계하고 있습니다. 솔루션은 객체 내구성을 최대화해야 합니다. 또한 객체는 언제든지, 그리고 얼마나 오랫동안이든 즉시 사용할 수 있어야 합니다. 사용자는 업로드 후 첫 30일 동안 객체에 자주 액세스하지만, 30일이 지난 객체에 대해서는 액세스 가능성이 훨씬 낮습니다.

다음 중 가장 비용 효율적으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 모든 객체를 S3 Standard에 저장하고, 30일 후에 S3 Glacier로 전환하는 S3 수명 주기 규칙을 설정합니다.
B. 모든 객체를 S3 Standard에 저장하고, 30일 후에 S3 Standard-Infrequent Access(S3 Standard-IA)로 전환하는 S3 수명 주기 규칙을 설정합니다.
C. 모든 객체를 S3 Standard에 저장하고, 30일 후에 S3 One Zone-Infrequent Access(S3 One Zone-IA)로 전환하는 S3 수명 주기 규칙을 설정합니다.
D. 모든 객체를 S3 Intelligent-Tiering에 저장하고, 30일 후에 S3 Standard-Infrequent Access(S3 Standard-IA)로 전환하는 S3 수명 주기 규칙을 설정합니다.

```
A solutions architect is designing an application that will allow business users to upload objects to Amazon S3. The solution needs to maximize object durability. Objects also must be readily available at any time and for any length of time. Users will access objects frequently within the first 30 days after the objects are uploaded, but users are much less likely to access objects that are older than 30 days.  
  
Which solution meets these requirements MOST cost-effectively?

- A. Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 Glacier after 30 days.
- B. Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.
- C. Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.
- D. Store all the objects in S3 Intelligent-Tiering with an S3 Lifecycle rule to transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.
```

정답 : `B`

- S3 Standard는 처음 30일 동안의 잦은 액세스를 저지연으로 처리
- 이후 S3 Standard-IA로 전환하면 동일한 11 9's 내구성과 즉시 접근성(밀리초 단위)을 유지하면서 저장 비용 절감
- 액세스 패턴이 명확(30일 이후 드물게 접근)하므로 별도의 모니터링 비용이 드는 Intelligent-Tiering보다 비용 효율적

오답 이유

- A. Glacier(현 Flexible Retrieval/Deep Archive 계열)는 아카이브 용도로 복구 지연이 분~시간 단위일 수 있어 “언제든지 즉시 사용 가능” 요구를 충족하지 못합니다.

- C. One Zone-IA는 단일 AZ 저장으로 내구성·복원력 측면에서 위험이 커 “내구성 최대화” 요구에 부합하지 않습니다.

- D. Intelligent-Tiering은 접근 패턴이 불확실할 때 적합하며, 본 사례처럼 30일 이후 드물게 접근하는 명확한 패턴에서는 모니터링 비용만 증가할 수 있습니다. 또한 IA로의 별도 라이프사이클 전환은 중복 설정입니다.


## #607
한 회사가 온프레미스 데이터 센터에서 AWS 클라우드로 2계층 애플리케이션을 마이그레이션했습니다. 데이터 계층은 12TB의 범용 SSD Amazon Elastic Block Store(Amazon EBS) 스토리지를 사용하는 Amazon RDS for Oracle의 Multi-AZ 배포입니다. 애플리케이션은 평균 문서 크기 6MB로 데이터베이스에 문서를 BLOB(이진 대용량 객체)로 저장하도록 설계되어 있습니다.

데이터베이스 크기가 시간이 지남에 따라 증가하여 성능이 저하되고 스토리지 비용이 증가했습니다. 회사는 데이터베이스 성능을 개선해야 하며 고가용성과 복원력이 뛰어난 솔루션이 필요합니다.

다음 중 가장 비용 효율적으로 이러한 요구를 충족하는 솔루션은 무엇입니까?

A. RDS DB 인스턴스 크기를 줄입니다. 스토리지 용량을 24TiB로 늘립니다. 스토리지 유형을 Magnetic으로 변경합니다.
B. RDS DB 인스턴스 크기를 늘립니다. 스토리지 용량을 24TiB로 늘립니다. 스토리지 유형을 Provisioned IOPS로 변경합니다.
C. Amazon S3 버킷을 생성합니다. 애플리케이션을 업데이트하여 문서를 S3 버킷에 저장합니다. 객체 메타데이터는 기존 데이터베이스에 저장합니다.
D. Amazon DynamoDB 테이블을 생성합니다. 애플리케이션을 업데이트하여 DynamoDB를 사용합니다. AWS Database Migration Service(AWS DMS)를 사용하여 Oracle 데이터베이스에서 DynamoDB로 데이터를 마이그레이션합니다.

```
A company has migrated a two-tier application from its on-premises data center to the AWS Cloud. The data tier is a Multi-AZ deployment of Amazon RDS for Oracle with 12 TB of General Purpose SSD Amazon Elastic Block Store (Amazon EBS) storage. The application is designed to process and store documents in the database as binary large objects (blobs) with an average document size of 6 MB.  
  
The database size has grown over time, reducing the performance and increasing the cost of storage. The company must improve the database performance and needs a solution that is highly available and resilient.  
  
Which solution will meet these requirements MOST cost-effectively?

- A. Reduce the RDS DB instance size. Increase the storage capacity to 24 TiB. Change the storage type to Magnetic.
- B. Increase the RDS DB instance size. Increase the storage capacity to 24 TiChange the storage type to Provisioned IOPS.
- C. Create an Amazon S3 bucket. Update the application to store documents in the S3 bucket. Store the object metadata in the existing database.
- D. Create an Amazon DynamoDB table. Update the application to use DynamoDB. Use AWS Database Migration Service (AWS DMS) to migrate data from the Oracle database to DynamoDB.
```

정답 : `C`

- BLOB(문서)를 데이터베이스에서 분리해 Amazon S3에 저장하고, 메타데이터만 RDS(Oracle)에 유지하면 DB 사이즈가 크게 줄어 인덱스/버퍼 캐시 효율이 향상

오답 이유

- A. 인스턴스 크기를 줄이고 Magnetic(표준 HDD)으로 변경하면 IOPS/지연이 악화되어 성능 저하가 더 심해집니다. 비용은 줄더라도 요구사항(성능 개선, 고가용성)에 부합하지 않습니다.

- B. 인스턴스/스토리지를 키우고 io1/io2(Provisioned IOPS)로 바꾸면 성능은 나아질 수 있으나, BLOB을 계속 DB에 두는 구조라 스토리지/라이선스 비용이 크게 증가합니다. “가장 비용 효율적”이지 않습니다.

- D. OLTP 애플리케이션을 DynamoDB로 전환하려면 스키마/쿼리/트랜잭션 모델을 재설계해야 하며, 마이그레이션 난이도와 위험이 큽니다. 요구사항은 DB 성능/비용 최적화이지 NoSQL로의 전환이 아닙니다.


## #608
한 회사에는 전 세계 20,000개 이상의 소매 매장 위치에 배포된 클라이언트를 서비스하는 애플리케이션이 있습니다. 애플리케이션은 HTTPS 443 포트로 노출되는 백엔드 웹 서비스로 구성됩니다. 애플리케이션은 Application Load Balancer(ALB) 뒤의 Amazon EC2 인스턴스에서 호스팅됩니다. 소매 매장 위치는 공용 인터넷을 통해 웹 애플리케이션과 통신합니다. 회사는 각 소매 매장 위치가 해당 지역 ISP로부터 할당받은 IP 주소를 등록하도록 허용합니다.

회사 보안팀은 등록된 소매 매장 IP 주소에서만 액세스하도록 제한하여 애플리케이션 엔드포인트의 보안을 강화할 것을 권고합니다.

이 요구 사항을 충족하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?

A. ALB에 AWS WAF 웹 ACL을 연결합니다. ALB에서 IP 규칙 집합(IP rule set)을 사용하여 트래픽을 필터링합니다. 규칙의 IP 주소를 업데이트하여 등록된 IP 주소를 포함시킵니다.
B. ALB를 관리하기 위해 AWS Firewall Manager를 배포합니다. ALB로의 트래픽을 제한하도록 방화벽 규칙을 구성합니다. 등록된 IP 주소를 포함하도록 방화벽 규칙을 수정합니다.
C. IP 주소를 Amazon DynamoDB 테이블에 저장합니다. ALB에서 들어오는 요청이 등록된 IP 주소에서 왔는지 검증하기 위해 AWS Lambda 권한 부여 함수를 구성합니다.
D. ALB의 퍼블릭 인터페이스가 있는 서브넷의 네트워크 ACL을 구성합니다. 등록된 IP 주소 각각에 대한 항목으로 네트워크 ACL의 인바운드 규칙을 업데이트합니다.

```
A company has an application that serves clients that are deployed in more than 20.000 retail storefront locations around the world. The application consists of backend web services that are exposed over HTTPS on port 443. The application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The retail locations communicate with the web application over the public internet. The company allows each retail location to register the IP address that the retail location has been allocated by its local ISP.  
  
The company's security team recommends to increase the security of the application endpoint by restricting access to only the IP addresses registered by the retail locations.  
  
What should a solutions architect do to meet these requirements?

- A. Associate an AWS WAF web ACL with the ALB. Use IP rule sets on the ALB to filter traffic. Update the IP addresses in the rule to include the registered IP addresses.
- B. Deploy AWS Firewall Manager to manage the ALConfigure firewall rules to restrict traffic to the ALModify the firewall rules to include the registered IP addresses.
- C. Store the IP addresses in an Amazon DynamoDB table. Configure an AWS Lambda authorization function on the ALB to validate that incoming requests are from the registered IP addresses.
- D. Configure the network ACL on the subnet that contains the public interface of the ALB. Update the ingress rules on the network ACL with entries for each of the registered IP addresses.
```

정답 : `A`

- ALB에는 AWS WAF 웹 ACL을 연결할 수 있으며 IP set을 사용해 허용 목록 기반으로 등록된 소매점 IP에서만 접근을 허용하도록 필터링 가능
- IP set은 대량 IPv4/IPv6 주소 관리를 지원하고(여러 IP set/규칙으로 확장 가능), 콘솔/CLI/API로 자동 갱신이 가능해 운영 오버헤드가 낮음

오답 이유

- B. AWS Firewall Manager는 여러 계정/리전/리소스에 걸친 정책(예: WAF 규칙)을 중앙관리할 때 유용합니다. 단일 ALB에 대한 단순 허용 목록 적용에는 과도하며, 자체적으로 트래픽 필터링을 수행하는 게 아니라 결국 WAF 정책 배포를 관리할 뿐입니다.

- C. “Lambda 권한 부여자(authorizer)”는 API Gateway 개념입니다. ALB에는 커스텀 권한 부여 훅이 없으며, 소스 IP 검증은 L7에서 애플리케이션 코드로 처리해야 해 비효율적이고 신뢰성도 낮습니다.

- D. 네트워크 ACL은 규칙 수 제한이 커서(일반적으로 수십~백 단위) 2만 개 이상의 IP를 나열할 수 없습니다. 또한 NACL은 정적 관리가 번거롭고 세밀한 업데이트 자동화가 어렵습니다.


## #609
한 회사가 AWS Lake Formation을 사용하여 AWS에 데이터 분석 플랫폼을 구축하고 있습니다. 이 플랫폼은 Amazon S3와 Amazon RDS 같은 서로 다른 소스에서 데이터를 수집(ingest)할 것입니다. 회사는 민감한 정보를 포함한 데이터의 일부에 대한 접근을 방지하기 위한 안전한 솔루션이 필요합니다.

다음 중 운영 오버헤드가 가장 낮으면서 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. Lake Formation 테이블에 접근 권한이 포함된 IAM 역할을 생성합니다.
B. 행 수준 보안(row-level security)과 셀 수준 보안(cell-level security)을 구현하기 위해 데이터 필터(data filters)를 생성합니다.
C. Lake Formation이 데이터를 수집하기 전에 민감 정보를 제거하는 AWS Lambda 함수를 생성합니다.
D. 정기적으로 Lake Formation 테이블을 조회하여 민감 정보를 제거하는 AWS Lambda 함수를 생성합니다.

```
A company is building a data analysis platform on AWS by using AWS Lake Formation. The platform will ingest data from different sources such as Amazon S3 and Amazon RDS. The company needs a secure solution to prevent access to portions of the data that contain sensitive information.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an IAM role that includes permissions to access Lake Formation tables.
- B. Create data filters to implement row-level security and cell-level security.
- C. Create an AWS Lambda function that removes sensitive information before Lake Formation ingests the data.
- D. Create an AWS Lambda function that periodically queries and removes sensitive information from Lake Formation tables.
```

정답 : `B`

- Lake Formation은 세분화된 데이터 권한(테이블/열 권한, 행 수준 보안(데이터 필터), LF-태그 기반 권한)을 네이티브로 제공해, 민감 정보가 포함된 특정 열/행에 대한 접근을 차단 가능
- 이 방법은 ETL로 데이터를 변형･삭제할 필요가 없어 운영 오버헤드가 낮고 보안 정책을 중앙에서 일관되게 관리 가능

오답 이유

- A. IAM 역할만으로는 Lake Formation의 세분화된(행/열) 접근 제어를 대체할 수 없습니다. 데이터 일부를 선택적으로 차단하는 요구를 직접 충족하지 못합니다.

- C. 수집(ingest) 전에 Lambda로 민감 정보를 제거하면 데이터 자체가 변형되며, 파이프라인 유지보수·예외 처리 등 운영 부담이 큽니다. 또한 사용 목적에 따라 원본 보존 요구와 충돌할 수 있습니다.

- D. 주기적으로 Lambda로 민감 정보를 삭제하는 방식은 지연·일관성 문제를 야기하고, 스케줄/모니터링/실패 재처리 등 운영 오버헤드가 큽니다. 접근 제어(정교한 권한) 문제를 삭제 작업으로 해결하는 것은 부적절합니다.


## #610
한 회사가 VPC에서 실행되는 Amazon EC2 인스턴스를 배포했습니다. EC2 인스턴스는 원본 데이터를 Amazon S3 버킷에 적재하여 추후 처리할 수 있도록 합니다. 컴플라이언스 법규에 따라 데이터는 공용 인터넷을 통해 전송되어서는 안 됩니다. 회사의 온프레미스 데이터 센터에 있는 서버들은 EC2 인스턴스에서 실행되는 애플리케이션의 출력을 소비할 것입니다.

다음 중 어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. Amazon EC2용 인터페이스 VPC 엔드포인트를 배포합니다. 회사와 VPC 간에 AWS Site-to-Site VPN 연결을 생성합니다.
B. Amazon S3용 게이트웨이 VPC 엔드포인트를 배포합니다. 온프레미스 네트워크와 VPC 간에 AWS Direct Connect 연결을 설정합니다.
C. VPC에서 S3 버킷으로의 AWS Transit Gateway 연결을 설정합니다. 회사와 VPC 간에 AWS Site-to-Site VPN 연결을 생성합니다.
D. NAT 게이트웨이에 대한 라우트를 가진 프록시 EC2 인스턴스를 설정합니다. 프록시 EC2 인스턴스가 S3 데이터를 가져와 애플리케이션 인스턴스에 공급하도록 구성합니다.

```
A company deploys Amazon EC2 instances that run in a VPC. The EC2 instances load source data into Amazon S3 buckets so that the data can be processed in the future. According to compliance laws, the data must not be transmitted over the public internet. Servers in the company's on-premises data center will consume the output from an application that runs on the EC2 instances.  
  
Which solution will meet these requirements?

- A. Deploy an interface VPC endpoint for Amazon EC2. Create an AWS Site-to-Site VPN connection between the company and the VPC.
- B. Deploy a gateway VPC endpoint for Amazon S3. Set up an AWS Direct Connect connection between the on-premises network and the VPC.
- C. Set up an AWS Transit Gateway connection from the VPC to the S3 buckets. Create an AWS Site-to-Site VPN connection between the company and the VPC.
- D. Set up proxy EC2 instances that have routes to NAT gateways. Configure the proxy EC2 instances to fetch S3 data and feed the application instances.
```

정답 : `B`

- EC2 ↔ S3 트래픽을 인터넷을 거치지 않게 하려면 S3용 게이트웨이 VPC 엔드포인트 사용
- 온프레미스 ↔ VPC 간 트래픽을 공용 인터넷이 아닌 전용 회선으로 전송하려면 AWS Direct Connect가 적합

오답 이유

- A. Site-to-Site VPN은 암호화되더라도 전송 경로가 공용 인터넷입니다. 또한 EC2용 인터페이스 엔드포인트는 EC2 데이터 평면에 사용되지 않아 목적에 부합하지 않습니다.

- C. Transit Gateway는 S3로 직접 연결하지 않습니다. 그리고 Site-to-Site VPN은 여전히 인터넷을 사용하므로 요건을 위배합니다.

- D. NAT/프록시를 사용하면 S3 접근이 인터넷을 경유합니다. “공용 인터넷 경유 금지” 요건을 충족하지 못합니다.


## #611
한 회사에 REST 기반 인터페이스를 가진 애플리케이션이 있으며, 서드파티 벤더로부터 거의 실시간(near-real time)으로 데이터를 수신할 수 있습니다. 일단 수신되면, 애플리케이션은 데이터를 처리하고 추가 분석을 위해 저장합니다. 애플리케이션은 Amazon EC2 인스턴스에서 실행 중입니다.

서드파티 벤더는 애플리케이션에 데이터를 보낼 때 503 Service Unavailable 오류를 자주 받았습니다. 데이터 볼륨이 급증할 때 컴퓨팅 용량이 최대 한도에 도달하고 애플리케이션이 모든 요청을 처리할 수 없습니다.

더 확장 가능한 솔루션을 제공하기 위해 솔루션스 아키텍트는 어떤 설계를 권장해야 합니까?

A. Amazon Kinesis Data Streams를 사용하여 데이터를 수집합니다. AWS Lambda 함수를 사용해 데이터를 처리합니다.
B. 기존 애플리케이션 위에 Amazon API Gateway를 사용합니다. 서드파티 벤더를 위한 쿼터 제한이 있는 사용 계획을 생성합니다.
C. Amazon Simple Notification Service(Amazon SNS)를 사용하여 데이터를 수집합니다. EC2 인스턴스를 Application Load Balancer 뒤의 Auto Scaling 그룹에 넣습니다.
D. 애플리케이션을 컨테이너로 재패키징합니다. Amazon Elastic Container Service(Amazon ECS)의 EC2 런치 타입과 Auto Scaling 그룹을 사용하여 애플리케이션을 배포합니다.

```
A company has an application with a REST-based interface that allows data to be received in near-real time from a third-party vendor. Once received, the application processes and stores the data for further analysis. The application is running on Amazon EC2 instances.  
  
The third-party vendor has received many 503 Service Unavailable Errors when sending data to the application. When the data volume spikes, the compute capacity reaches its maximum limit and the application is unable to process all requests.  
  
Which design should a solutions architect recommend to provide a more scalable solution?

- A. Use Amazon Kinesis Data Streams to ingest the data. Process the data using AWS Lambda functions.
- B. Use Amazon API Gateway on top of the existing application. Create a usage plan with a quota limit for the third-party vendor.
- C. Use Amazon Simple Notification Service (Amazon SNS) to ingest the data. Put the EC2 instances in an Auto Scaling group behind an Application Load Balancer.
- D. Repackage the application as a container. Deploy the application using Amazon Elastic Container Service (Amazon ECS) using the EC2 launch type with an Auto Scaling group.
```

정답 : `A`

- 급격한 트래픽 스파이크로 인한 503을 줄이려면 수신(ingest)와 처리 컴퓨팅을 분리해야 함
- Kinesis Data Streams는 초당 대량 이벤트를 탄력적으로 버퍼링하고 Lambda(또는 Kinesis Client)로 근실시간 처리･확장을 자동화할 수 있어 백엔드 한계로 인한 드롭을 완화
- 프로듀서는 스트림에 밀어넣고, 컨슈머는 확장/재시도/체크포인팅으로 안정적으로 처리하므로 더 확장 가능하고 운영 부담이 낮음

오답 이유

- B. API Gateway의 쿼터/스로틀은 단순 제한만 제공하여 503 빈도를 낮출 수 있어도, 처리 백엔드의 병목 자체를 해소하지 못하며 데이터 유실 위험이 있습니다.

- C. SNS는 팬아웃 알림에 적합하며 순서/리플레이/고속 스트리밍 버퍼링 요구에 부적합합니다. 또한 ALB+오토스케일만으로는 급격한 스파이크에서 여전히 동기식 과부하가 발생할 수 있습니다.

- D. ECS로 재배포하고 오토스케일링을 사용해도 동기식 REST 패턴 자체는 그대로라 급격한 스파이크 시 드롭과 503 위험이 남습니다. 디커플링(버퍼링) 계층이 없습니다.


## #612
한 회사에는 프라이빗 서브넷의 Amazon EC2 인스턴스에서 실행되는 애플리케이션이 있습니다. 이 애플리케이션은 Amazon S3 버킷의 민감한 정보를 처리해야 합니다. 애플리케이션은 S3 버킷에 연결할 때 인터넷을 사용하면 안 됩니다.

다음 중 어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. 인터넷 게이트웨이를 구성합니다. S3 버킷 정책을 업데이트하여 인터넷 게이트웨리에서의 액세스를 허용합니다. 애플리케이션을 새 인터넷 게이트웨이를 사용하도록 업데이트합니다.
B. VPN 연결을 구성합니다. S3 버킷 정책을 업데이트하여 VPN 연결에서의 액세스를 허용합니다. 애플리케이션을 새 VPN 연결을 사용하도록 업데이트합니다.
C. NAT 게이트웨이를 구성합니다. S3 버킷 정책을 업데이트하여 NAT 게이트웨이에서의 액세스를 허용합니다. 애플리케이션을 새 NAT 게이트웨이를 사용하도록 업데이트합니다.
D. VPC 엔드포인트를 구성합니다. S3 버킷 정책을 업데이트하여 VPC 엔드포인트에서의 액세스를 허용합니다. 애플리케이션을 새 VPC 엔드포인트를 사용하도록 업데이트합니다.

```
A company has an application that runs on Amazon EC2 instances in a private subnet. The application needs to process sensitive information from an Amazon S3 bucket. The application must not use the internet to connect to the S3 bucket.  
  
Which solution will meet these requirements?

- A. Configure an internet gateway. Update the S3 bucket policy to allow access from the internet gateway. Update the application to use the new internet gateway.
- B. Configure a VPN connection. Update the S3 bucket policy to allow access from the VPN connection. Update the application to use the new VPN connection.
- C. Configure a NAT gateway. Update the S3 bucket policy to allow access from the NAT gateway. Update the application to use the new NAT gateway.
- D. Configure a VPC endpoint. Update the S3 bucket policy to allow access from the VPC endpoint. Update the application to use the new VPC endpoint.
```

정답 : `D`

- S3에 사설 경로로 접근하려면 S3용 VPC 엔드포인트(게이트웨이 엔드포인트)를 사용해야 함
- 이 경로는 트래픽이 AS 네트워크 내부로만 흐르므로 인터넷/NAT을 거치지 않음
- 버킷 정책에서 aws:SourceVpce 조건을 사용해 해당 VPC 엔드포인트에서만 접근 허용하도록 제한 가능

오답 이유

- A. 인터넷 게이트웨이는 퍼블릭 인터넷 경유가 전제됩니다. “인터넷을 사용하지 말 것” 요구와 상충합니다.

- B. VPN 연결은 인터넷 위의 암호화 터널을 사용하므로 여전히 인터넷을 경유합니다. 요건을 만족하지 않습니다.

- C. NAT 게이트웨이는 프라이빗 서브넷의 아웃바운드 인터넷 접근을 제공하는 구성요소입니다. 인터넷 비경유 요구에 부적합합니다.


## #613
한 회사는 Amazon EKS(Amazon Elastic Kubernetes Service)를 사용하여 컨테이너 애플리케이션을 실행합니다. EKS 클러스터는 Kubernetes secrets 객체에 민감한 정보를 저장합니다. 회사는 이 정보가 암호화되어 있음을 보장하고자 합니다.

다음 중 운영 오버헤드가 가장 적은 솔루션은 무엇입니까?

A. 컨테이너 애플리케이션이 AWS Key Management Service(AWS KMS)를 사용하여 정보를 암호화하도록 합니다.
B. AWS Key Management Service(AWS KMS)를 사용하여 EKS 클러스터에서 secrets 암호화를 활성화합니다.
C. AWS Lambda 함수를 구현하여 AWS Key Management Service(AWS KMS)를 사용해 정보를 암호화합니다.
D. AWS Systems Manager Parameter Store를 사용하여 AWS Key Management Service(AWS KMS)로 정보를 암호화합니다.

```
A company uses Amazon Elastic Kubernetes Service (Amazon EKS) to run a container application. The EKS cluster stores sensitive information in the Kubernetes secrets object. The company wants to ensure that the information is encrypted.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Use the container application to encrypt the information by using AWS Key Management Service (AWS KMS).
- B. Enable secrets encryption in the EKS cluster by using AWS Key Management Service (AWS KMS).
- C. Implement an AWS Lambda function to encrypt the information by using AWS Key Management Service (AWS KMS).
- D. Use AWS Systems Manager Parameter Store to encrypt the information by using AWS Key Management Service (AWS KMS).
```

정답 : `B`

- EKS는 Kubernetes secrets에 대한 KMS 기반 암호화를 네이티브로 지원
- 클러스터에서 secrets encryption을 활성화 하면 etcd에 저장되는 시크릿이 at rest KMS로 암호화되며, 애플리케이션 코드 변경이나 별도 파이프라인 없이 보안이 강화

오답 이유

- A. 애플리케이션에 암호화/복호화 로직을 넣으면 코드 변경과 키 관리, 오류 처리 등 운영 복잡도가 증가합니다.

- C. Lambda로 중간 암호화를 수행하면 트리거/권한/에러 재시도 등 파이프라인을 운영해야 하므로 불필요하게 복잡합니다.

- D. Parameter Store의 SecureString은 유효한 대안이지만, 현재 Kubernetes secrets를 사용 중이라면 워크로드/매니페스트 변경과 통합 작업이 필요합니다. 네이티브 EKS secrets 암호화보다 운영 오버헤드가 큽니다.


## #614
한 회사가 다음 구성 요소로 이루어진 새로운 멀티 티어 웹 애플리케이션을 설계하고 있습니다:

• Auto Scaling 그룹의 일부로 Amazon EC2 인스턴스에서 실행되는 웹 및 애플리케이션 서버
• 데이터 저장을 위한 Amazon RDS DB 인스턴스

솔루션스 아키텍트는 오직 웹 서버만 애플리케이션 서버에 접근할 수 있도록 애플리케이션 서버에 대한 접근을 제한해야 합니다.

이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 애플리케이션 서버 앞에 AWS PrivateLink를 배포합니다. 네트워크 ACL을 구성하여 웹 서버만 애플리케이션 서버에 접근하도록 허용합니다.
B. 애플리케이션 서버 앞에 VPC 엔드포인트를 배포합니다. 보안 그룹을 구성하여 웹 서버만 애플리케이션 서버에 접근하도록 허용합니다.
C. 애플리케이션 서버의 Auto Scaling 그룹을 포함하는 대상 그룹과 함께 Network Load Balancer를 배포합니다. 네트워크 ACL을 구성하여 웹 서버만 애플리케이션 서버에 접근하도록 허용합니다.
D. 애플리케이션 서버의 Auto Scaling 그룹을 포함하는 대상 그룹과 함께 Application Load Balancer를 배포합니다. 보안 그룹을 구성하여 웹 서버만 애플리케이션 서버에 접근하도록 허용합니다.

```
A company is designing a new multi-tier web application that consists of the following components:  
  
• Web and application servers that run on Amazon EC2 instances as part of Auto Scaling groups  
• An Amazon RDS DB instance for data storage  
  
A solutions architect needs to limit access to the application servers so that only the web servers can access them.  
  
Which solution will meet these requirements?

- A. Deploy AWS PrivateLink in front of the application servers. Configure the network ACL to allow only the web servers to access the application servers.
- B. Deploy a VPC endpoint in front of the application servers. Configure the security group to allow only the web servers to access the application servers.
- C. Deploy a Network Load Balancer with a target group that contains the application servers' Auto Scaling group. Configure the network ACL to allow only the web servers to access the application servers.
- D. Deploy an Application Load Balancer with a target group that contains the application servers' Auto Scaling group. Configure the security group to allow only the web servers to access the application servers.
```

정답 : `D`

- 내부용 ALB를 애플리케이션 계층 앞에 두고, 보안 그룹에서 '웹 서버 보안 그룹'만 소스로 허용하면 "웹 → 앱"만 통과시키고 다른 접근은 차단 가능
- 보안 그룹은 상태 저장이며 SG 간 참조를 지원해, 오토 스케일링으로 인스턴스가 변해도 자동으로 정책이 적용되어 운영 오버헤드가 가장 낮음

오답 이유

- A. PrivateLink는 주로 다른 VPC/계정에 서비스를 노출할 때 쓰는 기술로, 동일 VPC 내 계층 분리와 접근 제어에는 과도합니다. 또한 NACL은 비상태성이라 관리가 번거롭습니다.

- B. ‘VPC 엔드포인트’는 AWS 서비스로의 사설 접근(Interface/Gateway 엔드포인트) 용도입니다. 자체 EC2 애플리케이션을 앞단에 두는 방법이 아닙니다.

- C. NLB는 L4 로드 밸런서로 가능하긴 하나, 접근 제어를 NACL로 하는 것은 비상태성 규칙 관리가 복잡하고 확장 시 유지가 어렵습니다. SG 기반 제어가 더 단순하고 안전합니다.


## #615
한 회사가 Amazon Elastic Kubernetes Service(Amazon EKS)에서 중요한 고객 대상 애플리케이션을 실행하고 있습니다. 애플리케이션은 마이크로서비스 아키텍처입니다. 회사는 애플리케이션의 메트릭과 로그를 수집, 집계, 요약하여 중앙화된 위치에서 확인할 수 있는 솔루션을 구현해야 합니다.

다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 기존 EKS 클러스터에서 Amazon CloudWatch 에이전트를 실행합니다. CloudWatch 콘솔에서 메트릭과 로그를 확인합니다.
B. 기존 EKS 클러스터에서 AWS App Mesh를 실행합니다. App Mesh 콘솔에서 메트릭과 로그를 확인합니다.
C. AWS CloudTrail이 데이터 이벤트를 캡처하도록 구성합니다. Amazon OpenSearch Service를 사용해 CloudTrail을 쿼리합니다.
D. 기존 EKS 클러스터에서 Amazon CloudWatch Container Insights를 구성합니다. CloudWatch 콘솔에서 메트릭과 로그를 확인합니다.

```
A company runs a critical, customer-facing application on Amazon Elastic Kubernetes Service (Amazon EKS). The application has a microservices architecture. The company needs to implement a solution that collects, aggregates, and summarizes metrics and logs from the application in a centralized location.  
  
Which solution meets these requirements?

- A. Run the Amazon CloudWatch agent in the existing EKS cluster. View the metrics and logs in the CloudWatch console.
- B. Run AWS App Mesh in the existing EKS cluster. View the metrics and logs in the App Mesh console.
- C. Configure AWS CloudTrail to capture data events. Query CloudTrail by using Amazon OpenSearch Service.
- D. Configure Amazon CloudWatch Container Insights in the existing EKS cluster. View the metrics and logs in the CloudWatch console.
```

정답 : `D`

- CloudWatch Container Insights는 EKS에서 DaemonSet(Cloud Watch 에이전트/Fluent Bit)을 통해 파트/노드/네임스페이스/컨트롤 플레인 지표와 애플리케이션 로그를 자동 수집･집계･요약해 CloudWatch로 중앙화
- 대시보드와 자동홛된 지표 파생(예: CPU/메모리/네트워크/재시작 수)로 운영 오버헤드가 가장 낮음

오답 이유

- A. 단순 CloudWatch 에이전트만 배포하면 OS 수준 지표/로그 중심이며, Kubernetes 오브젝트별(파드/네임스페이스/컨테이너) 집계·요약 대시보드가 자동 제공되지 않습니다. Container Insights가 요구사항에 더 직접적입니다.

- B. AWS App Mesh는 서비스 간 트래픽 관찰(서비스 메시)과 분산 트레이싱/메트릭 연계를 돕지만, 애플리케이션 전반의 로그 중앙화·요약을 기본 제공하지 않습니다. 메시 도입은 범위 외이며 오버헤드가 큽니다.

- C. CloudTrail 데이터 이벤트는 S3, Lambda 등 리소스 API 호출 감사를 위한 것으로, 애플리케이션 런타임 로그/메트릭 수집 용도가 아닙니다.


## #616
한 회사가 AWS에 최신 제품을 배포했습니다. 이 제품은 Network Load Balancer 뒤의 Auto Scaling 그룹에서 실행됩니다. 회사는 제품의 객체를 Amazon S3 버킷에 저장하고 있습니다.

최근 회사는 시스템에 대한 악의적인 공격을 경험했습니다. 회사는 AWS 계정, 워크로드, 그리고 S3 버킷에 대한 액세스 패턴을 지속적으로 모니터링하여 악의적인 활동을 탐지해야 합니다. 또한, 솔루션은 의심스러운 활동을 보고하고, 대시보드에서 이 정보를 표시해야 합니다.

이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. Amazon Macie를 구성하여 AWS Config에 탐지 결과를 보고합니다.  
B. Amazon Inspector를 구성하여 AWS CloudTrail에 탐지 결과를 보고합니다.  
C. Amazon GuardDuty를 구성하여 AWS Security Hub에 탐지 결과를 보고합니다.  
D. AWS Config를 구성하여 Amazon EventBridge에 탐지 결과를 보고합니다.

```
A company has deployed its newest product on AWS. The product runs in an Auto Scaling group behind a Network Load Balancer. The company stores the product’s objects in an Amazon S3 bucket.  
  
The company recently experienced malicious attacks against its systems. The company needs a solution that continuously monitors for malicious activity in the AWS account, workloads, and access patterns to the S3 bucket. The solution must also report suspicious activity and display the information on a dashboard.  
  
Which solution will meet these requirements?

- A. Configure Amazon Macie to monitor and report findings to AWS Config.
- B. Configure Amazon Inspector to monitor and report findings to AWS CloudTrail.
- C. Configure Amazon GuardDuty to monitor and report findings to AWS Security Hub.
- D. Configure AWS Config to monitor and report findings to Amazon EventBridge.
```

정답 : `C`

- Amazon GuardDuty는 AWS 계정, 네트워크, 워크로드, S3 데이터 액세스 패턴을 지속적으로 분석해 악의적인 활동(예: 비정상 API 호출, 무단 접근, 의심스러운 IAM 동작)을 탐지하는 지속적 보안 위협 탐지 서비스
- GuardDuty 탐지 결과는 AWS Security Hub와 통합되어 중앙 보안 대시보드에서 모니터링, 분석, 상관관계를 확인 가능

오답 이유

- A. Amazon Macie는 S3 데이터에서 개인 식별 정보(PII) 등 민감 데이터를 식별/분류/보호하는 서비스입니다. 악의적 행위나 접근 패턴 이상 탐지는 GuardDuty의 역할입니다.

- B. Amazon Inspector는 EC2, ECR, Lambda 환경의 취약점 평가(Vulnerability scanning)에 특화되어 있습니다. 실시간 악의적 활동 탐지는 수행하지 않습니다.

- D. AWS Config는 리소스 구성 변경을 추적하는 거버넌스/컴플라이언스 서비스입니다. 위협 탐지나 의심스러운 활동 모니터링 기능은 없습니다.


## #617
한 회사가 온프레미스 데이터 센터를 AWS로 마이그레이션하려 합니다. 데이터 센터에는 NFS 기반 파일 시스템에 데이터를 저장하는 스토리지 서버가 있습니다. 이 스토리지 서버에는 200 GB의 데이터가 있습니다. 회사는 기존 서비스에 중단 없이 데이터를 마이그레이션해야 합니다. 또한 AWS의 여러 리소스가 NFS 프로토콜을 사용하여 해당 데이터에 액세스할 수 있어야 합니다.

다음 중 가장 비용 효율적으로 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (두 가지 선택)

A. Amazon FSx for Lustre 파일 시스템을 생성합니다.
B. Amazon Elastic File System(Amazon EFS) 파일 시스템을 생성합니다.
C. 데이터를 수신할 Amazon S3 버킷을 생성합니다.
D. 운영 체제의 복사 명령을 수동으로 사용하여 데이터를 AWS 대상으로 푸시합니다.
E. 온프레미스 데이터 센터에 AWS DataSync 에이전트를 설치합니다. 온프레미스 위치와 AWS 간에 DataSync 작업을 사용합니다.

```
A company wants to migrate an on-premises data center to AWS. The data center hosts a storage server that stores data in an NFS-based file system. The storage server holds 200 GB of data. The company needs to migrate the data without interruption to existing services. Multiple resources in AWS must be able to access the data by using the NFS protocol.  
  
Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)

- A. Create an Amazon FSx for Lustre file system.
- B. Create an Amazon Elastic File System (Amazon EFS) file system.
- C. Create an Amazon S3 bucket to receive the data.
- D. Manually use an operating system copy command to push the data into the AWS destination.
- E. Install an AWS DataSync agent in the on-premises data center. Use a DataSync task between the on-premises location and AWS.
```

정답 : `B, E`

- B: 관리형 NFS 파일 시스템으로, 여러 VPC 리소스에서 동시에 NFS로 마운트 가능하고 확장･가용성･내구성이 높음
- E: 온프레미스 NFS 서버 ↔ EFS 간에 증분/병렬 복제와 무결성 검증을 제공하여 중단 없이(점증적 동기화 후 짧은 컷오버) 마이그레이션 수행

오답 이유

- A. FSx for Lustre는 HPC/일괄 처리에 최적화된 고성능 파일 시스템으로, 일반 NFS 공유 대체 및 비용 효율 목표에는 과도합니다. 또한 AWS 리소스 전반의 표준 NFS 공유 용도에는 EFS가 적합합니다.

- C. S3는 객체 스토리지로 NFS 프로토콜을 직접 제공하지 않습니다. “여러 리소스가 NFS로 접근” 요건을 충족하지 못합니다.

- D. 수동 복사(rsync/OS copy)는 속도·무결성·재시도/재개 관리가 부족하고, 동기화 유지가 어려워 무중단 전환을 보장하기 어렵습니다. DataSync가 더 안전하고 운영 부담이 낮습니다.


## #618
한 회사는 us-east-1 리전에서 SMB 파일 공유를 볼륨으로 마운트한 Amazon EC2 인스턴스용으로 Amazon FSx for Windows File Server를 사용하려고 합니다. 회사는 계획된 시스템 유지 보수 또는 계획되지 않은 서비스 중단 시 RPO(복구 시점 목표)를 5분으로 요구합니다. 회사는 파일 시스템을 us-west-2 리전으로 복제해야 합니다. 복제된 데이터는 5년 동안 어떤 사용자도 삭제할 수 없어야 합니다.

다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. us-east-1에 배포 유형이 Single-AZ 2인 FSx for Windows File Server 파일 시스템을 생성합니다. AWS Backup을 사용해 us-west-2로 백업을 복사하는 규칙이 포함된 일일 백업 계획을 생성합니다. us-west-2의 대상 볼트에 대해 AWS Backup Vault Lock을 컴플라이언스 모드로 구성합니다. 최소 기간을 5년으로 설정합니다.

B. us-east-1에 배포 유형이 Multi-AZ인 FSx for Windows File Server 파일 시스템을 생성합니다. AWS Backup을 사용해 us-west-2로 백업을 복사하는 규칙이 포함된 일일 백업 계획을 생성합니다. us-west-2의 대상 볼트에 대해 AWS Backup Vault Lock을 거버넌스 모드로 구성합니다. 최소 기간을 5년으로 설정합니다.

C. us-east-1에 배포 유형이 Multi-AZ인 FSx for Windows File Server 파일 시스템을 생성합니다. AWS Backup을 사용해 us-west-2로 백업을 복사하는 규칙이 포함된 일일 백업 계획을 생성합니다. us-west-2의 대상 볼트에 대해 AWS Backup Vault Lock을 컴플라이언스 모드로 구성합니다. 최소 기간을 5년으로 설정합니다.

D. us-east-1에 배포 유형이 Single-AZ 2인 FSx for Windows File Server 파일 시스템을 생성합니다. AWS Backup을 사용해 us-west-2로 백업을 복사하는 규칙이 포함된 일일 백업 계획을 생성합니다. us-west-2의 대상 볼트에 대해 AWS Backup Vault Lock을 거버넌스 모드로 구성합니다. 최소 기간을 5년으로 설정합니다.

```
A company wants to use Amazon FSx for Windows File Server for its Amazon EC2 instances that have an SMB file share mounted as a volume in the us-east-1 Region. The company has a recovery point objective (RPO) of 5 minutes for planned system maintenance or unplanned service disruptions. The company needs to replicate the file system to the us-west-2 Region. The replicated data must not be deleted by any user for 5 years.  
  
Which solution will meet these requirements?

- A. Create an FSx for Windows File Server file system in us-east-1 that has a Single-AZ 2 deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-west-2. Configure AWS Backup Vault Lock in compliance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.
- B. Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-west-2. Configure AWS Backup Vault Lock in governance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.
- C. Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-west-2. Configure AWS Backup Vault Lock in compliance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.
- D. Create an FSx for Windows File Server file system in us-east-1 that has a Single-AZ 2 deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-west-2. Configure AWS Backup Vault Lock in governance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.
```

정답 : `C`

- RPO 5분: 리전 내 장애/유지보수에 대해 FSx for Windows Multi-AZ 배포는 동기식 복제로 고가용성을 제공해 매우 짧은 RPO/RTO를 충족
- 교차 리전 복제/보존: AWS Backup의 일일 백업 + 교차 리전 복사로 us-west-2에 백업 사본을 유지
- 삭제 불가(5년 WORM): AWS Backup Vault Lock '컴플라이언스 모드'는 어떤 사용자(루트 포함)도 보존 기간 동안 삭제/수정할 수 없도록 보장(거버넌스 모드는 관리자 우회 가능)

오답 이유

- A. Single-AZ 2는 리전 내 장애 시 RPO/RTO 측면에서 Multi-AZ보다 불리하여 “RPO 5분” 요구에 최적이 아닙니다. Vault Lock 모드는 올바르나 가용성 요구에서 미흡합니다.

- B. Multi-AZ는 적절하지만 Vault Lock을 거버넌스 모드로 설정하면 권한 있는 사용자가 보존 정책을 우회할 수 있어 “어떤 사용자도 삭제 불가” 요건에 어긋납니다.

- D. Single-AZ 2 + 거버넌스 모드는 가용성과 보존 불변성(WORM) 두 측면 모두에서 요구사항에 미달합니다.


## #619
한 회사는 개발자들에게 AWS Organizations를 통해 개별 AWS 계정을 제공하려고 합니다. 그러나 표준 보안 제어도 유지해야 합니다. 각 개발자는 자신에게 할당된 AWS 계정에 대해 루트 사용자 수준의 접근 권한을 가지게 됩니다.  
솔루션스 아키텍트는 새로운 개발자 계정에 적용된 필수 AWS CloudTrail 구성이 수정되지 않도록 보장해야 합니다.

이 요구 사항을 충족하는 조치는 무엇입니까?

A. CloudTrail 변경을 금지하는 IAM 정책을 생성하고 이를 루트 사용자에 연결합니다.  
B. 개발자 계정 내에서 organization trails 옵션을 활성화한 새로운 CloudTrail을 생성합니다.  
C. CloudTrail 변경을 금지하는 서비스 제어 정책(SCP)을 생성하고 이를 개발자 계정에 연결합니다.  
D. CloudTrail용 서비스 연결 역할(Service-linked role)을 생성하고 관리 계정의 ARN에서만 변경을 허용하는 정책 조건을 구성합니다.

```
A solutions architect is designing a security solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Because the individual developers will have AWS account root user-level access to their own accounts, the solutions architect wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified.  
  
Which action meets these requirements?

- A. Create an IAM policy that prohibits changes to CloudTrail. and attach it to the root user.
- B. Create a new trail in CloudTrail from within the developer accounts with the organization trails option enabled.
- C. Create a service control policy (SCP) that prohibits changes to CloudTrail, and attach it the developer accounts.
- D. Create a service-linked role for CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the management account.
```

정답 : `C`

- AWS Organizations의 서비스 제어 정책(SCP)은 하위 계정(OU 또는 개별 계정)에 대해 루트 사용자 포함 모든 IAM 사용자/역할의 권한 상한선을 설정
- SCP로 CloudTrail의 StopLoggin, DeleteTrail, UpdateTrail, PutEventSelectors 등 API를 명시적으로 Deny하면 개발자가 루트 권한을 가지고 있어도 CloudTrail 구성을 변경할 수 없음

오답 이유

- A. 루트 사용자에게는 IAM 정책을 연결할 수 없습니다. IAM 정책은 IAM 사용자, 그룹, 역할에만 적용됩니다. 따라서 CloudTrail 변경 방지는 불가능합니다.

- B. organization trail은 관리 계정에서 생성되어야 하며, 하위(개발자) 계정에서는 해당 옵션을 활성화할 수 없습니다. 개발자 계정 내 trail은 중앙 제어가 불가능합니다.

- D. 서비스 연결 역할(Service-linked role)은 AWS 서비스에서 계정 내에서 특정 작업을 수행하기 위한 역할로, CloudTrail 변경 제어를 위한 수단이 아닙니다.


## #620
한 회사가 비즈니스에 중요한 애플리케이션을 AWS 클라우드에 배포할 계획입니다.  
이 애플리케이션은 **내구성이 있는 스토리지(durable storage)** 와 **일관된 저지연 성능(consistently low-latency performance)** 이 필요합니다.

이러한 요구 사항을 충족하기 위해 솔루션스 아키텍트는 어떤 스토리지 유형을 권장해야 합니까?

A. 인스턴스 스토어 볼륨(Instance store volume)  
B. Amazon ElastiCache for Memcached 클러스터  
C. 프로비저닝된 IOPS SSD Amazon Elastic Block Store (Amazon EBS) 볼륨  
D. 처리량 최적화 HDD Amazon Elastic Block Store (Amazon EBS) 볼륨

```
A company is planning to deploy a business-critical application in the AWS Cloud. The application requires durable storage with consistent, low-latency performance.  
  
Which type of storage should a solutions architect recommend to meet these requirements?

- A. Instance store volume
- B. Amazon ElastiCache for Memcached cluster
- C. Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume
- D. Throughput Optimized HDD Amazon Elastic Block Store (Amazon EBS) volume
```

정답 : `C`

- Provisioned IOPS SSD(EBS io1 또는 io2)는 고성능, 일관된 저지연, 높은 내구성을 제공하는 블록 스토리지
- 비즈니스 크리티컬한 트랜잭션 워크로드(예: DB, ERP 등)에 가장 적합
- IOPS를 사전에 프로비저닝하여 지속적인 성능 예측 가능성 확보 가능

오답 이유

- A. Instance store volume은 EC2 호스트에 물리적으로 연결된 임시 스토리지로, 인스턴스 종료 시 데이터가 손실되므로 내구성 요구를 충족하지 못합니다.

- B. Amazon ElastiCache for Memcached는 인메모리 캐시 서비스로서 내구성을 제공하지 않으며, 휘발성 캐시 용도에 적합합니다.

- D. Throughput Optimized HDD(EBS st1)는 대용량 순차 I/O(예: 빅데이터, 로그 처리)에 최적화되어 있으며, 낮은 지연(latency)을 요구하는 트랜잭션성 워크로드에는 부적합합니다.