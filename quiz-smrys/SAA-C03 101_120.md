---
created: 2025-09-28 09:53:26
last_modified: 2025-09-30 12:39:50
---
## #101
솔루션 설계자는 퍼블릭 서브넷과 프라이빗 서브넷으로 구성된 VPC를 설계하고 있습니다. VPC와 서브넷은 IPv4 CIDR 블록을 사용합니다. 고가용성을 위해 세 개의 가용 영역(AZ) 각각에 퍼블릭 서브넷 하나와 프라이빗 서브넷 하나가 있습니다. 퍼블릭 서브넷에는 인터넷 접근을 제공하기 위해 인터넷 게이트웨이가 사용됩니다. 프라이빗 서브넷의 Amazon EC2 인스턴스는 소프트웨어 업데이트를 다운로드할 수 있도록 인터넷 접근이 필요합니다.  
프라이빗 서브넷에 인터넷 접근을 가능하게 하려면 솔루션 설계자는 무엇을 해야 합니까?

- A. 각 AZ의 퍼블릭 서브넷마다 하나씩 총 세 개의 NAT 게이트웨이를 생성합니다. 각 AZ용으로 비-VPC 트래픽을 해당 AZ의 NAT 게이트웨이로 전달하도록 프라이빗 라우트 테이블을 생성합니다.  
- B. 각 AZ의 프라이빗 서브넷마다 하나씩 총 세 개의 NAT 인스턴스를 생성합니다. 각 AZ용으로 비-VPC 트래픽을 해당 AZ의 NAT 인스턴스로 전달하도록 프라이빗 라우트 테이블을 생성합니다.  
- C. 프라이빗 서브넷 중 하나에 두 번째 인터넷 게이트웨이를 생성합니다. 프라이빗 서브넷의 라우트 테이블을 업데이트하여 비-VPC 트래픽을 프라이빗 인터넷 게이트웨이로 전달합니다.  
- D. 퍼블릭 서브넷 중 하나에 이그레스 전용 인터넷 게이트웨이(egress-only IGW)를 생성합니다. 프라이빗 서브넷의 라우트 테이블을 업데이트하여 비-VPC 트래픽을 이 이그레스 전용 인터넷 게이트웨이로 전달합니다.

```
A solutions architect is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances to download software updates.  
What should the solutions architect do to enable Internet access for the private subnets?

- A. Create three NAT gateways, one for each public subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT gateway in its AZ.
- B. Create three NAT instances, one for each private subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT instance in its AZ.
- C. Create a second internet gateway on one of the private subnets. Update the route table for the private subnets that forward non-VPC traffic to the private internet gateway.
- D. Create an egress-only internet gateway on one of the public subnets. Update the route table for the private subnets that forward non-VPC traffic to the egress-only Internet gateway.
```

정답 : `A`

- 프라이빗 서브넷의 IPv4 인스턴스가 인터넷에 접속하려면 퍼블릭 경로(인터넷 게이트웨이)를 통해 나가야함.
- 프라이빗 서브넷에서 직접 IGW로 나갈 수 없으므로 NAT가 필요
- NAT Gateway(관리형 NAT)를 각 AZ 퍼블릭 서브넷에 배치
	- AZ 장애가 발생해도 같은 AZ 내의 NAT가 살아 있으면 프라이빗 인스턴스의 아웃바운드가 유지되어 고가용성 확보
	- 프라이빗 인스턴스가 동일 AZ의 NAT를 사용하면 Cross-AZ 데이터 전송 요금과 지연을 피할 수 있음
	- NAT Gateway는 AWS가 관리하므로 운영 오버헤드가 적음

오답 이유

- **B (NAT 인스턴스)**
	- NAT 인스턴스(사용자 관리 EC2)는 동작하나, 관리(패치, 확장, 장애 처리)가 필요하므로 운영 오버헤드가 큽니다. 또한 단일 인스턴스 장애를 막으려면 추가 구성(Auto Scaling, 장애 조치 등)이 필요합니다. 오늘날 권장되는 방식은 관리형 NAT Gateway입니다.
    
- **C (두 번째 인터넷 게이트웨이)**
	- VPC에는 인터넷 게이트웨이(IGW)를 각 VPC당 **하나만** 연결할 수 있습니다. 게이트웨이는 서브넷에 직접 연결되는 개념이 아니며, “프라이빗 인터넷 게이트웨이”라는 것은 존재하지 않습니다. 따라서 이 옵션은 기술적으로 불가능합니다.
    
- **D (egress-only IGW)**
	- egress-only 인터넷 게이트웨이는 **IPv6 전용**으로, IPv4 트래픽에는 적용되지 않습니다. 문제에서 VPC와 서브넷은 **IPv4** CIDR을 사용하므로 이 옵션은 부적합합니다.


## #102
한 회사는 온프레미스 데이터 센터를 AWS로 마이그레이션하려고 합니다. 데이터 센터는 데이터를 NFS 기반 파일 시스템에 저장하는 SFTP 서버를 호스트합니다. 서버에는 전송해야 할 200GB의 데이터가 있습니다. 서버는 Amazon Elastic File System(Amazon EFS) 파일 시스템을 사용하는 Amazon EC2 인스턴스에서 호스트되어야 합니다.  
이 작업을 자동화하려면 솔루션 설계자는 어떤 단계 조합을 수행해야 합니까? (두 가지 선택)

A. EC2 인스턴스를 EFS 파일 시스템과 동일한 가용 영역에 시작합니다.  
B. 온프레미스 데이터 센터에 AWS DataSync 에이전트를 설치합니다.  
C. EC2 인스턴스에 데이터를 위한 보조 Amazon Elastic Block Store(Amazon EBS) 볼륨을 생성합니다.  
D. 운영체제의 복사 명령을 수동으로 사용하여 데이터를 EC2 인스턴스로 푸시합니다.  
E. AWS DataSync를 사용하여 온프레미스 SFTP 서버에 대한 적절한 위치 구성(location configuration)을 생성합니다.

```
A company wants to migrate an on-premises data center to AWS. The data center hosts an SFTP server that stores its data on an NFS-based file system. The server holds 200 GB of data that needs to be transferred. The server must be hosted on an Amazon EC2 instance that uses an Amazon Elastic File System (Amazon EFS) file system.  
Which combination of steps should a solutions architect take to automate this task? (Choose two.)

- A. Launch the EC2 instance into the same Availability Zone as the EFS file system.
- B. Install an AWS DataSync agent in the on-premises data center.
- C. Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance for the data.
- D. Manually use an operating system copy command to push the data to the EC2 instance.
- E. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.
```

정답 : `B, E`

- 온프레미스의 SFTP(NFS 기반) 데이터를 AWS EFS로 자동화하여 안전하고 신뢰성 있게 전송하려면 AWS DataSync가 적합
	- AWS DataSync는 파일 시스템(NFS, SMB), SFTP 등에서 AWS 서비스(EFS, S3 등) 고속, 병렬 전송을 제공하며 자동 재시도와 검증 기능 포함
- B (DataSync 에이전트 설치) : 온프레미스에서 DataSync를 사용하려면 에이전트를 배포. 에이전트가 온프레미스 파일 시스템을 읽고 DataSync 서비스로 데이터를 전송
- E (DataSync 위치 구성 생성) : DataSync 콘솔/CLI에서 온프레미스 소스(SFTP/NFS 위치)와 대상(EFS)을 나타내는 위치(location)을 생성하고, 태스크를 생성하여 전송을 자동화. 스케줄링, 필터링, 전송 옵션 등을 설정해 자동화된 전송 작업 가능

오답 이유

- **A. EC2 인스턴스를 EFS와 동일 AZ에 시작**
	- 부분적으로 오해의 소지가 있음. EFS는 리전 서비스이며 각 AZ에 마운트 타깃(mount target)을 생성하여 여러 AZ의 EC2에서 접근하게 설계되어 있습니다. 반드시 **EC2를 특정 AZ에만** 시작해야 하는 것은 아니며, EFS 접근을 위해선 해당 AZ에 마운트 타깃이 있어야 합니다. 또한 이 선택만으로 전송 자동화가 되지 않으므로 정답 조합으로 적절하지 않습니다.
    
- **C. EC2에 보조 EBS 볼륨 생성**
	- EBS는 AZ-결정적(AZ-bound) 블록 스토리지로, EFS의 목적(공유 파일 시스템, 다중 인스턴스 접속, 다중 AZ 내 내구성)을 대체하지 않습니다. 질문 요구는 EFS에 데이터를 저장해야 하므로 EBS는 부적합합니다.
    
- **D. OS 복사 명령으로 수동 전송**
	- 수동 복사는 자동화가 되지 않으며 장애/재시도/검증/성능(병렬) 같은 DataSync의 장점을 제공하지 않습니다. 운영 오버헤드가 큽니다.


## #103
한 회사는 매일 같은 시간에 실행되는 AWS Glue 추출, 변환 및 로드(ETL) 작업을 가지고 있습니다. 이 작업은 Amazon S3 버킷에 있는 XML 데이터를 처리합니다. 매일 새로운 데이터가 S3 버킷에 추가됩니다. 솔루션 설계자는 AWS Glue가 매번 실행될 때 모든 데이터를 처리한다는 것을 발견했습니다.  
솔루션 설계자는 AWS Glue가 이전 데이터를 다시 처리하지 않도록 하기 위해 무엇을 해야 합니까?

A. 작업을 편집하여 작업 북마크(job bookmarks)를 사용합니다.  
B. 작업을 편집하여 데이터가 처리된 후 데이터를 삭제합니다.  
C. 작업을 편집하여 NumberOfWorkers 필드를 1로 설정합니다.  
D. FindMatches 머신 러닝(ML) 변환을 사용합니다.

```
A company has an AWS Glue extract, transform, and load (ETL) job that runs every day at the same time. The job processes XML data that is in an Amazon S3 bucket. New data is added to the S3 bucket every day. A solutions architect notices that AWS Glue is processing all the data during each run.  
What should the solutions architect do to prevent AWS Glue from reprocessing old data?

- A. Edit the job to use job bookmarks.
- B. Edit the job to delete data after the data is processed.
- C. Edit the job by setting the NumberOfWorkers field to 1.
- D. Use a FindMatches machine learning (ML) transform.
```

정답 : `A`

- AWS Glue Job Bookmarks는 ETL 작업이 이전 실행에서 처리된 데이터를 추적하도록 해줌
- 따라서 Glue가 매번 S3 버킷의 전체 데이터를 재처리하지 않고, 새로 추가된 데이터만 선택적으로 처리 가능


오답 이유

- **B. 데이터 삭제**
	- 데이터를 삭제하면 S3에 원본 데이터를 보관할 수 없게 되며, 이는 분석 및 재처리(필요 시) 요구를 방해합니다. 실무적으로 원본 데이터를 삭제하는 것은 일반적으로 잘못된 방법입니다.
    
- **C. NumberOfWorkers = 1**
	- 워커(worker) 수를 줄이는 것은 **병렬 처리 성능**만 조정할 뿐이며, Glue가 어떤 데이터를 처리할지에 영향을 주지 않습니다. 따라서 여전히 전체 데이터를 처리하게 됩니다.
    
- **D. FindMatches ML Transform**
	- 이는 AWS Glue의 머신러닝 기반 데이터 중복 제거/매칭 기능으로, reprocessing 방지와는 무관합니다. XML 데이터를 매일 새로 처리하는 시나리오와 맞지 않습니다.


## #104
솔루션 설계자는 웹사이트를 위한 고가용성 인프라를 설계해야 합니다. 웹사이트는 Amazon EC2 인스턴스에서 실행되는 Windows 웹 서버로 구동됩니다. 설계자는 수천 개의 IP 주소에서 발생하는 대규모 DDoS 공격을 완화할 수 있는 솔루션을 구현해야 합니다. 웹사이트의 다운타임은 허용되지 않습니다.  
이러한 공격으로부터 웹사이트를 보호하기 위해 어떤 조치를 취해야 합니까? (두 가지 선택)

A. AWS Shield Advanced를 사용하여 DDoS 공격을 차단합니다.  
B. Amazon GuardDuty를 구성하여 공격자를 자동으로 차단합니다.  
C. 정적 및 동적 콘텐츠 모두에 대해 웹사이트가 Amazon CloudFront를 사용하도록 구성합니다.  
D. AWS Lambda 함수를 사용하여 공격자 IP 주소를 VPC 네트워크 ACL에 자동으로 추가합니다.  
E. EC2 스팟 인스턴스를 대상 추적(타깃 트래킹) 스케일링 정책(80% CPU 사용률)과 함께 Auto Scaling 그룹으로 사용합니다.

```
A solutions architect must design a highly available infrastructure for a website. The website is powered by Windows web servers that run on Amazon EC2 instances. The solutions architect must implement a solution that can mitigate a large-scale DDoS attack that originates from thousands of IP addresses. Downtime is not acceptable for the website.  
Which actions should the solutions architect take to protect the website from such an attack? (Choose two.)

- A. Use AWS Shield Advanced to stop the DDoS attack.
- B. Configure Amazon GuardDuty to automatically block the attackers.
- C. Configure the website to use Amazon CloudFront for both static and dynamic content.
- D. Use an AWS Lambda function to automatically add attacker IP addresses to VPC network ACLs.
- E. Use EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy that is set to 80% CPU utilization.
```

정답 : `A, C`

- C - CloudFront 사용
	- CloudFront 같은 클로벌 CDN/엣지 네트워크는 트래픽을 전 세계 엣지 로케이션으로 분산시켜 오리진(EC2)으로 향하는 요청 부하는 크게 낮춰줌
	- 캐싱 가능한 정적 콘텐츠를 엣지에서 제공하면 원본 부하가 줄고 동적 요청도 엣지에서 일부 완화(최적화, 압축, TLS 종료, Lambda@Egde 등)할 수 있어 대규모 DDoS 공격에 대한 초기 흡수 능력이 크게 향상
	- 즉시 도입 가능하고 다운타임 없이 완화 효과 제공
- A - Shied Advanced 사용
	- Shield Advacned는 대규모 네트워크/존송계층(및 일부 애플리케이션 계층) DDoS 공격에 대해 AWS 차원에서 완화 기능 제공
	- CluodFront/ALB/ELB와 통합되어 공격 트래픽을 자동으로 흡수 및 완화
	- 24/7 DDoS 대응팀(DDOS Response Team, DRT) 지원과 DDoS 관련 비용 보호(일부 요금 보상) 제공


오답 이유

- **B. GuardDuty를 구성하여 공격자를 자동으로 차단**
	- GuardDuty는 이상 탐지/위협 인텔리전스 서비스로, 의심스러운 활동을 식별(탐지·알림)하지만 **자동으로 네트워크 트래픽을 차단하지 않습니다**. 차단을 자동화하려면 추가로 Lambda/Firewall Manager/Network ACL/SG 등과 연동해야 하고, 이 또한 대규모 IP 목록 차단에는 비효율적입니다.
    
- **D. Lambda로 공격자 IP를 네트워크 ACL에 자동 추가**
	- 네트워크 ACL은 상태 비저장이고 규칙 수(관리·성능) 제한이 있으며, 수천 개 IP를 실시간으로 추가/삭제하는 것은 **규모·속도·성능 면에서 비현실적**입니다. 또한 NACL 변경은 전파 지연과 관리 복잡성을 유발합니다. 더구나 대규모 공격을 NACL로 방어하면 오버헤드와 오류 위험이 큽니다.
    
- **E. 스팟 인스턴스를 Auto Scaling으로 사용**
	- 스팟 인스턴스는 **언제든 회수(중단)** 될 수 있으므로 가용성이 중요한 웹사이트의 DDoS 방어(특히 “다운타임 불허”)에 적합하지 않습니다. 스팟은 비용 절감에는 유리하지만 DDoS 방어 또는 안정성 확보용 솔루션으로 권장되지 않습니다.


## #105
한 회사가 새로운 서버리스 워크로드를 배포할 준비를 하고 있습니다. 솔루션 설계자는 AWS Lambda 함수를 실행하는 데 사용될 권한을 최소 권한 원칙(Principle of Least Privilege)에 따라 구성해야 합니다. Amazon EventBridge(Amazon CloudWatch Events) 규칙이 이 함수를 호출합니다.  
이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 함수에 실행 역할을 추가하고, 작업으로 lambda:InvokeFunction을, 주체(Principal)로 *을 지정합니다.  
B. 함수에 실행 역할을 추가하고, 작업으로 lambda:InvokeFunction을, 주체로 Service: lambda.amazonaws.com을 지정합니다.  
C. 함수에 리소스 기반 정책을 추가하고, 작업으로 lambda:*을, 주체로 Service: events.amazonaws.com을 지정합니다.  
D. 함수에 리소스 기반 정책을 추가하고, 작업으로 lambda:InvokeFunction을, 주체로 Service: events.amazonaws.com을 지정합니다.

```
A company is preparing to deploy a new serverless workload. A solutions architect must use the principle of least privilege to configure permissions that will be used to run an AWS Lambda function. An Amazon EventBridge (Amazon CloudWatch Events) rule will invoke the function.  
Which solution meets these requirements?

- A. Add an execution role to the function with lambda:InvokeFunction as the action and * as the principal.
- B. Add an execution role to the function with lambda:InvokeFunction as the action and Service: lambda.amazonaws.com as the principal.
- C. Add a resource-based policy to the function with lambda:* as the action and Service: events.amazonaws.com as the principal.
- D. Add a resource-based policy to the function with lambda:InvokeFunction as the action and Service: events.amazonaws.com as the principal.
```


정답 : `D`

- EventBridge 규칙이 람다 함수를 호출하려면, 람다 함수 리소스에 대한 리소스 기반 정책 (Resource-based policy) 을 추가해야 함
- 해당 정책에서 허용할 동작은 lambda:InvokeFunction, 호출 주체는 events.amazonaws.com (EventBridge)

오답 이유

- _A. 실행 역할에 lambda:InvokeFunction, Principal= 지정_*
    - 실행 역할은 Lambda가 다른 AWS 서비스에 접근할 때 사용하는 것임. EventBridge가 Lambda를 호출할 때는 **리소스 기반 정책**이 필요하지 실행 역할이 아님. 또한 Principal에 `*`을 지정하는 것은 최소 권한 원칙 위반.
    
- **B. 실행 역할에 lambda:InvokeFunction, Principal=lambda.amazonaws.com**
    - 마찬가지로 실행 역할은 EventBridge 호출 권한과 무관. Lambda 서비스 자체가 자기 자신을 호출하는 구조는 아님.
    
- _C. 리소스 기반 정책에 lambda: 과 Principal=events.amazonaws.com_*
    - 동작을 lambda:*로 지정하면 Lambda 함수에 대한 **모든 작업**을 허용하게 되며, 이는 최소 권한 원칙에 위배됨.


## #106
한 회사는 기밀 데이터를 Amazon S3에 저장할 준비를 하고 있습니다. 규정 준수를 위해 데이터는 저장 시점에 암호화되어야 합니다. 감사 목적으로 암호화 키 사용 내역을 로깅해야 합니다. 키는 매년 교체(로테이션)되어야 합니다.  
어떤 솔루션이 이러한 요구사항을 충족하고 운영 관점에서 가장 효율적입니까?

A. 고객이 제공한 키로 서버측 암호화 (SSE-C)  
B. Amazon S3가 관리하는 키로 서버측 암호화 (SSE-S3)  
C. AWS KMS 키로 서버측 암호화 (SSE-KMS) — 수동 회전  
D. AWS KMS 키로 서버측 암호화 (SSE-KMS) — 자동 회전

```
A company is preparing to store confidential data in Amazon S3. For compliance reasons, the data must be encrypted at rest. Encryption key usage must be logged for auditing purposes. Keys must be rotated every year.  
Which solution meets these requirements and is the MOST operationally efficient?

- A. Server-side encryption with customer-provided keys (SSE-C)
- B. Server-side encryption with Amazon S3 managed keys (SSE-S3)
- C. Server-side encryption with AWS KMS keys (SSE-KMS) with manual rotation
- D. Server-side encryption with AWS KMS keys (SSE-KMS) with automatic rotation
```

정답 : `D`

- 요구사항
	- 저장 시 암호화(at-rest encryption)
	- 키 사용 로깅(감사)
	- 연간 키 회전
	- 운영 효율성
- SSE-KMS는 S3와 통합되어 객체 저장 시 KMS로 키 기반 암호화를 수행
- KMS는 CloudTrail을 통해 키 사용 호출을 로깅하므로 감사 요구 충족
- 고객 관리형 KMS 키(CMK) 에서 자동 키 회전을 활성화 하면 AWS가 매년 (약1년 주기) 키를 자동으로 교체

오답 이유

- **A. SSE-C (고객 제공 키)**
    - 키를 고객이 직접 제공·관리해야 하므로 키 보관·회전·보안 책임이 전적으로 고객에게 있으며, **자동 회전 기능이 없고** 키 사용 로깅을 KMS 수준처럼 통합적으로 제공하지 않습니다. 운영 부담이 큽니다.
    
- **B. SSE-S3 (S3 관리 키)**
    - S3가 키를 관리하여 암호화는 제공하나 **키 사용을 KMS 수준에서 CloudTrail로 상세 로깅하지 못합니다**(KMS API 호출 로그가 없음). 또한 사용자가 키 회전 정책을 직접 제어할 수 없습니다. 감사·회전 요구를 충족시키기 어렵습니다.
    
- **C. SSE-KMS with manual rotation**
    - KMS 사용으로 로깅은 가능하지만, **수동 회전은 운영 오버헤드가 크고 사고 위험(사람 실수 등)** 이 존재합니다. 자동 회전에 비해 운영 효율성이 떨어집니다.


## #107
자전거 공유 회사는 피크 운영 시간 동안 자전거의 위치를 추적하기 위한 멀티티어 아키텍처를 개발 중입니다. 회사는 이 데이터 포인트들을 기존 분석 플랫폼에서 사용하려고 합니다. 솔루션 설계자는 이 아키텍처를 지원할 수 있는 가장 적합한 멀티티어 옵션을 결정해야 합니다. 데이터 포인트는 REST API를 통해 접근 가능해야 합니다.  
어떤 조치가 위치 데이터 저장 및 검색 요구사항을 충족합니까?

A. Amazon Athena와 Amazon S3를 사용합니다.  
B. Amazon API Gateway와 AWS Lambda를 사용합니다.  
C. Amazon QuickSight와 Amazon Redshift를 사용합니다.  
D. Amazon API Gateway와 Amazon Kinesis Data Analytics를 사용합니다.

```
A bicycle sharing company is developing a multi-tier architecture to track the location of its bicycles during peak operating hours. The company wants to use these data points in its existing analytics platform. A solutions architect must determine the most viable multi-tier option to support this architecture. The data points must be accessible from the REST API.  
Which action meets these requirements for storing and retrieving location data?

- A. Use Amazon Athena with Amazon S3.
- B. Use Amazon API Gateway with AWS Lambda.
- C. Use Amazon QuickSight with Amazon Redshift.
- D. Use Amazon API Gateway with Amazon Kinesis Data Analytics.
```

정답 : `D`

- 요구사항
	- 피크 시간 동안 대량의 실시간/근실시간(near-real-time) 위치 데이터 수집
	- 수집된 데이터가 기존 분석 플랫폼으로 전달
	- 데이터 포인트는 REST API를 통해 접근/전송 가능해야함
	- 멀티 티어 아키텍처를 선호
- API Gateway는 REST API 엔드포인트를 제공하여 모바일/디바이스가 위치 이벤트(위치 포인트)를 전송하게 해줌
- API Gateway를 통해 들어온 이벤트를 Kinesis Data Streams / Firehose로 전달하고, Kinesis Data Analytics를 사용하면 스트리밍 데이터를 SQL 스타일로 실시간 처리 및 집계할 수 있음
- 처리된 결과는 분석 플랫폼(ex. Redshift, S3, Elasticsearch)으로 실시간 전달하거나 상태 저장을 위해 DynamoDB에 쓸 수 있음

오답 이유

- **A. Amazon Athena + Amazon S3**
    - Athena/S3 조합은 **배치 쿼리(또는 저장된 데이터의 대화형 쿼리)** 에 적합합니다. 실시간 수집·처리에는 부적합하며, REST API를 통한 실시간 쓰기/읽기 시나리오를 직접적으로 지원하지 않습니다.
    
- **B. API Gateway + Lambda**
    - API Gateway + Lambda는 REST 기반 수집 및 소규모 처리에 적합하나, **피크 시간의 고빈도 스트리밍 데이터**를 대규모로 안정적으로 흡수하고 실시간 분석 플랫폼으로 파이프라인을 구성하는 데에는 확장성과 비용 측면에서 덜 적합합니다. (물론 설계에 따라 가능하나, 스트리밍/분석 연동에는 Kinesis가 더 적절)
    
- **C. QuickSight + Redshift**
    - QuickSight는 시각화 도구이고 Redshift는 데이터웨어하우스입니다. 둘은 분석·보고에는 적합하지만 **실시간 데이터 수집(REST API)·스트리밍 처리**를 직접 처리하거나 수집 계층을 대체하지 못합니다.


## #108
한 회사는 자동차 판매 웹사이트를 운영하며, 자동차 매물 정보를 Amazon RDS 데이터베이스에 저장합니다. 자동차가 판매되면, 웹사이트에서 해당 매물이 제거되어야 하고, 그 데이터는 여러 대상 시스템으로 전송되어야 합니다.  
솔루션 설계자가 권장해야 할 설계는 무엇입니까?

A. Amazon RDS 데이터베이스가 업데이트될 때 트리거되는 AWS Lambda 함수를 생성하여 정보를 Amazon Simple Queue Service (Amazon SQS) 큐에 전송하고, 대상 시스템이 이를 소비하도록 합니다.  

B. Amazon RDS 데이터베이스가 업데이트될 때 트리거되는 AWS Lambda 함수를 생성하여 정보를 Amazon Simple Queue Service (Amazon SQS) FIFO 큐에 전송하고, 대상 시스템이 이를 소비하도록 합니다.  

C. RDS 이벤트 알림을 구독하고, Amazon Simple Queue Service (Amazon SQS) 큐로 전송한 뒤 여러 Amazon Simple Notification Service (Amazon SNS) 주제로 팬아웃합니다. AWS Lambda 함수를 사용하여 대상 시스템을 업데이트합니다.  

D. RDS 이벤트 알림을 구독하고, Amazon Simple Notification Service (Amazon SNS) 주제로 전송한 뒤 여러 Amazon Simple Queue Service (Amazon SQS) 큐로 팬아웃합니다. AWS Lambda 함수를 사용하여 대상 시스템을 업데이트합니다.

```
A company has an automobile sales website that stores its listings in a database on Amazon RDS. When an automobile is sold, the listing needs to be removed from the website and the data must be sent to multiple target systems.  
Which design should a solutions architect recommend?

- A. Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) queue for the targets to consume.
- B. Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) FIFO queue for the targets to consume.
- C. Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon SQS) queue fanned out to multiple Amazon Simple Notification Service (Amazon SNS) topics. Use AWS Lambda functions to update the targets.
- D. Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon SNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the targets.
```

정답 : `D`

- RDS 이벤트는 직접 애플리케이션 레벨의 데이터 변경(INSERT, UPDATE, DELETE)을 감지할 수는 없지만, Amazon RDS 이벤트 구독을 통해 특정 이벤트를 SNS로 전달 가능
- SNS -> SQS 팬아웃 패턴은 다수의 구독자를 대상으로 동일 메시지를 전달하는 가장 일반적이고 확장 가능한 방식
- SQS 큐를 소비하는 람다를 연결하면 각 대상 시스템별 맞춤 업데이트 가능

오답 이유

- **A. Lambda + SQS (단일 큐)**
    - 단일 큐에만 데이터를 넣으므로, 다수의 대상 시스템에 팬아웃 전송하려면 각 소비자가 동일 큐를 풀링해야 함 → 운영 복잡도 증가.
    
- **B. Lambda + SQS FIFO 큐**
    - FIFO 큐는 순서 보장 및 중복 제거에 초점. 본 문제는 **순서 보장**이 아니라 **멀티 타겟 전송**이 요구사항이므로 불필요하게 제약이 있는 선택.
    
- **C. RDS 이벤트 → SQS → SNS → Targets**
    - 이벤트 흐름이 잘못됨. RDS 이벤트는 기본적으로 SNS로 전송 가능하나, 바로 SQS로 보내는 구조는 지원되지 않음. 즉, 아키텍처 상 부적절.


## #109
한 회사는 Amazon S3에 데이터를 저장해야 하며, 데이터가 변경되지 않도록 해야 합니다.  
회사는 Amazon S3에 새로 업로드되는 객체가 회사가 객체를 수정하기로 결정할 때까지 일정 기간 동안 변경 불가능하도록 유지되기를 원합니다.  
회사의 AWS 계정에서 특정 사용자만 객체를 삭제할 수 있어야 합니다.  
이 요구사항을 충족하기 위해 솔루션 설계자가 수행해야 할 작업은 무엇입니까?

A. S3 Glacier 금고를 생성합니다. 객체에 대해 쓰기 한 번, 읽기 다회(WORM) 금고 잠금 정책을 적용합니다.  
B. S3 Object Lock이 활성화된 S3 버킷을 생성합니다. 버전 관리를 활성화합니다. 유지 기간을 100년으로 설정합니다. 새로운 객체의 기본 유지 모드로 거버넌스 모드를 사용합니다.  
C. S3 버킷을 생성합니다. 객체를 수정하는 S3 API 이벤트를 추적하도록 AWS CloudTrail을 사용합니다. 알림이 발생하면 회사가 보유한 백업 버전에서 수정된 객체를 복원합니다.  
D. S3 Object Lock이 활성화된 S3 버킷을 생성합니다. 버전 관리를 활성화합니다. 객체에 법적 보류(Legal Hold)를 추가합니다. 객체를 삭제해야 하는 사용자 IAM 정책에 s3:PutObjectLegalHold 권한을 추가합니다.

```
A company needs to store data in Amazon S3 and must prevent the data from being changed. The company wants new objects that are uploaded to Amazon S3 to remain unchangeable for a nonspecific amount of time until the company decides to modify the objects. Only specific users in the company's AWS account can have the ability 10 delete the objects.  
What should a solutions architect do to meet these requirements?

- A. Create an S3 Glacier vault. Apply a write-once, read-many (WORM) vault lock policy to the objects.
- B. Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Set a retention period of 100 years. Use governance mode as the S3 bucket’s default retention mode for new objects.
- C. Create an S3 bucket. Use AWS CloudTrail to track any S3 API events that modify the objects. Upon notification, restore the modified objects from any backup versions that the company has.
- D. Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Add a legal hold to the objects. Add the s3:PutObjectLegalHold permission to the IAM policies of users who need to delete the objects.
```

정답 : `D`

- S3 Object Lock의 Legal Hold 기능은 기간 제한 없이 객체를 변경/삭제할 수 없도록 보호하는 기능
- Legal Hold는 보유를 해제할 권한이 있는 사용자만 해제 가능 -> 특정 사용자만 삭제 가능 조건 충족
- 버전 관리를 활성화하면 객체 변경 시 이전 버전 복원 가능

오답 이유

- **A. S3 Glacier WORM**
    - Glacier Vault Lock은 **장기 보관용 아카이빙**에 적합하며, 삭제/수정 불가 WORM 정책은 기간 기반이며, S3 Object Lock처럼 실시간 접근 제어가 불가능.
    
- **B. Object Lock + Governance Mode + 100년 유지**
    - Governance 모드는 **특정 권한을 가진 사용자**가 객체를 덮어쓰거나 삭제할 수 있음 → “특정 사용자만 삭제 가능” 요구와 정확히 일치하지 않음.
    - Legal Hold가 아니라 Governance Mode를 사용하면 일부 권한 사용자도 제한 없이 삭제 가능.
    
- **C. CloudTrail + 복원**
    - 사후 복원 방식으로, **객체 변경 방지**를 실시간으로 보장하지 못함 → 규제 준수 요구사항 미충족.

## #110
한 소셜 미디어 회사는 사용자가 웹사이트에 이미지를 업로드할 수 있도록 합니다. 웹사이트는 Amazon EC2 인스턴스에서 실행됩니다. 업로드 요청 동안 웹사이트는 이미지를 표준 크기로 조정하고 조정된 이미지를 Amazon S3에 저장합니다.  
사용자들은 웹사이트로의 업로드 요청이 느리다고 보고하고 있습니다.  
회사는 애플리케이션 내 결합도를 줄이고 웹사이트 성능을 향상시킬 필요가 있습니다.  
솔루션 설계자는 이미지 업로드를 위한 가장 운영 효율적인 프로세스를 설계해야 합니다.  
이 요구사항을 충족하기 위해 솔루션 설계자가 수행해야 할 조합은 무엇입니까? (두 가지 선택)

A. 애플리케이션이 이미지를 S3 Glacier에 업로드하도록 구성합니다.  
B. 웹 서버가 원본 이미지를 Amazon S3에 업로드하도록 구성합니다.  
C. 각 사용자의 브라우저가 사전 서명된 URL을 사용하여 Amazon S3로 직접 이미지를 업로드하도록 애플리케이션을 구성합니다.  
D. 이미지가 업로드될 때 AWS Lambda 함수를 호출하도록 S3 이벤트 알림을 구성합니다. 함수를 사용하여 이미지를 조정합니다.  
E. 업로드된 이미지를 조정하기 위해 예약된 일정에 따라 AWS Lambda 함수를 호출하도록 Amazon EventBridge(CloudWatch Events) 규칙을 생성합니다.

```
A social media company allows users to upload images to its website. The website runs on Amazon EC2 instances. During upload requests, the website resizes the images to a standard size and stores the resized images in Amazon S3. Users are experiencing slow upload requests to the website.  
The company needs to reduce coupling within the application and improve website performance. A solutions architect must design the most operationally efficient process for image uploads.  
Which combination of actions should the solutions architect take to meet these requirements? (Choose two.)

- A. Configure the application to upload images to S3 Glacier.
- B. Configure the web server to upload the original images to Amazon S3.
- C. Configure the application to upload images directly from each user's browser to Amazon S3 through the use of a presigned URL
- D. Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded. Use the function to resize the image.
- E. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that invokes an AWS Lambda function on a schedule to resize uploaded images.
```

정답 : `C, D`

- C : 사용자의 브라우저가 직접 S3에 업로드하도록 하면 EC2 웹 서버가 이미지 업로드 트래픽을 처리하지 않아도 되어 웹사이트 성능 개선
- D : S3 Event Notifications를 통해 람다가 이미지 크기 조정을 처리하면 웹 서버와 이미지 처리 로직의 결합도를 줄임

오답 이유

- **A. S3 Glacier**
    - Glacier는 장기 보관용 스토리지로, **즉시 액세스 불가** → 업로드 후 바로 사용해야 하는 이미지 처리에 적합하지 않음.
    
- **B. 웹 서버가 원본 이미지를 S3에 업로드**
    - 웹 서버가 여전히 업로드 트래픽을 처리 → EC2 부하 해소 불가.
    
- **E. EventBridge를 사용하여 예약된 Lambda 실행**
    - 예약 기반 처리 → 이미지를 업로드 직후 처리할 수 없음 → **사용자가 업로드 직후 이미지를 이용해야 하는 요구사항 불충족**.


## #111
한 회사가 최근 메시지 처리 시스템을 AWS로 마이그레이션했습니다.  
시스템은 Amazon EC2 인스턴스에서 실행되는 ActiveMQ 큐로 메시지를 수신합니다.  
메시지는 Amazon EC2에서 실행되는 컨슈머 애플리케이션이 처리합니다.  
컨슈머 애플리케이션은 메시지를 처리하고 결과를 Amazon EC2에서 실행되는 MySQL 데이터베이스에 기록합니다.  
회사는 이 애플리케이션이 **높은 가용성**을 가지면서 **낮은 운영 복잡도**를 갖기를 원합니다.  

어떤 아키텍처가 가장 높은 가용성을 제공합니까?

A. 다른 가용 영역에 두 번째 ActiveMQ 서버를 추가합니다. 다른 가용 영역에 추가 컨슈머 EC2 인스턴스를 추가합니다. MySQL 데이터베이스를 다른 가용 영역에 복제합니다.  
B. Amazon MQ를 사용하여 두 개의 가용 영역에 걸쳐 액티브/스탠바이 브로커를 구성합니다. 다른 가용 영역에 추가 컨슈머 EC2 인스턴스를 추가합니다. MySQL 데이터베이스를 다른 가용 영역에 복제합니다.  
C. Amazon MQ를 사용하여 두 개의 가용 영역에 걸쳐 액티브/스탠바이 브로커를 구성합니다. 다른 가용 영역에 추가 컨슈머 EC2 인스턴스를 추가합니다. Amazon RDS for MySQL을 사용하고 Multi-AZ를 활성화합니다.  
D. Amazon MQ를 사용하여 두 개의 가용 영역에 걸쳐 액티브/스탠바이 브로커를 구성합니다. 두 개의 가용 영역에 걸쳐 컨슈머 EC2 인스턴스용 Auto Scaling 그룹을 추가합니다. Amazon RDS for MySQL을 사용하고 Multi-AZ를 활성화합니다.

```
A company recently migrated a message processing system to AWS. The system receives messages into an ActiveMQ queue running on an Amazon EC2 instance. Messages are processed by a consumer application running on Amazon EC2. The consumer application processes the messages and writes results to a MySQL database running on Amazon EC2. The company wants this application to be highly available with low operational complexity.  
Which architecture offers the HIGHEST availability?

- A. Add a second ActiveMQ server to another Availability Zone. Add an additional consumer EC2 instance in another Availability Zone. Replicate the MySQL database to another Availability Zone.
- B. Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an additional consumer EC2 instance in another Availability Zone. Replicate the MySQL database to another Availability Zone.
- C. Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an additional consumer EC2 instance in another Availability Zone. Use Amazon RDS for MySQL with Multi-AZ enabled.
- D. Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an Auto Scaling group for the consumer EC2 instances across two Availability Zones. Use Amazon RDS for MySQL with Multi-AZ enabled.
```

정답 : `D`

- Amazon MQ with asctive/standby brokers -> 메시지 브로커의 고가용성 확보
- 오토 스케일링 그룹으로 EC2 컨슈머 인스턴스 -> 컨슈머가 가용 영역 장애 시 자동으로 복구되고 부하 분산 가능
- Amazon RDS for MySQL with Multi-AZ -> 데이터베이스 장애 시 자동 페일오버, 관리형 서비스로 운영 복잡도 최소화

오답 이유

- **A:**
    - ActiveMQ를 EC2에 직접 설치 → 브로커 관리 및 장애 복구 수동.
    - MySQL도 EC2 → 관리 복잡도 높음.
    - HA는 가능하지만 **운영 부담이 큼**.
    
- **B:**
    - 브로커를 Amazon MQ로 전환 → HA 확보, 관리 부담 감소.
    - 컨슈머는 단일 EC2 → 한 가용 영역 장애 시 메시지 처리 중단.
    - MySQL은 EC2 복제 → 운영 부담 여전히 높음.
    
- **C:**
    - 컨슈머는 여전히 단일 EC2 → 장애 시 처리 중단.
    - RDS Multi-AZ로 DB HA 확보 → 개선되었지만, 컨슈머 HA 부족.


## #112
한 회사가 컨테이너화된 웹 애플리케이션을 온프레미스 서버에서 호스팅하고 있습니다.  
애플리케이션은 들어오는 요청을 처리합니다.  
요청 수가 빠르게 증가하고 있으며, 온프레미스 서버는 증가하는 요청 수를 처리할 수 없습니다.  
회사는 애플리케이션을 최소한의 코드 변경과 최소한의 개발 노력으로 AWS로 이전하고자 합니다.  

어떤 솔루션이 **가장 낮은 운영 부담(LEAST operational overhead)**으로 이러한 요구사항을 충족합니까?

A. Amazon Elastic Container Service(Amazon ECS)에서 AWS Fargate를 사용하여 컨테이너화된 웹 애플리케이션을 실행합니다. 서비스 Auto Scaling을 사용합니다. Application Load Balancer를 사용하여 들어오는 요청을 분산합니다.  

B. 두 개의 Amazon EC2 인스턴스를 사용하여 컨테이너화된 웹 애플리케이션을 호스팅합니다. Application Load Balancer를 사용하여 들어오는 요청을 분산합니다.  

C. AWS Lambda를 사용하여 지원되는 언어 중 하나로 새 코드를 작성합니다. 요청 부하를 지원하기 위해 여러 Lambda 함수를 생성합니다. Amazon API Gateway를 Lambda 함수의 진입점으로 사용합니다.  

D. AWS ParallelCluster와 같은 고성능 컴퓨팅(HPC) 솔루션을 사용하여 들어오는 요청을 적절한 규모로 처리할 수 있는 HPC 클러스터를 구축합니다.

```
A company hosts a containerized web application on a fleet of on-premises servers that process incoming requests. The number of requests is growing quickly. The on-premises servers cannot handle the increased number of requests. The company wants to move the application to AWS with minimum code changes and minimum development effort.  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized web application with Service Auto Scaling. Use an Application Load Balancer to distribute the incoming requests.
- B. Use two Amazon EC2 instances to host the containerized web application. Use an Application Load Balancer to distribute the incoming requests.
- C. Use AWS Lambda with a new code that uses one of the supported languages. Create multiple Lambda functions to support the load. Use Amazon API Gateway as an entry point to the Lambda functions.
- D. Use a high performance computing (HPC) solution such as AWS ParallelCluster to establish an HPC cluster that can process the incoming requests at the appropriate scale.
```

정답 : `A`

- Fargate + ECS -> 서버를 관리할 필요 없이 컨테이너 실행 가능 : 최소 운영 부담
- Service Auto Scaling -> 요청 수에 따라 컨테이너 수를 자동 조정
- Application Load Balancer -> 들어오는 요청을 컨테이너에 분산, 기존 컨테이너 코드를 거의 변경하지 않고 사용 가능

오답 이유

- **B:**
    - EC2를 직접 관리해야 하며, 컨테이너 관리 및 확장 자동화 필요 → 운영 부담 높음.
    - 최소 코드 변경은 가능하지만, Auto Scaling 설정, OS/패치 관리 등 관리 필요.
    
- **C:**
    - 기존 컨테이너 코드를 Lambda로 전환 → **코드 변경 필요**, 언어 및 환경 제약 존재.
    - 운영 부담은 낮지만, 최소 개발 노력 요구사항을 만족하지 않음.
    
- **D:**
    - HPC 솔루션은 대규모 연산용으로 설계됨, 웹 요청 처리에는 과도함.
    - 운영 복잡도 높고, 기존 애플리케이션 코드 재작성 필요.


## #113
한 회사는 보고용으로 50 TB의 데이터를 사용합니다.  
회사는 이 데이터를 온프레미스에서 AWS로 이전하고자 합니다.  
회사의 데이터 센터에서 사용자 정의 애플리케이션이 매주 데이터 변환 작업을 수행합니다.  
회사는 데이터 이전이 완료될 때까지 애플리케이션을 일시 중지하고자 하며, 데이터 이전 프로세스를 가능한 빨리 시작해야 합니다.  
데이터 센터에는 추가 워크로드를 위한 네트워크 대역폭이 없습니다.  
솔루션스 아키텍트는 데이터를 이전하고 변환 작업이 AWS Cloud에서 계속 실행되도록 구성해야 합니다.  

어떤 솔루션이 **최소 운영 부담(LEAST operational overhead)**으로 이러한 요구사항을 충족합니까?

A. AWS DataSync를 사용하여 데이터를 이동합니다. AWS Glue를 사용하여 사용자 정의 변환 작업을 생성합니다.  
B. AWS Snowcone 장치를 주문하여 데이터를 이동합니다. 변환 애플리케이션을 장치에 배포합니다.  
C. AWS Snowball Edge Storage Optimized 장치를 주문합니다. 데이터를 장치에 복사합니다. AWS Glue를 사용하여 사용자 정의 변환 작업을 생성합니다.  
D. Amazon EC2 컴퓨팅이 포함된 AWS Snowball Edge Storage Optimized 장치를 주문합니다. 데이터를 장치에 복사합니다. 변환 애플리케이션을 실행할 새 EC2 인스턴스를 AWS에 생성합니다.

```
A company uses 50 TB of data for reporting. The company wants to move this data from on premises to AWS. A custom application in the company’s data center runs a weekly data transformation job. The company plans to pause the application until the data transfer is complete and needs to begin the transfer process as soon as possible.  
The data center does not have any available network bandwidth for additional workloads. A solutions architect must transfer the data and must configure the transformation job to continue to run in the AWS Cloud.  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS DataSync to move the data. Create a custom transformation job by using AWS Glue.
- B. Order an AWS Snowcone device to move the data. Deploy the transformation application to the device.
- C. Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom transformation job by using AWS Glue.
- D. Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy the data to the device. Create a new EC2 instance on AWS to run the transformation application.
```


정답 : `C`

- Snowball Edge Storage Optimized -> 50 TB와 같은 대규모 데이터를 네트워크 없이 오프라인으로 안전하게 전송 가능
- 데이터가 AWS로 도착한 후 AWS Glue를 사용해 변환 작업 실행 -> 기존 데이터 변환 애플리케이션을 AWS Glue로 옮겨 최소한의 운영부담으로 클라우드에서 계속 실행 가능
- 네트워크 제약이 있으므로 DataSync는 적합하지 않음
- Snowcone은 용량이 충분하지 않아 50 TB 처리 불가
- Snowball Edge + EC2는 불필요하게 EC2를 별도로 관리해야하므로 운영 부담 증가

오답 이유

- **A:**
    - DataSync는 네트워크 기반 서비스이므로 데이터 센터에 충분한 네트워크 대역폭이 필요 → 조건 불만족.
    
- **B:**
    - Snowcone 장치 용량이 50 TB에 비해 부족 → 데이터 전체 이전 불가.
    
- **D:**
    - Snowball Edge의 EC2 기능을 사용하고 별도 EC2 인스턴스를 생성하는 것은 Glue 사용보다 운영 부담이 커짐.
    - Glue 사용(C)이 더 간단하고 서버리스 방식으로 운영 부담 최소화 가능.


## #114
한 회사는 사용자가 사진을 업로드하고 이미지에 사진 프레임을 추가할 수 있는 이미지 분석 애플리케이션을 만들었습니다.  
사용자는 이미지를 업로드하고, 이미지에 어떤 사진 프레임을 추가할지 나타내는 메타데이터를 업로드합니다.  
애플리케이션은 단일 Amazon EC2 인스턴스와 Amazon DynamoDB를 사용하여 메타데이터를 저장합니다.  

애플리케이션의 인기가 높아지고 있으며, 사용자 수가 증가하고 있습니다.  
회사는 동시 사용자 수가 시간대 및 요일에 따라 크게 변동할 것으로 예상합니다.  
회사는 애플리케이션이 증가하는 사용자 수요를 충족하도록 **자동으로 확장**할 수 있도록 보장해야 합니다.  

어떤 솔루션이 이러한 요구사항을 충족합니까?

A. AWS Lambda를 사용하여 사진을 처리합니다. 사진과 메타데이터를 DynamoDB에 저장합니다.  
B. Amazon Kinesis Data Firehose를 사용하여 사진을 처리하고, 사진과 메타데이터를 저장합니다.  
C. AWS Lambda를 사용하여 사진을 처리합니다. 사진은 Amazon S3에 저장하고, DynamoDB는 메타데이터를 저장하는 용도로 유지합니다.  
D. EC2 인스턴스 수를 3개로 늘립니다. 사진과 메타데이터를 저장하는 데 Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) 볼륨을 사용합니다.

```
A company has created an image analysis application in which users can upload photos and add photo frames to their images. The users upload images and metadata to indicate which photo frames they want to add to their images. The application uses a single Amazon EC2 instance and Amazon DynamoDB to store the metadata.  
The application is becoming more popular, and the number of users is increasing. The company expects the number of concurrent users to vary significantly depending on the time of day and day of week. The company must ensure that the application can scale to meet the needs of the growing user base.  
Which solution meats these requirements?

- A. Use AWS Lambda to process the photos. Store the photos and metadata in DynamoDB.
- B. Use Amazon Kinesis Data Firehose to process the photos and to store the photos and metadata.
- C. Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain DynamoDB to store the metadata.
- D. Increase the number of EC2 instances to three. Use Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volumes to store the photos and metadata.
```

정답 : `C`

- AWS Lambda -> 서버리스 방식으로 동시 사용자 수의 변동에 따라 자동으로 확장 가능
- 사진 저장은 Amazon S3 -> 대용량 데이터에 적합하며 내구성이 높고 비용 효율적
- DynamoDB -> 메타데이터 저장 용도로 계속 사용 가능

오답 이유

- **A:**
    - Lambda + DynamoDB만 사용하면 큰 파일(이미지)을 DynamoDB에 저장하게 되어 **비용이 매우 높고 성능 문제가 발생**할 수 있음.
    
- **B:**
    - Kinesis Data Firehose는 스트리밍 데이터 처리용으로 적합하며, 이미지 파일과 같은 **대용량 바이너리 데이터를 처리하기에는 부적합**.
    
- **D:**
    - EC2를 3개로 확장하고 EBS io2를 사용하면 수동으로 인스턴스를 관리해야 하고, 사용량 변동에 **자동 확장이 불가** → 운영 부담 증가.


## #115
한 의료 기록 회사가 Amazon EC2 인스턴스에서 애플리케이션을 호스팅하고 있습니다.  
애플리케이션은 Amazon S3에 저장된 고객 데이터 파일을 처리합니다.  
EC2 인스턴스는 퍼블릭 서브넷에 호스팅되어 있습니다.  
EC2 인스턴스는 인터넷을 통해 Amazon S3에 접근하지만, 다른 네트워크 접근은 필요하지 않습니다.  

새로운 요구사항에서는 파일 전송 네트워크 트래픽이 인터넷을 거치지 않고 **프라이빗 경로**를 통해 이루어져야 합니다.  

이 요구사항을 충족하기 위해 솔루션스 아키텍트가 권장해야 하는 네트워크 아키텍처 변경은 무엇입니까?

A. NAT 게이트웨이를 생성합니다. 퍼블릭 서브넷의 라우트 테이블을 구성하여 Amazon S3로의 트래픽을 NAT 게이트웨이를 통해 보내도록 설정합니다.  
B. EC2 인스턴스의 보안 그룹을 구성하여 S3 프리픽스 목록으로의 트래픽만 허용되도록 아웃바운드 트래픽을 제한합니다.  
C. EC2 인스턴스를 프라이빗 서브넷으로 이동합니다. Amazon S3용 VPC 엔드포인트를 생성하고, 해당 엔드포인트를 프라이빗 서브넷 라우트 테이블에 연결합니다.  
D. VPC에서 인터넷 게이트웨이를 제거합니다. AWS Direct Connect 연결을 설정하고, Amazon S3로의 트래픽을 Direct Connect를 통해 라우팅합니다.

```
A medical records company is hosting an application on Amazon EC2 instances. The application processes customer data files that are stored on Amazon S3. The EC2 instances are hosted in public subnets. The EC2 instances access Amazon S3 over the internet, but they do not require any other network access.  
A new requirement mandates that the network traffic for file transfers take a private route and not be sent over the internet.  
Which change to the network architecture should a solutions architect recommend to meet this requirement?

- A. Create a NAT gateway. Configure the route table for the public subnets to send traffic to Amazon S3 through the NAT gateway.
- B. Configure the security group for the EC2 instances to restrict outbound traffic so that only traffic to the S3 prefix list is permitted.
- C. Move the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the endpoint to the route table for the private subnets.
- D. Remove the internet gateway from the VPC. Set up an AWS Direct Connect connection, and route traffic to Amazon S3 over the Direct Connect connection.
```


정답 : `C`

- EC2를 프라이빗 서브넷으로 이동 -> 인터넷 게이트웨이를 사용하지 않음
- VPC 엔드포인트 (S3 Gateway Endpoint)를 사용하면 트래픽이 AWS 네트워크 내에서 직접 S3로 전달되며 인터넷을 거치지 않음
- 엔드포인트를 프라이빗 서브넷 라우트 테이블에 연결 -> EC2에서 S3 액세스 시 인터넷 사용 없음

오답 이유

- **A:**
    - NAT 게이트웨이를 사용하면 EC2가 퍼블릭 서브넷을 통해 인터넷으로 나가게 됨 → 트래픽은 여전히 인터넷을 통과. 요구사항 위반.
    
- **B:**
    - 보안 그룹는 트래픽 필터링만 가능 → 트래픽 경로를 제어할 수 없음. 트래픽은 여전히 인터넷을 통과.
    
- **D:**
    - Direct Connect는 온프레미스 → AWS 연결용. S3 액세스를 위해 별도로 구성할 수 있으나 **복잡하며 최소 운영 부담을 제공하지 않음**. 요구사항을 간단히 해결할 수 있는 방법 아님.


## #116
한 회사가 기업 웹사이트를 위해 인기 있는 콘텐츠 관리 시스템(CMS)을 사용하고 있습니다.  
그러나 필요한 패치와 유지 관리가 부담스럽습니다.  
회사는 웹사이트를 재설계하고 새 솔루션을 원합니다.  
웹사이트는 연간 4회 업데이트되며, 동적 콘텐츠가 반드시 필요하지 않습니다.  
솔루션은 높은 확장성과 향상된 보안을 제공해야 합니다.  

운영 부담이 가장 적은 방식으로 요구사항을 충족하는 변경 조합은 무엇입니까? (두 가지 선택)

A. 웹사이트 앞에 Amazon CloudFront를 구성하여 HTTPS 기능을 사용합니다.  
B. 웹사이트 앞에 AWS WAF 웹 ACL을 배포하여 HTTPS 기능을 제공합니다.  
C. 웹사이트 콘텐츠를 관리하고 제공하기 위해 AWS Lambda 함수를 생성하고 배포합니다.  
D. 새 웹사이트를 생성하고 Amazon S3 버킷을 만듭니다. S3 버킷에서 정적 웹사이트 호스팅을 활성화하여 웹사이트를 배포합니다.  
E. 새 웹사이트를 생성합니다. 애플리케이션 로드 밸런서 뒤에 Auto Scaling 그룹으로 Amazon EC2 인스턴스를 배포하여 웹사이트를 운영합니다.

```
A company uses a popular content management system (CMS) for its corporate website. However, the required patching and maintenance are burdensome. The company is redesigning its website and wants anew solution. The website will be updated four times a year and does not need to have any dynamic content available. The solution must provide high scalability and enhanced security.  
Which combination of changes will meet these requirements with the LEAST operational overhead? (Choose two.)

- A. Configure Amazon CloudFront in front of the website to use HTTPS functionality.
- B. Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality.
- C. Create and deploy an AWS Lambda function to manage and serve the website content.
- D. Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.
- E. Create the new website. Deploy the website by using an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer.
```

정답 : `A, D`

- 동적 콘텐츠 필요 없음 -> 정적 웹사이트 가능
- 연간 4회 업데이트 -> 빈번한 변경 X -> 정적 호스팅 적합
- 높은 확장성과 보안 -> CloudFront + S3 조합으로 제공 가능
- D: S3 정적 웹사이트 호스팅 -> 운영 부담 최소화, 서버 관리 불필요
- A: CloudFront 배포 -> 전 세계 사용자에게 빠른 콘텐츠 제공, HTTPS 지원, 보안 강화


오답 이유

- **B:**
    - AWS WAF는 웹 애플리케이션 방화벽 기능 제공.
    - HTTPS 제공 기능은 아니며, S3/CloudFront 없이 단독으로 HTTPS 지원 불가.
    
- **C:**
    - Lambda를 사용하면 서버리스 동적 처리 필요.
    - 단순 정적 사이트에는 불필요하며, 운영 부담 증가.
    
- **E:**
    - EC2 + ALB + Auto Scaling → 서버 관리 필요, 동적 트래픽 처리용.
    - 정적 사이트에서는 불필요하며, 운영 부담 증가.


## #117
한 회사가 애플리케이션 로그를 Amazon CloudWatch Logs 로그 그룹에 저장합니다.  
새 정책에 따라 회사는 모든 애플리케이션 로그를 거의 실시간으로 Amazon OpenSearch Service(Amazon Elasticsearch Service)에 저장해야 합니다.  

운영 부담이 가장 적은 방식으로 이 요구사항을 충족하는 솔루션은 무엇입니까?

A. CloudWatch Logs 구독을 구성하여 로그를 Amazon OpenSearch Service(Amazon Elasticsearch Service)로 스트리밍합니다.  

B. AWS Lambda 함수를 생성합니다. 로그 그룹을 사용하여 함수를 호출하고 로그를 Amazon OpenSearch Service(Amazon Elasticsearch Service)에 기록합니다.  

C. Amazon Kinesis Data Firehose 전달 스트림을 생성합니다. 로그 그룹을 전달 스트림의 소스로 구성하고 Amazon OpenSearch Service(Amazon Elasticsearch Service)를 전달 스트림의 대상으로 구성합니다.  

D. 각 애플리케이션 서버에 Amazon Kinesis Agent를 설치하고 구성하여 로그를 Amazon Kinesis Data Streams로 전송합니다. Kinesis Data Streams를 구성하여 로그를 Amazon OpenSearch Service(Amazon Elasticsearch Service)로 전달합니다.

```
A company stores its application logs in an Amazon CloudWatch Logs log group. A new policy requires the company to store all application logs in Amazon OpenSearch Service (Amazon Elasticsearch Service) in near-real time.  
Which solution will meet this requirement with the LEAST operational overhead?

- A. Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).
- B. Create an AWS Lambda function. Use the log group to invoke the function to write the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).
- C. Create an Amazon Kinesis Data Firehose delivery stream. Configure the log group as the delivery streams sources. Configure Amazon OpenSearch Service (Amazon Elasticsearch Service) as the delivery stream's destination.
- D. Install and configure Amazon Kinesis Agent on each application server to deliver the logs to Amazon Kinesis Data Streams. Configure Kinesis Data Streams to deliver the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).
```


정답 : `A`

- CloudWatch Logs 구독을 통해 바로 OpenSearch로 스트리밍 가능
- 서버리스 방식이며 운영 부담이 최소화
- 추가 인프라 관리나 람다, Kinesis 구성 필요 없음

오답 이유

- **B:**
    - Lambda를 사용하면 동작 가능하지만, 로그 처리량이 많으면 Lambda 관리 및 배포, 오류 처리 등 운영 부담 증가.
    
- **C:**
    - Firehose 사용 가능하지만, CloudWatch Logs를 직접 소스로 연결하려면 Firehose 설정이 필요하며, Lambda 없이도 A 옵션이 더 간단.
    
- **D:**
    - Kinesis Agent 설치 필요 → 서버마다 설치 및 관리 필요.
    - 운영 부담이 매우 높음.

## #118
한 회사가 여러 가용 영역(AZ)에서 Amazon EC2 인스턴스로 실행되는 웹 기반 애플리케이션을 구축하고 있습니다.  
웹 애플리케이션은 총 900TB에 달하는 텍스트 문서 저장소에 대한 액세스를 제공합니다.  
회사는 웹 애플리케이션이 높은 수요를 경험할 수 있는 시기가 있을 것으로 예상합니다.  
솔루션 아키텍트는 웹 애플리케이션의 수요를 항상 충족할 수 있도록 텍스트 문서 저장 구성 요소가 확장 가능하도록 해야 합니다.  
회사는 솔루션의 전체 비용에 대해서도 우려하고 있습니다.  

이 요구사항을 가장 비용 효율적으로 충족하는 스토리지 솔루션은 무엇입니까?

A. Amazon Elastic Block Store (Amazon EBS)  
B. Amazon Elastic File System (Amazon EFS)  
C. Amazon OpenSearch Service (Amazon Elasticsearch Service)  
D. Amazon S3

```
A company is building a web-based application running on Amazon EC2 instances in multiple Availability Zones. The web application will provide access to a repository of text documents totaling about 900 TB in size. The company anticipates that the web application will experience periods of high demand. A solutions architect must ensure that the storage component for the text documents can scale to meet the demand of the application at all times. The company is concerned about the overall cost of the solution.  
Which storage solution meets these requirements MOST cost-effectively?

- A. Amazon Elastic Block Store (Amazon EBS)
- B. Amazon Elastic File System (Amazon EFS)
- C. Amazon OpenSearch Service (Amazon Elasticsearch Service)
- D. Amazon S3
```

정답 : `D`

- 요구사항
	- 900TB 규모의 텍스트 문서
	- 웹 애플리케이션 접근
	- 수요 변화 대응 가능
	- 비용 효율적
- Amazon S3는 페타바이트 수준까지 확장 가능하며 EC2에서 직접 액세스 가능
- EBS/EFS는 수백 TB 규모에서는 비용이 매우 높고 관리 부담도 큼
- OpenSearch는 검색 및 분석용으로 최적화, 단순 저장용으로는 비용 과다
- S3는 객체 스토리지로서 읽기/쓰기 성능과 확장성을 자동으로 제공하고 비용 효율적


오답 이유

- **A. Amazon EBS**
    - 단일 인스턴스 블록 스토리지. 여러 AZ에서 공유 불가 → 확장성 부족
    - 900TB 저장 시 비용 과다
    
- **B. Amazon EFS**
    - NFS 기반 파일 스토리지. 다중 AZ 지원 가능하지만, 대규모 데이터(900TB)에서는 비용 매우 높음
    
- **C. Amazon OpenSearch Service**
    - 검색과 분석 용도에 적합. 단순 대용량 객체 저장에는 부적합
    - 900TB 저장 시 비용 및 관리 부담 큼


## #119
한 글로벌 회사가 Amazon API Gateway를 사용하여 미국(us-east-1) 및 아시아-태평양(ap-southeast-2) 리전에서 로열티 클럽 사용자를 위한 REST API를 설계하고 있습니다.  
솔루션 아키텍트는 SQL 인젝션과 크로스 사이트 스크립팅(XSS) 공격으로부터 여러 계정에 걸쳐 있는 API Gateway 관리 REST API를 보호할 수 있는 솔루션을 설계해야 합니다.  

관리 부담을 최소화하면서 이러한 요구사항을 충족하는 솔루션은 무엇입니까?

A. 두 리전에서 AWS WAF를 설정합니다. Regional 웹 ACL을 API 스테이지에 연결합니다.  
B. 두 리전에서 AWS Firewall Manager를 설정합니다. AWS WAF 규칙을 중앙에서 구성합니다.  
C. 두 리전에서 AWS Shield를 설정합니다. Regional 웹 ACL을 API 스테이지에 연결합니다.  
D. 한 리전에서 AWS Shield를 설정합니다. Regional 웹 ACL을 API 스테이지에 연결합니다.

```
A global company is using Amazon API Gateway to design REST APIs for its loyalty club users in the us-east-1 Region and the ap-southeast-2 Region. A solutions architect must design a solution to protect these API Gateway managed REST APIs across multiple accounts from SQL injection and cross-site scripting attacks.  
Which solution will meet these requirements with the LEAST amount of administrative effort?

- A. Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage.
- B. Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules.
- C. Set up AWS Shield in bath Regions. Associate Regional web ACLs with an API stage.
- D. Set up AWS Shield in one of the Regions. Associate Regional web ACLs with an API stage.
```

정답 : `B`

- 요구사항
	- SQL 인젝션 및 XSS 공격 방지
	- 여러 계정 및 여러 리전
	- 관리 부담 최소화
- AWS WAF는 웹 애플리케이션 공격(SQLi, XSS 등) 방지 기능 제공
- AWS Firewall Manager를 사용하면 여러 계정 및 여러 리전에 걸쳐 WAF 규칙을 중앙에서 관리 가능
- 직접 각 리전과 계정에서 WAF를 구성하는 것보다 관리 편의성 및 일관성이 뛰어남

오답 이유

- **A. 두 리전에서 AWS WAF를 직접 설정**
    - 기능상 가능하지만, 여러 계정과 리전에서 개별 관리 → 관리 부담 큼
    
- **C,D. AWS Shield**
    - Shield는 DDoS 보호용 서비스
    - SQLi/XSS 공격 방지 기능 없음
    - Shield와 웹 ACL 연계는 불가능하며 요구사항에 부적합


## #120
한 회사는 us-west-2 리전에서 Network Load Balancer(NLB) 뒤에 있는 세 개의 Amazon EC2 인스턴스에서 자체 관리 DNS 솔루션을 구현했습니다.  
대부분의 사용자는 미국과 유럽에 위치합니다. 회사는 솔루션의 성능과 가용성을 향상하고자 합니다.  
회사는 eu-west-1 리전에서 세 개의 EC2 인스턴스를 시작하고 구성한 다음, 새로운 NLB의 대상(Target)으로 추가했습니다.  

모든 EC2 인스턴스로 트래픽을 라우팅할 수 있는 솔루션은 무엇입니까?

A. Amazon Route 53 지리 위치(Geolocation) 라우팅 정책을 만들어 두 NLB 중 하나로 요청을 라우팅합니다. Amazon CloudFront 배포를 생성하고 Route 53 레코드를 배포의 오리진으로 사용합니다.  
B. AWS Global Accelerator의 표준 가속기를 생성합니다. us-west-2와 eu-west-1에 엔드포인트 그룹을 생성합니다. 두 NLB를 엔드포인트 그룹의 엔드포인트로 추가합니다.  
C. 여섯 EC2 인스턴스에 Elastic IP 주소를 연결합니다. Amazon Route 53 지리 위치 라우팅 정책을 만들어 요청을 여섯 EC2 인스턴스 중 하나로 라우팅합니다. Amazon CloudFront 배포를 생성하고 Route 53 레코드를 배포의 오리진으로 사용합니다.  
D. 두 NLB를 두 개의 Application Load Balancer(ALB)로 교체합니다. Amazon Route 53 지연 시간(Latency) 라우팅 정책을 만들어 요청을 두 ALB 중 하나로 라우팅합니다. Amazon CloudFront 배포를 생성하고 Route 53 레코드를 배포의 오리진으로 사용합니다.

```
A company has implemented a self-managed DNS solution on three Amazon EC2 instances behind a Network Load Balancer (NLB) in the us-west-2 Region. Most of the company's users are located in the United States and Europe. The company wants to improve the performance and availability of the solution. The company launches and configures three EC2 instances in the eu-west-1 Region and adds the EC2 instances as targets for a new NLB.  
Which solution can the company use to route traffic to all the EC2 instances?

- A. Create an Amazon Route 53 geolocation routing policy to route requests to one of the two NLBs. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution’s origin.
- B. Create a standard accelerator in AWS Global Accelerator. Create endpoint groups in us-west-2 and eu-west-1. Add the two NLBs as endpoints for the endpoint groups.
- C. Attach Elastic IP addresses to the six EC2 instances. Create an Amazon Route 53 geolocation routing policy to route requests to one of the six EC2 instances. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution's origin.
- D. Replace the two NLBs with two Application Load Balancers (ALBs). Create an Amazon Route 53 latency routing policy to route requests to one of the two ALBs. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution’s origin.
```

정답 : `B`

- AWS Global Accelerator는 전 세계적으로 사용자를 가까운 리전의 엔드포인트(NLB, ALB, EC2)로 라우팅하여 지연 시간 최소화 및 가용성 향상
- 엔드포인트 그룹을 각 리전에 새엉하고 NLB를 엔드포인트로 추가하면, 미국과 유럽 사용자에게 가장 가까운 NLB로 자동 라우팅 가능
- Route 53 지리 위치 라우팅보다 자동 최적화 및 글로벌 Failvoer 기능 제공

오답 이유

- **A. Route 53 + CloudFront**
    - CloudFront는 콘텐츠 캐싱 중심이며, **동적 DNS 트래픽 라우팅 최적화 기능 부족**
    - NLB 대상 라우팅에는 적합하지 않음
    
- **C. Elastic IP + Route 53 지리 위치 + CloudFront**
    - EC2 개별 인스턴스 직접 라우팅은 **운영 부담 높음**, 장애 복구 기능 부족
    
- **D. ALB + Route 53 Latency + CloudFront**
    - ALB는 DNS 기반 라우팅 최적화 가능하지만, **글로벌 트래픽 최적화와 Failover 관리**는 Global Accelerator가 더 적합
    - 또한 기존 NLB를 ALB로 교체하면 TCP 수준 트래픽 처리에서 제한 가능