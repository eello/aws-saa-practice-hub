---
created: 2025-10-13 09:31:49
last_modified: 2025-10-13 09:51:35
---
## #421
한 회사는 고가용성 SFTP 서비스를 운영하고 있습니다. SFTP 서비스는 인터넷의 신뢰할 수 있는 IP 소스에서의 트래픽을 수신하기 위해 탄력적 IP 주소(elastic IP addresses)로 실행되는 두 개의 Amazon EC2 Linux 인스턴스를 사용합니다. SFTP 서비스는 인스턴스에 연결된 공유 스토리지로 지원됩니다. 사용자 계정은 SFTP 서버에서 Linux 사용자로 생성 및 관리됩니다.

회사는 높은 IOPS 성능과 매우 구성 가능한 보안을 제공하는 서버리스 옵션을 원합니다. 또한 회사는 사용자 권한에 대한 제어를 유지하고자 합니다.

어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. 암호화된 Amazon Elastic Block Store(Amazon EBS) 볼륨을 생성합니다. 신뢰할 수 있는 IP 주소만 허용하는 퍼블릭 엔드포인트를 갖는 AWS Transfer Family SFTP 서비스를 생성합니다. EBS 볼륨을 SFTP 서비스 엔드포인트에 연결합니다. 사용자에게 SFTP 서비스에 대한 액세스를 부여합니다.
B. 암호화된 Amazon Elastic File System(Amazon EFS) 볼륨을 생성합니다. 탄력적 IP 주소와 인터넷 연결 액세스를 갖춘 VPC 엔드포인트를 사용하는 AWS Transfer Family SFTP 서비스를 생성합니다. 신뢰할 수 있는 IP 주소만 허용하는 보안 그룹을 엔드포인트에 연결합니다. EFS 볼륨을 SFTP 서비스 엔드포인트에 연결합니다. 사용자에게 SFTP 서비스에 대한 액세스를 부여합니다.
C. 기본 암호화가 활성화된 Amazon S3 버킷을 생성합니다. 신뢰할 수 있는 IP 주소만 허용하는 퍼블릭 엔드포인트를 갖는 AWS Transfer Family SFTP 서비스를 생성합니다. S3 버킷을 SFTP 서비스 엔드포인트에 연결합니다. 사용자에게 SFTP 서비스에 대한 액세스를 부여합니다.
D. 기본 암호화가 활성화된 Amazon S3 버킷을 생성합니다. 프라이빗 서브넷에서 내부 액세스를 갖는 VPC 엔드포인트를 사용하는 AWS Transfer Family SFTP 서비스를 생성합니다. 신뢰할 수 있는 IP 주소만 허용하는 보안 그룹을 연결합니다. S3 버킷을 SFTP 서비스 엔드포인트에 연결합니다. 사용자에게 SFTP 서비스에 대한 액세스를 부여합니다.

```
A company runs a highly available SFTP service. The SFTP service uses two Amazon EC2 Linux instances that run with elastic IP addresses to accept traffic from trusted IP sources on the internet. The SFTP service is backed by shared storage that is attached to the instances. User accounts are created and managed as Linux users in the SFTP servers.  
  
The company wants a serverless option that provides high IOPS performance and highly configurable security. The company also wants to maintain control over user permissions.  
  
Which solution will meet these requirements?

- A. Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the EBS volume to the SFTP service endpoint. Grant users access to the SFTP service.
- B. Create an encrypted Amazon Elastic File System (Amazon EFS) volume. Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access. Attach a security group to the endpoint that allows only trusted IP addresses. Attach the EFS volume to the SFTP service endpoint. Grant users access to the SFTP service.
- C. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.
- D. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a VPC endpoint that has internal access in a private subnet. Attach a security group that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.
```

정답 : `B`

- Transfer Family는 SFTP 서버를 완전관리형으로 제공하며 EC2 운영과 패치/가용성 관리를 제거
- EFS는 POSIX 파일시스템으로 대규모 동시성, 높은 IOPS/처리량(모드/성능 설정 및 필요시 Provisioned Throughput)과 공유 접근을 제공
- VPC 호스티드 엔드포인트(인터넷 연결형)를 사용하면 EIP 할당과 보안 그룹으로 소스 IP 허용 목록을 적용 가능
- EFS 백엔드와 POSIX UID/GID/디렉터리 권한을 매핑해 세밀한 사용자별 권한 유지 가능

오답 이유

- **A. EBS를 Transfer Family 엔드포인트에 연결**    
    Transfer Family는 **백엔드로 S3 또는 EFS만 지원**하며 **EBS 연결은 불가**합니다. 기술적으로 성립하지 않습니다.

- **C. S3 백엔드 + 퍼블릭 엔드포인트(IP 제한)**    
    서버리스와 IP 제한은 가능하지만, **S3는 객체 스토리지**로 POSIX 권한/디렉터리 퍼미션 기반 제어가 필요할 때 제약이 큼. 또한 **IOPS/메타데이터 작업 빈번한 SFTP 워크로드**에선 EFS가 더 적합합니다.

- **D. S3 백엔드 + 프라이빗 서브넷 내부 전용 VPC 엔드포인트**
    내부 전용이면 **인터넷의 신뢰 IP 소스에서 직접 접속 불가**(별도의 VPN/Direct Connect/프록시 필요). 문제의 전제(인터넷 신뢰 IP에서 접속)와 상충합니다. 또한 S3의 권한/IOPS 특성 문제는 C와 동일.


## #422
한 회사가 AWS에서 새로운 머신 러닝(ML) 모델 솔루션을 개발하고 있습니다.  
이 모델들은 독립적인 마이크로서비스로 개발되며, 각 모델은 시작 시 Amazon S3에서 약 1GB의 모델 데이터를 가져와 메모리에 로드합니다.  
사용자들은 비동기 API를 통해 모델에 접근합니다.  
사용자는 단일 요청 또는 여러 요청(batch)을 보낼 수 있으며, 결과를 받을 위치를 지정할 수 있습니다.  

회사는 수백 명의 사용자에게 모델을 제공합니다.  
모델의 사용 패턴은 불규칙합니다. 일부 모델은 며칠 또는 몇 주 동안 사용되지 않을 수 있고,  
다른 모델은 한 번에 수천 개의 요청 배치를 받을 수도 있습니다.  

이러한 요구 사항을 충족하기 위해 솔루션스 아키텍트는 어떤 설계를 추천해야 합니까?

A. API의 요청을 Network Load Balancer(NLB)로 전달합니다. 모델을 AWS Lambda 함수로 배포하고 NLB가 Lambda를 호출하도록 구성합니다.  
B. API의 요청을 Application Load Balancer(ALB)로 전달합니다. 모델을 Amazon Elastic Container Service(Amazon ECS) 서비스로 배포하고, Amazon Simple Queue Service(Amazon SQS) 큐에서 읽어오도록 구성합니다. AWS App Mesh를 사용하여 SQS 큐 크기에 따라 ECS 클러스터 인스턴스를 확장합니다.  
C. API의 요청을 Amazon Simple Queue Service(Amazon SQS) 큐로 전달합니다. 모델을 AWS Lambda 함수로 배포하고 SQS 이벤트로 Lambda가 호출되도록 구성합니다. SQS 큐 크기에 따라 Lambda 함수의 vCPU 수를 조정하기 위해 AWS Auto Scaling을 사용합니다.  
D. API의 요청을 Amazon Simple Queue Service(Amazon SQS) 큐로 전달합니다. 모델을 Amazon Elastic Container Service(Amazon ECS) 서비스로 배포하고 큐에서 읽어오도록 구성합니다. SQS 큐 크기에 따라 ECS 클러스터와 서비스의 복제본 수를 모두 Auto Scaling하도록 설정합니다.

```
A company is developing a new machine learning (ML) model solution on AWS. The models are developed as independent microservices that fetch approximately 1 GB of model data from Amazon S3 at startup and load the data into memory. Users access the models through an asynchronous API. Users can send a request or a batch of requests and specify where the results should be sent.  
  
The company provides models to hundreds of users. The usage patterns for the models are irregular. Some models could be unused for days or weeks. Other models could receive batches of thousands of requests at a time.  
  
Which design should a solutions architect recommend to meet these requirements?

- A. Direct the requests from the API to a Network Load Balancer (NLB). Deploy the models as AWS Lambda functions that are invoked by the NLB.
- B. Direct the requests from the API to an Application Load Balancer (ALB). Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from an Amazon Simple Queue Service (Amazon SQS) queue. Use AWS App Mesh to scale the instances of the ECS cluster based on the SQS queue size.
- C. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as AWS Lambda functions that are invoked by SQS events. Use AWS Auto Scaling to increase the number of vCPUs for the Lambda functions based on the SQS queue size.
- D. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.
```

정답 : `D`


- 모델은 시작 시 S3에서 약 1GB의 데이터를 로드 → 콜드 스타트 비용이 크므로 Lambda 부적합
- 모델의 요청 패턴은 불규칙 → 자동 확장 및 축소가 필수 → 오토 스케일링 정책을 통해 SQS 큐의 길이에 따라 ECS 서비스 Task 개수를 동적 조정 가능
- 비동기 요청 처리 → 큐 기반 설계 필요 → SQS 큐는 요청을 버퍼링 및 비동기 처리하므로 폭주 트래픽 완화 가능
- Fargate 런타임을 사용하면 서버리스처럼 관리 부담을 줄일 수 있고 사용하지 않을 때 비용이 거의 없음

오답 이유

- **A. NLB → Lambda**
    - Lambda는 **1GB 모델 파일을 매번 cold start 시 로드**해야 하므로 초기 응답 지연이 매우 큼.        
    - 또한 Lambda는 짧은 실행 시간(최대 15분) 및 제한된 메모리 환경으로 대용량 ML 모델에 부적합.

- **B. ALB → ECS + SQS + App Mesh**
    - App Mesh는 서비스 간 트래픽 제어용(서비스 메시)으로 **SQS 기반 Auto Scaling과 무관**.
    - ALB를 통해 직접 ECS로 요청을 보내면 **비동기 처리(버퍼링)** 불가.
    - 따라서 SQS 큐 기반 설계가 아닌 이 접근은 비효율적.

- **C. SQS → Lambda**
    - SQS로 비동기 트리거는 가능하지만, Lambda는 모델 로딩(1GB) 시 **매 호출마다 cold start 부하**가 발생.
    - 또한 Lambda의 메모리/스토리지 한계로 대용량 ML 모델 유지에 부적합.
    - Auto Scaling “vCPU 조정”은 Lambda에 존재하지 않는 개념입니다(동시성 제어는 가능하나 vCPU 직접 지정 불가).

## #423
솔루션스 아키텍트는 다음 JSON 텍스트를 아이덴티티 기반(Identity-based) 정책으로 사용하여 특정 권한을 부여하려고 합니다:

```
{
	"Statement": [
		{
			"Action": [
				"ssm:ListDocuments",
				"ssm:GetDocument"
			],
			"Effect": "Allow",
			"Resource": "*",
			"Sid": ""
		}
	],
	"Version": "2012-10-17"
}
```

이 정책을 어떤 IAM 주체(Principal)에 연결할 수 있습니까? (두 개를 선택하십시오.)

A. 역할(Role)  
B. 그룹(Group)  
C. 조직(Organization)  
D. Amazon Elastic Container Service (Amazon ECS) 리소스  
E. Amazon EC2 리소스

```
A solutions architect wants to use the following JSON text as an identity-based policy to grant specific permissions:  
  
{
	"Statement": {
		"Action": [
			"ssm:ListDocuments",
			"ssm:GetDocument"
		],
		"Effect": "Allow",
		"Resource": "*",
		"Sid": ""
	}
}
  
Which IAM principals can the solutions architect attach this policy to? (Choose two.)

- A. Role
- B. Group
- C. Organization
- D. Amazon Elastic Container Service (Amazon ECS) resource
- E. Amazon EC2 resource
```

정답 : `A, B`

- JSON은 아이덴티티 기반 정책으로 IAM 사용자나 그룹, 역할에 직접 연결할 수 있음
- Role은 AWS 서비스나 외부 주체가 임시로 권한을 위임받을 때 사용되므로, 아이덴티티 기반 정책 연결 가능
- Group은 여러 사용자에게 동일한 권한을 부여할 수 있는 IAM 엔티티로 정책 연결 가능

오답 이유

- **C. Organization** → AWS Organizations는 **서비스 제어 정책(SCP)** 을 사용하는 **리소스 기반 정책이 아닌 상위 수준 정책**을 관리하며, IAM 아이덴티티 정책을 직접 연결할 수 없습니다.
    
- **D. Amazon ECS resource** → ECS 서비스나 태스크 정의, 클러스터에는 **리소스 기반 정책(Resource-based policy)** 만 적용 가능하며, 아이덴티티 정책을 직접 붙일 수 없습니다.
    
- **E. Amazon EC2 resource** → EC2 인스턴스는 IAM 정책을 직접 가질 수 없습니다. EC2에 연결하려면 **인스턴스 프로파일을 통해 Role에 정책을 부착**해야 합니다.


## #424
한 회사가 Amazon EC2 온디맨드 인스턴스에서 커스텀 애플리케이션을 실행하고 있습니다. 이 애플리케이션에는 하루 24시간, 주 7일 내내 실행되어야 하는 프런트엔드 노드와, 워크로드에 따라 짧은 시간 동안만 실행되어야 하는 백엔드 노드가 있습니다. 백엔드 노드의 수는 하루 동안 변동됩니다.

회사는 워크로드에 따라 더 많은 인스턴스를 스케일 아웃/스케일 인해야 합니다.

다음 중 어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적으로 충족합니까?

A. 프런트엔드 노드에는 예약 인스턴스를 사용합니다. 백엔드 노드에는 AWS Fargate를 사용합니다.
B. 프런트엔드 노드에는 예약 인스턴스를 사용합니다. 백엔드 노드에는 스팟 인스턴스를 사용합니다.
C. 프런트엔드 노드에는 스팟 인스턴스를 사용합니다. 백엔드 노드에는 예약 인스턴스를 사용합니다.
D. 프런트엔드 노드에는 스팟 인스턴스를 사용합니다. 백엔드 노드에는 AWS Fargate를 사용합니다.

```
A company is running a custom application on Amazon EC2 On-Demand Instances. The application has frontend nodes that need to run 24 hours a day, 7 days a week and backend nodes that need to run only for a short time based on workload. The number of backend nodes varies during the day.  
  
The company needs to scale out and scale in more instances based on workload.  
  
Which solution will meet these requirements MOST cost-effectively?

- A. Use Reserved Instances for the frontend nodes. Use AWS Fargate for the backend nodes.
- B. Use Reserved Instances for the frontend nodes. Use Spot Instances for the backend nodes.
- C. Use Spot Instances for the frontend nodes. Use Reserved Instances for the backend nodes.
- D. Use Spot Instances for the frontend nodes. Use AWS Fargate for the backend nodes.
```

정답 : `B`

- 항상 가동해야하는 프런트엔드는 RI(예약 인스턴스) 또는 Savings Plans로 장기 약정 시 가장 낮은 단가 확보 가능
- 단기･변동형인 백엔드는 중단 허용이 있는 워크로드이므로 스팟 인스턴스로 대폭 비용 절감이 가능
	- 오토스케일링으로 수요 변화에 따라 유연하게 증감

오답 이유

- **A. RI + Fargate**: 백엔드를 컨테이너로 재패키징해야 하며, Fargate는 편의성은 높지만 스팟 대비 **단가가 더 높을 수 있어** “가장 비용 효율적” 요건에 덜 부합합니다.
    
- **C. Front에 스팟, Back에 RI**: 프런트엔드는 **24x7 필수 가용성** 요구가 있어 스팟의 **예고 중단** 위험을 감수하기 어렵습니다. 백엔드는 항상 켜둘 필요가 없어 **RI가 비효율적**입니다.
    
- **D. Front에 스팟, Back에 Fargate**: 프런트 스팟은 **가용성 리스크**가 크고, 백엔드 Fargate는 스팟보다 **비용 절감 효과가 낮을 수 있어** 전체적으로 비용 최적화에 불리합니다.


## #425
한 회사는 온프레미스에서 워크로드를 실행하기 위해 높은 블록 스토리지 용량을 사용합니다. 회사의 일일 피크 입출력 트랜잭션 초당 수는 15,000 IOPS를 넘지 않습니다. 회사는 워크로드를 Amazon EC2로 마이그레이션하고 스토리지 용량과 독립적으로 디스크 성능을 프로비저닝하고자 합니다.

어떤 Amazon Elastic Block Store (Amazon EBS) 볼륨 유형이 이러한 요구 사항을 가장 비용 효율적으로 충족합니까?

A. GP2 볼륨 유형
B. io2 볼륨 유형
C. GP3 볼륨 유형
D. io1 볼륨 유형

```
A company uses high block storage capacity to runs its workloads on premises. The company's daily peak input and output transactions per second are not more than 15,000 IOPS. The company wants to migrate the workloads to Amazon EC2 and to provision disk performance independent of storage capacity.  
  
Which Amazon Elastic Block Store (Amazon EBS) volume type will meet these requirements MOST cost-effectively?

- A. GP2 volume type
- B. io2 volume type
- C. GP3 volume type
- D. io1 volume type
```

정답 : `C`

- GP3는 용량과 독립적으로 IOPS/처리량을 프로비저닝 할 수 있으며(최대 16,000 IOPS, 1,000MB/s)
- 동일 용량 기준 gp2 대비 저렴

오답 이유

- **A. GP2**: IOPS가 **용량에 비례(약 3 IOPS/GB)** 하여 성능을 독립적으로 설정할 수 없습니다. 원하는 15k IOPS를 안정적으로 확보하려면 큰 용량이 필요해 **비용 비효율**입니다.
    
- **B. io2**: **프로비저닝 IOPS SSD**로 고성능·고내구성이 장점이지만 **단가가 높음**. 15k IOPS는 GP3로 충분히 충족 가능하므로 **가성비 열위**입니다.
    
- **D. io1**: 구세대 프로비저닝 IOPS SSD로 **비용이 높고** io2 대비도 열위입니다. 요구 성능 수준에 **과스펙/과금**입니다.

## #426
한 회사는 자사의 헬스케어 애플리케이션에서 생성되는 데이터를 저장해야 합니다. 애플리케이션의 데이터는 자주 변경됩니다. 새로운 규제에 따라 저장된 데이터의 모든 수준에서 감사(Audit) 액세스가 요구됩니다.

회사는 온프레미스 인프라에서 애플리케이션을 호스팅 중이며, 스토리지 용량이 부족해지고 있습니다. 솔루션스 아키텍트는 새로운 규제를 충족하면서 기존 데이터를 AWS로 안전하게 마이그레이션해야 합니다.

다음 중 어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. AWS DataSync를 사용하여 기존 데이터를 Amazon S3로 이동합니다. AWS CloudTrail을 사용하여 데이터 이벤트를 로깅합니다.
B. AWS Snowcone을 사용하여 기존 데이터를 Amazon S3로 이동합니다. AWS CloudTrail을 사용하여 관리 이벤트를 로깅합니다.
C. Amazon S3 Transfer Acceleration을 사용하여 기존 데이터를 Amazon S3로 이동합니다. AWS CloudTrail을 사용하여 데이터 이벤트를 로깅합니다.
D. AWS Storage Gateway를 사용하여 기존 데이터를 Amazon S3로 이동합니다. AWS CloudTrail을 사용하여 관리 이벤트를 로깅합니다.

```
A company needs to store data from its healthcare application. The application’s data frequently changes. A new regulation requires audit access at all levels of the stored data.  
  
The company hosts the application on an on-premises infrastructure that is running out of storage capacity. A solutions architect must securely migrate the existing data to AWS while satisfying the new regulation.  
  
Which solution will meet these requirements?

- A. Use AWS DataSync to move the existing data to Amazon S3. Use AWS CloudTrail to log data events.
- B. Use AWS Snowcone to move the existing data to Amazon S3. Use AWS CloudTrail to log management events.
- C. Use Amazon S3 Transfer Acceleration to move the existing data to Amazon S3. Use AWS CloudTrail to log data events.
- D. Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log management events.
```

정답 : `A`

- 감사 요구사항은 S3의 객체 수준 활동까지 추적해야 하므로 CloudTrail "데이터 이벤트" 활성화가 핵심
- DataSync는 온프레미스 → S3로의 보안･대규모･증분 동기화에 특화되어 있으며, 변경이 잦은 데이터를 효율적으로 지속 마이그레이션/동기화

오답 이유

- **B. Snowcone + 관리 이벤트**: Snowcone은 **오프라인/에지 전송** 장비로 **일회성 벌크 이관**엔 유용하지만, “자주 변경”되는 데이터를 지속 반영하기엔 부적합합니다. 또한 **관리 이벤트**만으로는 **객체 수준(Data events)** 감사를 충족하지 못합니다.
    
- **C. S3 Transfer Acceleration + 데이터 이벤트**: Transfer Acceleration은 **클라이언트 전 세계 업로드 가속** 목적이지, **온프레미스 파일 시스템의 대규모/증분 이관 및 검증**을 자동화하지 않습니다. **변경 동기화/검증/스케줄링** 면에서 DataSync 대비 운용 부담이 큽니다.
    
- **D. Storage Gateway + 관리 이벤트**: File Gateway로 S3에 파일을 올릴 수는 있으나, **CloudTrail 관리 이벤트만**으로는 **객체 수준 감사** 요구를 충족하지 못합니다. 또한 기존 대용량 데이터의 **초기 대규모 이관/검증/증분 처리**에는 DataSync가 더 적합합니다.


## #427
솔루션스 아키텍트가 MySQL 데이터베이스를 사용하는 복잡한 Java 애플리케이션을 구현하고 있습니다.  
이 Java 애플리케이션은 Apache Tomcat에서 배포되어야 하며, 고가용성을 가져야 합니다.

다음 중 솔루션스 아키텍트가 이러한 요구 사항을 충족하기 위해 수행해야 할 작업은 무엇입니까?

A. 애플리케이션을 AWS Lambda에 배포합니다. Amazon API Gateway API를 구성하여 Lambda 함수와 연결합니다.  
B. AWS Elastic Beanstalk을 사용하여 애플리케이션을 배포합니다. 로드 밸런싱된 환경과 롤링 배포 정책을 구성합니다.  
C. 데이터베이스를 Amazon ElastiCache로 마이그레이션합니다. ElastiCache 보안 그룹을 구성하여 애플리케이션에서의 액세스를 허용합니다.  
D. Amazon EC2 인스턴스를 시작합니다. 인스턴스에 MySQL 서버를 설치합니다. 서버에 애플리케이션을 구성합니다. AMI를 생성합니다. Auto Scaling 그룹과 함께 사용할 시작 템플릿을 생성합니다.

```
A solutions architect is implementing a complex Java application with a MySQL database. The Java application must be deployed on Apache Tomcat and must be highly available.  
  
What should the solutions architect do to meet these requirements?

- A. Deploy the application in AWS Lambda. Configure an Amazon API Gateway API to connect with the Lambda functions.
- B. Deploy the application by using AWS Elastic Beanstalk. Configure a load-balanced environment and a rolling deployment policy.
- C. Migrate the database to Amazon ElastiCache. Configure the ElastiCache security group to allow access from the application.
- D. Launch an Amazon EC2 instance. Install a MySQL server on the EC2 instance. Configure the application on the server. Create an AMI. Use the AMI to create a launch template with an Auto Scaling group.
```

정답 : `B`

- Elastic Beanstalk는 Java 및 Apache Tomcat을 완전히 지원하는 PaaS 서비스
	- 고가용성(로드 밸런서 + Auto Scaling) 및 자동 배포/롤백/모니터링을 쉽게 구성 가능
- MySQL 데이터베이스를 RDS로 분리해 관리할 수 있으며, Beanstalk의 환경 구성으로 EC2, ALB, Auto Scaling을 모두 자동화 가능

오답 이유

- **A. Lambda + API Gateway**: Lambda는 **짧은 실행 시간(15분 제한)** 기반의 서버리스 함수 환경입니다. Apache Tomcat 기반의 **상태 유지형, 장시간 실행 Java 애플리케이션**은 부적합합니다.
    
- **C. ElastiCache로 마이그레이션**: ElastiCache는 **캐싱 서비스(Redis/Memcached)** 이며, **데이터베이스를 대체할 수 없습니다.** MySQL을 캐시로 교체하는 것은 요구사항과 불일치합니다.
    
- **D. EC2 + AMI + Auto Scaling**: 수동 설정과 관리(패치, 로드밸런싱, 배포 관리 등)의 **운영 부담이 큼**. Elastic Beanstalk이 제공하는 **자동화된 고가용성 배포 및 관리 기능**을 직접 구현해야 하므로 **비효율적**입니다.


## #428
서버리스 애플리케이션은 Amazon API Gateway, AWS Lambda, 그리고 Amazon DynamoDB를 사용합니다. Lambda 함수는 DynamoDB 테이블에 읽기 및 쓰기 권한이 필요합니다.

다음 중 어떤 솔루션이 Lambda 함수에 가장 안전하게 DynamoDB 테이블에 대한 액세스를 제공합니까?

A. Lambda 함수에 대한 프로그래매틱 액세스를 가진 IAM 사용자를 생성합니다. DynamoDB 테이블에 읽기 및 쓰기 액세스를 허용하는 정책을 사용자에게 연결합니다. access_key_id와 secret_access_key 매개변수를 Lambda 환경 변수의 일부로 저장합니다. 다른 AWS 사용자가 Lambda 함수 구성을 읽고 쓰기 액세스하지 못하도록 합니다.
B. 신뢰할 수 있는 서비스로 Lambda를 포함하는 IAM 역할을 생성합니다. DynamoDB 테이블에 읽기 및 쓰기 액세스를 허용하는 정책을 역할에 연결합니다. Lambda 함수의 구성을 업데이트하여 새 역할을 실행 역할로 사용하도록 합니다.
C. Lambda 함수에 대한 프로그래매틱 액세스를 가진 IAM 사용자를 생성합니다. DynamoDB 테이블에 읽기 및 쓰기 액세스를 허용하는 정책을 사용자에게 연결합니다. access_key_id와 secret_access_key 매개변수를 AWS Systems Manager Parameter Store의 보안 문자열 매개변수로 저장합니다. Lambda 함수 코드가 DynamoDB 테이블에 연결하기 전에 보안 문자열 매개변수를 검색하도록 업데이트합니다.
D. 신뢰할 수 있는 서비스로 DynamoDB를 포함하는 IAM 역할을 생성합니다. Lambda 함수로부터 읽기 및 쓰기 액세스를 허용하는 정책을 역할에 연결합니다. 새로운 역할을 실행 역할로 연결하도록 Lambda 함수 코드를 업데이트합니다.

```
A serverless application uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. The Lambda function needs permissions to read and write to the DynamoDB table.  
  
Which solution will give the Lambda function access to the DynamoDB table MOST securely?

- A. Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the DynamoDB table. Store the access_key_id and secret_access_key parameters as part of the Lambda environment variables. Ensure that other AWS users do not have read and write access to the Lambda function configuration.
- B. Create an IAM role that includes Lambda as a trusted service. Attach a policy to the role that allows read and write access to the DynamoDB table. Update the configuration of the Lambda function to use the new role as the execution role.
- C. Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the DynamoDB table. Store the access_key_id and secret_access_key parameters in AWS Systems Manager Parameter Store as secure string parameters. Update the Lambda function code to retrieve the secure string parameters before connecting to the DynamoDB table.
- D. Create an IAM role that includes DynamoDB as a trusted service. Attach a policy to the role that allows read and write access from the Lambda function. Update the code of the Lambda function to attach to the new role as an execution role.
```

정답 : `B`

- 가장 안전한 방법은 IAM 역할(Execution Role)을 사용해 AWS Lambda(신뢰 주체)가 실행 시점에 임시 자격 증명(STS)을 자동으로 받아 DynamoDB에 접근하게 하는 것
- 정적 액세스 키를 저장하지 않으므로 키 유출 위험이 없고, 권한은 최소권한 정책으로 역할에만 부여

오답 이유

- **A**: IAM 사용자 키를 **환경 변수에 저장**하는 것은 보안상 취약합니다. 키 순환과 유출 위험이 존재하며, 서버리스에 권장되지 않습니다.
    
- **C**: Parameter Store로 키를 안전하게 저장하더라도 **정적 자격 증명 사용 자체**가 모범사례가 아닙니다. Lambda 실행 역할을 쓰는 것이 더 안전하고 단순합니다.
    
- **D**: 역할의 **신뢰 주체(trust policy)** 는 **Lambda** 여야 합니다. DynamoDB는 신뢰 주체가 아니며, 코드에서 역할을 “붙이는” 방식이 아니라 **함수 설정에 실행 역할을 지정**해야 합니다.


## #429
다음 IAM 정책이 IAM 그룹에 연결되어 있습니다. 이 그룹에 적용된 정책은 이것뿐입니다.

```
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Sid": "1",
			"Effect": "Allow",
			"Action": "ec2:*",
			"Resource": "*",
			"Condition": {
				"StringEquals": {
					"ec2:Region": "us-east-1"
				}
			}
		},
		{
			"Sid": "2",
			"Effect": "Deny",
			"Action": [
				"ec2:StopInstances",
				"ec2:TerminateInstances"
			],
			"Resource": "*",
			"Condition": {
				"BoolIfExists": { "aws:MultiFactorAuthPresent": false }
			}
		}
	]
}
```

이 정책에 대한 그룹 구성원의 유효 IAM 권한은 무엇입니까?

A. 그룹 구성원은 us-east-1 리전 내에서 모든 Amazon EC2 작업이 허용됩니다. Allow 권한 이후의 문은 적용되지 않습니다.
B. 그룹 구성원은 다중 요소 인증(MFA)으로 로그인하지 않는 한 us-east-1 리전에서 모든 Amazon EC2 권한이 거부됩니다.
C. 그룹 구성원은 다중 요소 인증(MFA)으로 로그인할 때 모든 리전에 대해 ec2:StopInstances 및 ec2:TerminateInstances 권한이 허용됩니다. 그룹 구성원은 다른 모든 Amazon EC2 작업이 허용됩니다.
D. 그룹 구성원은 us-east-1 리전 내에서만 다중 요소 인증(MFA)으로 로그인할 때 ec2:StopInstances 및 ec2:TerminateInstances 권한이 허용됩니다. 그룹 구성원은 us-east-1 리전 내의 다른 모든 Amazon EC2 작업이 허용됩니다.

```
The following IAM policy is attached to an IAM group. This is the only policy applied to the group.  
  
```
```
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Sid": "1",
			"Effect": "Allow",
			"Action": "ec2:*",
			"Resource": "*",
			"Condition": {
				"StringEquals": {
					"ec2:Region": "us-east-1"
				}
			}
		},
		{
			"Sid": "2",
			"Effect": "Deny",
			"Action": [
				"ec2:StopInstances",
				"ec2:TerminateInstances"
			],
			"Resource": "*",
			"Condition": {
				"BoolIfExists": { "aws:MultiFactorAuthPresent": false }
			}
		}
	]
}
```
```
  
What are the effective IAM permissions of this policy for group members?

- A. Group members are permitted any Amazon EC2 action within the us-east-1 Region. Statements after the Allow permission are not applied.
- B. Group members are denied any Amazon EC2 permissions in the us-east-1 Region unless they are logged in with multi-factor authentication (MFA).
- C. Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for all Regions when logged in with multi-factor authentication (MFA). Group members are permitted any other Amazon EC2 action.
- D. Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for the us-east-1 Region only when logged in with multi-factor authentication (MFA). Group members are permitted any other Amazon EC2 action within the us-east-1 Region.
```

정답 : `D`

- 첫 번째 문은 us-east-1 리전 조건에서 모든 EC2 작업을 허용
- 두 번째 문은 MFA가 없거나 키가 없는 경우 StopInstances/TerminateInstances를 명시적으로 거부
	- 명시적 Deny는 Allow보다 우선하므로, us-east-1에서도 이 두 작업은 MFA가 있을 때만 허용
- 그외 리전에는 Allow가 없으므로 암묵적 거부

오답 이유

- **A**: “Allow 이후 문은 적용되지 않는다”는 잘못입니다. **명시적 Deny는 항상 최우선**이며, StopInstances/TerminateInstances는 **MFA 없으면 거부**됩니다.
    
- **B**: us-east-1에서 **모든 EC2 권한이 MFA 없으면 거부**라는 해석은 과도합니다. 거부 조건은 **두 액션(Stop/Terminate)** 에만 적용됩니다.
    
- **C**: “다른 모든 EC2 작업이 허용”은 **모든 리전**을 의미하나, Allow는 **us-east-1에만** 적용됩니다. 다른 리전은 **암묵적 거부**입니다.


## #430
한 제조 회사에는 기계 센서가 .csv 파일을 Amazon S3 버킷에 업로드합니다. 이 .csv 파일은 이미지로 변환되어야 하며, 그래픽 보고서를 자동으로 생성하기 위해 가능한 한 빨리 사용 가능해야 합니다.

이미지는 1개월 후에는 더 이상 관련이 없지만, .csv 파일은 연 2회 머신 러닝(ML) 모델을 학습시키기 위해 보관해야 합니다. ML 학습과 감사는 수주 전에 계획됩니다.

다음 중 어떤 단계 조합이 이러한 요구 사항을 가장 비용 효율적으로 충족합니까? (두 개를 선택하십시오.)

A. .csv 파일을 매시간 다운로드하고 이미지 파일을 생성한 다음 이미지를 S3 버킷에 업로드하는 Amazon EC2 스팟 인스턴스를 시작합니다.
B. .csv 파일을 이미지로 변환하고 이미지를 S3 버킷에 저장하는 AWS Lambda 함수를 설계합니다. .csv 파일이 업로드될 때 Lambda 함수를 호출합니다.
C. S3 버킷의 .csv 파일과 이미지 파일에 대한 S3 수명 주기 규칙을 생성합니다. 업로드 후 1일 뒤에 .csv 파일을 S3 Standard에서 S3 Glacier로 전환합니다. 이미지 파일은 30일 후 만료시킵니다.
D. S3 버킷의 .csv 파일과 이미지 파일에 대한 S3 수명 주기 규칙을 생성합니다. 업로드 후 1일 뒤에 .csv 파일을 S3 Standard에서 S3 One Zone-Infrequent Access(S3 One Zone-IA)로 전환합니다. 이미지 파일은 30일 후 만료시킵니다.
E. S3 버킷의 .csv 파일과 이미지 파일에 대한 S3 수명 주기 규칙을 생성합니다. 업로드 후 1일 뒤에 .csv 파일을 S3 Standard에서 S3 Standard-Infrequent Access(S3 Standard-IA)로 전환합니다. 이미지 파일은 Reduced Redundancy Storage(RRS)에 보관합니다.

```
A manufacturing company has machine sensors that upload .csv files to an Amazon S3 bucket. These .csv files must be converted into images and must be made available as soon as possible for the automatic generation of graphical reports.  
  
The images become irrelevant after 1 month, but the .csv files must be kept to train machine learning (ML) models twice a year. The ML trainings and audits are planned weeks in advance.  
  
Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)

- A. Launch an Amazon EC2 Spot Instance that downloads the .csv files every hour, generates the image files, and uploads the images to the S3 bucket.
- B. Design an AWS Lambda function that converts the .csv files into images and stores the images in the S3 bucket. Invoke the Lambda function when a .csv file is uploaded.
- C. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Glacier 1 day after they are uploaded. Expire the image files after 30 days.
- D. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 1 day after they are uploaded. Expire the image files after 30 days.
- E. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 1 day after they are uploaded. Keep the image files in Reduced Redundancy Storage (RRS).
```

정답 : `B, C`

- S3 이벤트 → Lambda 변환: 업로드 즉시 트리거되어 가장 빠르게 이미지를 생성･적재하므로 "가능한 한 빨리 사용 가능" 요구 충족
	- 서버 관리 없이 확장성/비용 효율도 우수
- CSV → Glacier, 이미지 30일 만료: CSV는 연 2회만 필요하고 사전 계획이 가능하므로 저렴한 S3 Glacier(현 Flexible Retrieval)로 전환하는 것이 가장 비용 효율적, 이미지는 1개월 후 불필요하므로 30일 만료ㅕ로 저장비 최소화

오답 이유

- **A (EC2 스팟으로 매시간 배치 처리)**: 매시간 폴링은 **지연**을 유발(“가능한 한 빨리” 위배)하며, 인스턴스 관리/오토스케일/실패 복구 등 **운영 오버헤드**가 큽니다. 스팟 회수 리스크도 존재.
    
- **D (CSV→One Zone-IA)**: 접근은 즉시 가능하지만 연 2회, 사전 계획 가능한 워크로드에는 **Glacier가 더 저렴**합니다. 또한 One Zone-IA는 단일 AZ 내 저장으로 **내구성/가용성 리스크**가 상대적으로 큽니다.
    
- **E (CSV→Standard-IA, 이미지 RRS)**: Standard-IA는 Glacier 대비 **저장 단가가 높아** 비효율. **RRS는 더 이상 권장되지 않으며** 표준 대비 내구성이 낮아 규정/감사 관점에서 부적합할 수 있습니다.


## #431
한 회사가 새로운 비디오 게임을 웹 애플리케이션으로 개발했습니다. 애플리케이션은 VPC의 3티어 아키텍처로 구성되어 있으며, 데이터베이스 계층에는 Amazon RDS for MySQL을 사용합니다. 여러 플레이어가 동시에 온라인에서 경쟁할 것입니다. 게임 개발자는 거의 실시간(near-real time)으로 상위 10위(scoreboard)를 표시하고, 현재 점수를 보존한 채 게임을 중지했다가 복원하는 기능을 제공하기를 원합니다.

이 요구사항을 충족하려면 솔루션스 아키텍트는 무엇을 해야 합니까?

A. 웹 애플리케이션이 점수를 표시할 수 있도록 캐시하기 위해 Amazon ElastiCache for Memcached 클러스터를 설정합니다.
B. 웹 애플리케이션이 점수를 표시할 수 있도록 점수를 계산하고 캐시하기 위해 Amazon ElastiCache for Redis 클러스터를 설정합니다.
C. 애플리케이션의 일부 섹션에서 스코어보드를 캐시하기 위해 웹 애플리케이션 앞에 Amazon CloudFront 배포를 배치합니다.
D. 스코어보드를 계산하기 위한 쿼리를 실행하고 웹 애플리케이션에 대한 읽기 트래픽을 제공하기 위해 Amazon RDS for MySQL에서 읽기 전용 복제본을 생성합니다.

```
A company has developed a new video game as a web application. The application is in a three-tier architecture in a VPC with Amazon RDS for MySQL in the database layer. Several players will compete concurrently online. The game’s developers want to display a top-10 scoreboard in near-real time and offer the ability to stop and restore the game while preserving the current scores.  
  
What should a solutions architect do to meet these requirements?

- A. Set up an Amazon ElastiCache for Memcached cluster to cache the scores for the web application to display.
- B. Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores for the web application to display.
- C. Place an Amazon CloudFront distribution in front of the web application to cache the scoreboard in a section of the application.
- D. Create a read replica on Amazon RDS for MySQL to run queries to compute the scoreboard and serve the read traffic to the web application.
```

정답 : `B`

- 거의 실시간 상위 10위 집계는 Redis의 Sorted Set(ZSET)으로 매우 효율적으로 계산/조회 가능
- 중지･복원(데이터 보존) 요구는 Redis의 영속화(RDB/AOF) + 스냅샷/백업 + Multi-AZ로 충족 가능
- 읽기･쓰기 지연이 낮고 원자적 증가 연산으로 레이스 컨디션 없이 순위 업데이트 가능

오답 이유

- **A. Memcached**: 인메모리 캐시로 **영속성(persistence) 없음** → 중지·복원 시 점수 보존 불가. 또한 순위 연산용 **Sorted Set** 같은 고급 자료구조/원자 증가 연산 부재.
    
- **C. CloudFront**: 주로 **정적/캐시 가능한 콘텐츠** 가속용. 스코어보드는 **자주 변하는 동적 데이터**라 엣지 캐싱이 적합하지 않으며 집계 기능도 제공하지 않습니다.
    
- **D. RDS Read Replica**: 순위 계산 쿼리는 **빈번한 정렬/집계로 부하 및 지연**이 크고, 복제 지연으로 **near-real time** 요구를 만족하기 어려움. 또한 중지·복원 시 빠른 원자적 업데이트/조회에 비효율적.


## #432
한 전자상거래 회사가 머신 러닝(ML) 알고리즘을 사용해 모델을 구축하고 학습하려고 합니다. 회사는 이 모델을 사용하여 복잡한 시나리오를 시각화하고 고객 데이터의 트렌드를 감지할 것입니다. 아키텍처 팀은 증강된 데이터를 분석하고 BI 대시보드에서 데이터를 직접 사용하기 위해 ML 모델을 리포팅 플랫폼과 통합하고자 합니다.

다음 중 운영 오버헤드가 가장 적은 솔루션은 무엇입니까?

A. AWS Glue를 사용하여 ML 변환(ML transform)을 생성해 모델을 구축 및 학습합니다. Amazon OpenSearch Service를 사용해 데이터를 시각화합니다.
B. Amazon SageMaker를 사용해 모델을 구축 및 학습합니다. Amazon QuickSight를 사용해 데이터를 시각화합니다.
C. AWS Marketplace의 사전 구성된 ML AMI를 사용해 모델을 구축 및 학습합니다. Amazon OpenSearch Service를 사용해 데이터를 시각화합니다.
D. Amazon QuickSight에서 계산 필드를 사용해 모델을 구축 및 학습합니다. Amazon QuickSight를 사용해 데이터를 시각화합니다.

```
An ecommerce company wants to use machine learning (ML) algorithms to build and train models. The company will use the models to visualize complex scenarios and to detect trends in customer data. The architecture team wants to integrate its ML models with a reporting platform to analyze the augmented data and use the data directly in its business intelligence dashboards.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS Glue to create an ML transform to build and train models. Use Amazon OpenSearch Service to visualize the data.
- B. Use Amazon SageMaker to build and train models. Use Amazon QuickSight to visualize the data.
- C. Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train models. Use Amazon OpenSearch Service to visualize the data.
- D. Use Amazon QuickSight to build and train models by using calculated fields. Use Amazon QuickSight to visualize the data.
```

정답 : `B`

- SageMaker는 데이터 준비, 실험, 학습, 배포(엔드포인트/배치 변환)까지 완전관리형으로 제공하는 ML 플랫폼으로 운영 오버헤드가 가장 낮음
- QuickSight는 S3/RedShift/RDS 등 다양한 소스와 통합되고 BI 대시보드를 손쉽게 구축할 수 있어 증강된 데이터(예: 배치 추론 결과)를 직접 시각화/분석하기에 적합

오답 이유

- **A. Glue ML transform + OpenSearch**: Glue의 **ML Transform**은 주로 **디듀플리케이션/엔티티 매칭** 등 데이터 품질 작업에 초점이며, 일반적인 모델 학습·운영 파이프라인을 대체하지 않습니다. OpenSearch는 로그/검색/시계열 시각화에 강점이나 **전사 BI 대시보드** 용도로는 QuickSight 대비 부적합합니다.
    
- **C. ML AMI + OpenSearch**: AMI 기반 자가 관리(EC2 크기/패치/확장/보안)로 **운영 부담이 큼**. 시각화도 BI 기능이 제한적인 OpenSearch를 쓰므로 요구에 최적이 아님.
    
- **D. QuickSight 계산 필드**: QuickSight는 **ML 모델을 구축/학습하는 플랫폼이 아닙니다**(내장 ML Insights는 예측/이상탐지 등 보조적 기능). 일반 ML 파이프라인 요구를 충족하지 못합니다.


## #433
한 회사가 여러 AWS 계정에서 프로덕션 및 비프로덕션 환경 워크로드를 운영하고 있습니다.  
이 계정들은 AWS Organizations의 조직에 속해 있습니다.  
회사는 비용 사용 태그(Cost Usage Tag)의 수정이 불가능하도록 방지하는 솔루션을 설계해야 합니다.

다음 중 이러한 요구사항을 충족할 수 있는 솔루션은 무엇입니까?

A. 권한이 부여된 주체를 제외하고 태그 수정이 불가능하도록 사용자 지정 AWS Config 규칙을 생성합니다.  
B. 태그 수정을 방지하기 위해 AWS CloudTrail에서 사용자 지정 트레일(custom trail)을 생성합니다.  
C. 권한이 부여된 주체를 제외하고 태그 수정을 방지하기 위해 서비스 제어 정책(SCP)을 생성합니다.  
D. 태그 수정을 방지하기 위해 사용자 지정 Amazon CloudWatch 로그를 생성합니다.

```
A company is running its production and nonproduction environment workloads in multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to design a solution that will prevent the modification of cost usage tags.  
  
Which solution will meet these requirements?

- A. Create a custom AWS Config rule to prevent tag modification except by authorized principals.
- B. Create a custom trail in AWS CloudTrail to prevent tag modification.
- C. Create a service control policy (SCP) to prevent tag modification except by authorized principals.
- D. Create custom Amazon CloudWatch logs to prevent tag modification.
```

정답 : `C`

- SCP는 조직 전체에서 계정의 IAM 권한 상한선을 정의
	- 어떤 IAM 사용자나 역할이 태그를 수정하려 해도, SCP에서 해당 작업을 제한하면 모든 계정 수준에서 차단됨
	- 비용 태그의 무단 변경을 방지하면서, 특정 허용된 주체만 수정 가능하도록 제어 가능

오답 이유

- **A. AWS Config rule** → Config는 **태그 변경을 탐지/알림**할 수는 있지만, **실제 변경을 차단(prevent)** 할 수 없습니다. 사후 모니터링 용도이지 정책 강제는 불가합니다.
    
- **B. CloudTrail custom trail** → CloudTrail은 **감사 및 로깅** 서비스이며, **행위를 방지하는 기능이 없습니다.** 태그 변경은 기록 가능하지만 제어 불가입니다.
    
- **D. CloudWatch logs** → 로그 수집·모니터링 도구로, **권한 제어 기능이 전혀 없습니다.** 태그 수정 방지는 불가능합니다.


## #434
한 회사는 AWS 클라우드에 애플리케이션을 호스팅하고 있습니다. 애플리케이션은 Auto Scaling 그룹의 Amazon EC2 인스턴스에서 실행되며, Elastic Load Balancer 뒤에 있습니다. 또한 Amazon DynamoDB 테이블을 사용합니다. 회사는 최소한의 다운타임으로 다른 AWS 리전에서 애플리케이션을 사용할 수 있도록 보장하고자 합니다.

이 요구사항을 최소한의 다운타임으로 충족하려면 솔루션스 아키텍트는 무엇을 해야 합니까?

A. 재해 복구 리전에 Auto Scaling 그룹과 로드 밸런서를 생성합니다. DynamoDB 테이블을 글로벌 테이블로 구성합니다. DNS 페일오버를 구성하여 새 재해 복구 리전의 로드 밸런서를 가리키도록 합니다.
B. 필요 시 실행되도록 EC2 인스턴스, 로드 밸런서 및 DynamoDB 테이블을 생성하는 AWS CloudFormation 템플릿을 만듭니다. DNS 페일오버를 구성하여 새 재해 복구 리전의 로드 밸런서를 가리키도록 합니다.
C. 필요 시 실행되도록 EC2 인스턴스와 로드 밸런서를 생성하는 AWS CloudFormation 템플릿을 만듭니다. DynamoDB 테이블을 글로벌 테이블로 구성합니다. DNS 페일오버를 구성하여 새 재해 복구 리전의 로드 밸런서를 가리키도록 합니다.
D. 재해 복구 리전에 Auto Scaling 그룹과 로드 밸런서를 생성합니다. DynamoDB 테이블을 글로벌 테이블로 구성합니다. Amazon CloudWatch 알람을 생성하여 AWS Lambda 함수를 트리거하고 Amazon Route 53이 재해 복구 로드 밸런서를 가리키도록 업데이트합니다.

```
A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to ensure the application can be made available in anotherAWS Region with minimal downtime.  
  
What should a solutions architect do to meet these requirements with the LEAST amount of downtime?

- A. Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.
- B. Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be launched when needed Configure DNS failover to point to the new disaster recovery Region's load balancer.
- C. Create an AWS CloudFormation template to create EC2 instances and a load balancer to be launched when needed. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.
- D. Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm to trigger an AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer.
```

정답 : `A`

- 최소 다운타임을 위해서는 DR 리전에 미리(웜 스탠바이) 컴퓨트 계층을 준비하고, 데이터 계층은 DynamoDB Global Tables로 멀티 리전 복제를 유지
- 장애 시 Route 53 DNS 페일오버로 DR 리전의 로드 밸런서로 빠르게 전환하면 애플리케이션과 데이터가 이미 준비되어 있어 다운타임 최소화

오답 이유

- **B**: 리소스를 **필요 시 생성(콜드/필요 시 프로비저닝)** 하면 생성·부팅·배포 시간 때문에 **다운타임이 길어짐**. 또한 DynamoDB 글로벌 구성도 빠짐.
    
- **C**: 데이터는 글로벌 테이블로 괜찮지만, 컴퓨트/로드 밸런서를 **필요 시 생성**하므로 전환 시 **시간 지연** 발생 → 최소 다운타임 요건 불충족.
    
- **D**: A와 유사한 구성이지만, **Route 53의 헬스 체크 기반 DNS 페일오버**만으로 자동 전환이 가능해 **CloudWatch+Lambda로 갱신하는 추가 복잡도**는 불필요함(운영 오버헤드 증가).


## #435
한 회사는 온프레미스 데이터 센터의 MySQL 데이터베이스를 2주 이내에 AWS로 마이그레이션해야 합니다. 데이터베이스 크기는 20 TB입니다. 회사는 최소한의 다운타임으로 마이그레이션을 완료하고자 합니다.

다음 중 어떤 솔루션이 가장 비용 효율적으로 데이터베이스를 마이그레이션합니까?

A. AWS Snowball Edge Storage Optimized 디바이스를 주문합니다. AWS Database Migration Service (AWS DMS)와 AWS Schema Conversion Tool (AWS SCT)을 사용하여 지속적인 변경(replication of ongoing changes) 복제를 통해 데이터베이스를 마이그레이션합니다. 마이그레이션을 완료하고 지속 복제를 계속하기 위해 Snowball Edge 디바이스를 AWS로 보냅니다.
B. AWS Snowmobile 차량을 주문합니다. AWS Database Migration Service (AWS DMS)와 AWS Schema Conversion Tool (AWS SCT)을 사용하여 지속적인 변경을 통해 데이터베이스를 마이그레이션합니다. 마이그레이션을 완료하고 지속 복제를 계속하기 위해 Snowmobile 차량을 AWS로 다시 보냅니다.
C. GPU가 포함된 AWS Snowball Edge Compute Optimized 디바이스를 주문합니다. AWS Database Migration Service (AWS DMS)와 AWS Schema Conversion Tool (AWS SCT)을 사용하여 지속적인 변경을 통해 데이터베이스를 마이그레이션합니다. 마이그레이션을 완료하고 지속 복제를 계속하기 위해 Snowball 디바이스를 AWS로 보냅니다.
D. 데이터 센터와의 연결을 설정하기 위해 전용 1Gb 전용 AWS Direct Connect 연결을 주문합니다. AWS Database Migration Service (AWS DMS)와 AWS Schema Conversion Tool (AWS SCT)을 사용하여 지속적인 변경 복제를 통해 데이터베이스를 마이그레이션합니다.

```
A company needs to migrate a MySQL database from its on-premises data center to AWS within 2 weeks. The database is 20 TB in size. The company wants to complete the migration with minimal downtime.  
  
Which solution will migrate the database MOST cost-effectively?

- A. Order an AWS Snowball Edge Storage Optimized device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes. Send the Snowball Edge device to AWS to finish the migration and continue the ongoing replication.
- B. Order an AWS Snowmobile vehicle. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowmobile vehicle back to AWS to finish the migration and continue the ongoing replication.
- C. Order an AWS Snowball Edge Compute Optimized with GPU device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowball device to AWS to finish the migration and continue the ongoing replication
- D. Order a 1 GB dedicated AWS Direct Connect connection to establish a connection with the data center. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes.
```

정답 : `A`

- 20 TB 대용량 초기 적재는 오프라인 전송(Snowball Edge Storage Optimized)로 빠르고 저렴하게 시드 가능
- DMS DCD(지속 변경 복제)를 병행해 컷오버 시점까지 변경분을 따라가 다운타임 최소화

오답 이유

- **B. Snowmobile**: 엑사바이트급 마이그레이션용으로 **20 TB에는 과도(비용·오버헤드 큼)**, 납기/절차도 무거움.
    
- **C. Snowball Edge Compute Optimized(GPU)**: GPU/컴퓨트가 필요한 워크로드 처리용으로 **스토리지 대역폭/용량 대비 비용이 높고 불필요**. Storage Optimized가 적합.
    
- **D. 1Gb Direct Connect + DMS**: 이론상 대역폭은 충분하지만 **신규 DX 설치·개통까지 수주~수개월** 소요 가능, **일회성 이전에 비용 비효율**. 또한 인터넷/VPN 대비 운영복잡도 증가.

## #436
한 회사가 온프레미스 PostgreSQL 데이터베이스를 Amazon RDS for PostgreSQL DB 인스턴스로 마이그레이션했습니다. 회사는 새 제품을 성공적으로 출시했습니다. 데이터베이스의 워크로드가 증가했습니다. 회사는 인프라를 추가하지 않고 더 큰 워크로드를 수용하기를 원합니다.

다음 중 이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?

A. 총 워크로드에 대해 예약 DB 인스턴스를 구매합니다. Amazon RDS for PostgreSQL DB 인스턴스를 더 크게 만듭니다.
B. Amazon RDS for PostgreSQL DB 인스턴스를 멀티 AZ DB 인스턴스로 만듭니다.
C. 총 워크로드에 대해 예약 DB 인스턴스를 구매합니다. 다른 Amazon RDS for PostgreSQL DB 인스턴스를 추가합니다.
D. Amazon RDS for PostgreSQL DB 인스턴스를 온디맨드 DB 인스턴스로 만듭니다.

```
A company moved its on-premises PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. The company successfully launched a new product. The workload on the database has increased. The company wants to accommodate the larger workload without adding infrastructure.  
  
Which solution will meet these requirements MOST cost-effectively?

- A. Buy reserved DB instances for the total workload. Make the Amazon RDS for PostgreSQL DB instance larger.
- B. Make the Amazon RDS for PostgreSQL DB instance a Multi-AZ DB instance.
- C. Buy reserved DB instances for the total workload. Add another Amazon RDS for PostgreSQL DB instance.
- D. Make the Amazon RDS for PostgreSQL DB instance an on-demand DB instance.
```

정답 : `A`

- 성능 측면에서 수직 확장(인스턴스 클래스 상향)이 워크로드 증가를 추가 구성요소 없이 즉시 수용할 수 있는 가장 단순한 방법
- 예약 인스턴스(RI)를 구매하면 동일 성능을 가장 낮은 비용으로 운영할 수 있어 가장 비용 효율적

오답 이유

- **B. Multi-AZ**: 고가용성/내구성은 향상하지만 **성능이나 처리량이 증가하지 않습니다**(스탠바이는 읽기 처리에 참여하지 않음). 비용만 증가.
    
- **C. 다른 RDS 인스턴스 추가**: **인프라 추가**에 해당하며, 표준 RDS 단일 프라이머리 구조에서 쓰기 스케일 아웃은 불가. 읽기 부하 분산을 원하면 **리드 리플리카**를 명시해야 하나, 보기에는 그 언급이 없음.
    
- **D. 온디맨드로 전환**: 온디맨드는 **요금제 선택**일 뿐 성능 개선과 무관. 오히려 RI 대비 **비용이 증가**할 수 있음.


## #437
한 회사는 Amazon EC2 인스턴스 뒤에 Application Load Balancer(ALB)와 Auto Scaling 그룹을 사용해 전자상거래 웹사이트를 운영하고 있습니다. 사이트는 변경되는 IP 주소를 가진 불법 외부 시스템의 높은 요청률로 인해 성능 문제가 발생하고 있습니다. 보안 팀은 웹사이트에 대한 잠재적인 DDoS 공격을 우려하고 있습니다. 회사는 정상 사용자에게 미치는 영향을 최소화하는 방식으로 불법적인 수신 요청을 차단해야 합니다.

솔루션스 아키텍트는 무엇을 권장해야 합니까?

A. Amazon Inspector를 배포하고 ALB에 연결합니다.
B. AWS WAF를 배포하고 ALB에 연결한 후 속도 제한(rate-limiting) 규칙을 구성합니다.
C. ALB와 연결된 네트워크 ACL에 규칙을 배포하여 수신 트래픽을 차단합니다.
D. Amazon GuardDuty를 배포하고 구성 시 속도 제한 보호를 활성화합니다.

```
A company operates an ecommerce website on Amazon EC2 instances behind an Application Load Balancer (ALB) in an Auto Scaling group. The site is experiencing performance issues related to a high request rate from illegitimate external systems with changing IP addresses. The security team is worried about potential DDoS attacks against the website. The company must block the illegitimate incoming requests in a way that has a minimal impact on legitimate users.  
  
What should a solutions architect recommend?

- A. Deploy Amazon Inspector and associate it with the ALB.
- B. Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.
- C. Deploy rules to the network ACLs associated with the ALB to block the incomingtraffic.
- D. Deploy Amazon GuardDuty and enable rate-limiting protection when configuring GuardDuty.
```

정답 : `B`

- 변동 IP를 사용하는 과도한 요청 트래픽은 L7(HTTP/S) 수준에서 식별･차단이 효과적
- AWS WAF를 ALB에 연동하고 rate-based rule(예: IP당 초/분당 허용 요청 수 상한)을 적용하면 정상 사용자는 유지하면서 과도한 요청만 자동 억제 가능

오답 이유

- **A. Amazon Inspector**: 취약점/구성 진단 서비스로 **실시간 트래픽 차단 기능 없음**. DDoS/봇 트래픽 억제와 무관.
    
- **C. 네트워크 ACL**: **L3/L4 정적 필터**이며 **stateless**. 빈번히 바뀌는 IP를 지속 갱신하기 어렵고, L7 기준(URI/헤더/요청율) **레이트 제한 불가**. 정상 트래픽에도 영향이 커질 수 있음.
    
- **D. Amazon GuardDuty**: 위협 탐지/알림 서비스로 **자동 차단·레이트 제한 기능 없음**. 차단은 WAF/Security Group 등 별도 조치 필요.


## #438
한 회사가 외부 감사인과 회계 데이터를 공유하려 합니다. 데이터는 프라이빗 서브넷에 있는 Amazon RDS DB 인스턴스에 저장되어 있습니다. 감사인은 자체 AWS 계정을 가지고 있으며, 데이터베이스의 자체 사본이 필요합니다.

가장 안전하게 데이터베이스를 감사인과 공유하는 방법은 무엇입니까?

A. 데이터베이스의 읽기 전용 복제본을 생성합니다. 감사인에게 액세스를 부여하기 위해 IAM 표준 데이터베이스 인증을 구성합니다.
B. 데이터베이스 내용을 텍스트 파일로 내보냅니다. 파일을 Amazon S3 버킷에 저장합니다. 감사인을 위한 새로운 IAM 사용자를 생성합니다. 해당 사용자에게 S3 버킷에 대한 액세스를 부여합니다.
C. 데이터베이스 스냅샷을 Amazon S3 버킷으로 복사합니다. IAM 사용자를 생성합니다. S3 버킷의 객체에 대한 액세스를 부여하기 위해 감사인과 사용자의 키를 공유합니다.
D. 암호화된 스냅샷을 생성합니다. 스냅샷을 감사인과 공유합니다. AWS Key Management Service (AWS KMS) 암호화 키에 대한 액세스를 허용합니다.

```
A company wants to share accounting data with an external auditor. The data is stored in an Amazon RDS DB instance that resides in a private subnet. The auditor has its own AWS account and requires its own copy of the database.  
  
What is the MOST secure way for the company to share the database with the auditor?

- A. Create a read replica of the database. Configure IAM standard database authentication to grant the auditor access.
- B. Export the database contents to text files. Store the files in an Amazon S3 bucket. Create a new IAM user for the auditor. Grant the user access to the S3 bucket.
- C. Copy a snapshot of the database to an Amazon S3 bucket. Create an IAM user. Share the user's keys with the auditor to grant access to the object in the S3 bucket.
- D. Create an encrypted snapshot of the database. Share the snapshot with the auditor. Allow access to the AWS Key Management Service (AWS KMS) encryption key.
```

정답 : `D`

- 감사인이 자신의 AWS 계정에서 독립적인 DB 사본을 가져야 하므로, 원본 계정에서 RDS 스냅샷을 생성하고 이를 대상 계정과 공유
- 감사인이 자신의 계정으로 스냅샷을 복사해 복원하는 방식이 가장 안전하고 표준적인 방법
- 스냅샷이 암호화되어 있다면 스냅샷 공유와 함께 해당 KMS CMK에 대한 교차 계정 권한을 부여해야 함
	- 데이터 기밀성을 유지한 채 최소 권한으로 안전하게 공유

오답 이유

- **A. 읽기 전용 복제본 + IAM DB 인증**: 읽기 복제본은 **동일 계정 내 리소스**로 감사인의 **자체 계정 사본 요구**를 충족하지 못합니다. 또한 프라이빗 서브넷 접근, 네트워크/보안 구성 이슈가 남습니다.
    
- **B. S3로 텍스트 파일 내보내기 + 우리 계정의 IAM 사용자 제공**: 감사인이 **자체 계정**으로 소유/관리하는 **완전한 DB 사본**을 즉시 얻을 수 없습니다. 우리 계정에 사용자 생성 및 **자격증명 공유**는 보안 모범사례가 아닙니다.
    
- **C. 스냅샷을 S3로 복사 + 키 공유**: RDS 스냅샷을 **직접 S3 객체로 복사**하는 방식은 일반적인 스냅샷 공유 플로우가 아닙니다(스냅샷 내보내기 기능은 데이터 레이크용 형식 변환이지 RDS로의 간편 복원이 아닙니다). 또한 **액세스 키 공유**는 보안에 취약합니다.


## #439
솔루션스 아키텍트가 IP 주소 범위가 작은 VPC를 구성했습니다. VPC의 Amazon EC2 인스턴스 수가 증가하고 있으며, 향후 워크로드에 사용할 IP 주소 수가 부족합니다.

다음 중 운영 오버헤드를 가장 적게 들이고 이 문제를 해결하는 솔루션은 무엇입니까?

A. 추가 IPv4 CIDR 블록을 추가하여 IP 주소 수를 늘리고 VPC에 추가 서브넷을 생성합니다. 새로운 CIDR을 사용하여 새 리소스를 새 서브넷에 생성합니다.
B. 추가 서브넷이 있는 두 번째 VPC를 생성합니다. 두 번째 VPC를 첫 번째 VPC와 연결하기 위해 피어링 연결을 사용합니다. 라우트를 업데이트하고 두 번째 VPC의 서브넷에 새 리소스를 생성합니다.
C. AWS Transit Gateway를 사용하여 트랜짓 게이트웨이를 추가하고 두 번째 VPC를 첫 번째 VPC와 연결합니다. 트랜짓 게이트웨이와 VPC의 라우트를 업데이트합니다. 두 번째 VPC의 서브넷에 새 리소스를 생성합니다.
D. 두 번째 VPC를 생성합니다. Amazon EC2에서 호스팅되는 VPN 솔루션과 가상 프라이빗 게이트웨이를 사용하여 첫 번째 VPC와 두 번째 VPC 간에 Site-to-Site VPN 연결을 생성합니다. VPC 간 트래픽이 VPN을 통해 전달되도록 라우트를 업데이트합니다. 두 번째 VPC의 서브넷에 새 리소스를 생성합니다.

```
A solutions architect configured a VPC that has a small range of IP addresses. The number of Amazon EC2 instances that are in the VPC is increasing, and there is an insufficient number of IP addresses for future workloads.  
  
Which solution resolves this issue with the LEAST operational overhead?

- A. Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC. Create new resources in the new subnets by using the new CIDR.
- B. Create a second VPC with additional subnets. Use a peering connection to connect the second VPC with the first VPC Update the routes and create new resources in the subnets of the second VPC.
- C. Use AWS Transit Gateway to add a transit gateway and connect a second VPC with the first VPUpdate the routes of the transit gateway and VPCs. Create new resources in the subnets of the second VPC.
- D. Create a second VPC. Create a Site-to-Site VPN connection between the first VPC and the second VPC by using a VPN-hosted solution on Amazon EC2 and a virtual private gateway. Update the route between VPCs to the traffic through the VPN. Create new resources in the subnets of the second VPC.
```

정답 : `A`

- VPC에는 보조 IPv4 CIDR 블록을 추가해 주소 공간을 확장 가능
- 단일 VPC 내에서 새 CIDR로 새 서브넷만 추가하면 되므로, 라우팅/보안그룹/게이트웨이 등 기존 아키텍처 변경을 최소화

오답 이유

- **B (VPC 피어링 추가)**: 두 번째 VPC 생성, 피어링 설정, **각 VPC의 라우트 테이블 업데이트** 등 작업이 필요하고 **보안/운영 복잡도 증가**.
    
- **C (Transit Gateway)**: 멀티-VPC/멀티리전 허브형 네트워킹에 유용하지만, **이용 비용**과 **구성 복잡도**가 커짐. 단순한 IP 부족 문제 해결엔 과도.
    
- **D (Site-to-Site VPN)**: EC2 기반 VPN+VGW 구성은 **가용성/운영 부담/비용**이 높고, 단일 클라우드 내 VPC 간 연결로는 **비효율적**.


## #440
한 회사가 애플리케이션 테스트 중 Amazon RDS for MySQL DB 인스턴스를 사용했습니다.  
테스트 주기 종료 시 DB 인스턴스를 종료하기 전에 솔루션스 아키텍트는 두 가지 백업을 생성했습니다.  
첫 번째 백업은 **mysqldump 유틸리티**를 사용하여 데이터베이스 덤프를 생성했습니다.  
두 번째 백업은 **RDS 종료 시 최종 DB 스냅샷(final DB snapshot)** 옵션을 활성화하여 생성했습니다.

회사는 이제 새로운 테스트 주기를 계획 중이며, **가장 최근 백업**으로부터 새로운 DB 인스턴스를 생성하려고 합니다.  
또한 **MySQL 호환 Amazon Aurora 에디션**을 사용하여 DB 인스턴스를 호스팅하기로 결정했습니다.

다음 중 어떤 솔루션이 새로운 DB 인스턴스를 생성할 수 있습니까? (두 개를 선택하십시오.)

A. RDS 스냅샷을 직접 Aurora로 가져옵니다.  
B. RDS 스냅샷을 Amazon S3로 업로드한 다음 Aurora로 가져옵니다.  
C. 데이터베이스 덤프를 Amazon S3로 업로드한 다음 Aurora로 가져옵니다.  
D. AWS Database Migration Service (AWS DMS)를 사용하여 RDS 스냅샷을 Aurora로 가져옵니다.  
E. 데이터베이스 덤프를 Amazon S3로 업로드한 다음 AWS Database Migration Service (AWS DMS)를 사용하여 Aurora로 가져옵니다.

```
A company used an Amazon RDS for MySQL DB instance during application testing. Before terminating the DB instance at the end of the test cycle, a solutions architect created two backups. The solutions architect created the first backup by using the mysqldump utility to create a database dump. The solutions architect created the second backup by enabling the final DB snapshot option on RDS termination.  
  
The company is now planning for a new test cycle and wants to create a new DB instance from the most recent backup. The company has chosen a MySQL-compatible edition ofAmazon Aurora to host the DB instance.  
  
Which solutions will create the new DB instance? (Choose two.)

- A. Import the RDS snapshot directly into Aurora.
- B. Upload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora.
- C. Upload the database dump to Amazon S3. Then import the database dump into Aurora.
- D. Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora.
- E. Upload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS DMS) to import the database dump into Aurora.
```

정답 : `A, C`

- Aurora MySQL은 RDS MySQL 스냅샷으로부터 직접 복원(import) 지원
	- RDS 스냅샷을 선택하고 Migrate Snapshot to Aurora 기능을 사용해 Aurora 클러스터 생성 가능
	- 즉, RDS for MySQL → Aurora MySQL 간 스냅샷 기반 직접 마이그레이션 가능
- mysqldump는 논리적 백업 파일로, S3에 업로드 후 Aurora MySQL에서 LOAD DATA FROM S3 또는 MySQL 클라이언트를 통해 데이터를 직접 복원(import) 가능

오답 이유

- **B. RDS 스냅샷을 S3로 업로드 후 Aurora로 가져오기**
    - RDS 스냅샷은 **Aurora에서 직접 가져오도록 설계**되어 있으며, **S3로 내보내는 방식은 CSV/Parquet 등 데이터 레이크용**입니다.
    - Aurora로의 직접 복원은 **S3 업로드 과정 불필요**.
    
- **D. DMS를 사용해 RDS 스냅샷을 Aurora로 가져오기**
    - DMS는 **실행 중인 소스 DB → 대상 DB 실시간/증분 복제**용입니다.
    - 스냅샷(정적 백업 파일)을 DMS로 직접 마이그레이션할 수 없습니다.
    
- **E. DMS + mysqldump**
    - mysqldump는 **정적 파일**이며, DMS는 **소스 DB 엔드포인트**가 필요합니다.
    - 따라서 mysqldump 파일을 직접 DMS로 복제할 수 없습니다.