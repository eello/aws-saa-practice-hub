---
created: 2025-10-12 10:09:48
last_modified: 2025-10-12 15:17:24
---
## #401
한 회사가 기존 애플리케이션을 고가용성과 복원력이 있도록 AWS 클라우드에서 운영하고자 합니다. 현재 버전의 애플리케이션은 회사의 데이터 센터에 있습니다. 애플리케이션은 최근 예기치 않은 정전으로 데이터베이스 서버가 장애를 일으킨 후 데이터 손실을 겪었습니다.

회사는 단일 장애 지점을 피하는 솔루션이 필요합니다. 또한 솔루션은 사용자 수요에 맞게 확장할 수 있어야 합니다.

어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. 여러 가용 영역에 걸쳐 Auto Scaling 그룹의 Amazon EC2 인스턴스로 애플리케이션 서버를 배포합니다. Amazon RDS DB 인스턴스를 Multi-AZ 구성으로 사용합니다.
B. 단일 가용 영역의 Auto Scaling 그룹에서 Amazon EC2 인스턴스로 애플리케이션 서버를 배포합니다. 데이터베이스는 EC2 인스턴스에 배포합니다. EC2 Auto Recovery를 활성화합니다.
C. 여러 가용 영역에 걸쳐 Auto Scaling 그룹의 Amazon EC2 인스턴스로 애플리케이션 서버를 배포합니다. 단일 가용 영역의 읽기 복제본이 있는 Amazon RDS DB 인스턴스를 사용합니다. 기본 DB 인스턴스가 장애가 나면 읽기 복제본을 승격하여 대체합니다.
D. 여러 가용 영역에 걸쳐 Auto Scaling 그룹의 Amazon EC2 인스턴스로 애플리케이션 서버를 배포합니다. 기본 및 보조 데이터베이스 서버를 여러 가용 영역의 EC2 인스턴스에 배포합니다. Amazon EBS Multi-Attach을 사용하여 인스턴스 간에 공유 스토리지를 생성합니다.

```
A company wants to use the AWS Cloud to make an existing application highly available and resilient. The current version of the application resides in the company's data center. The application recently experienced data loss after a database server crashed because of an unexpected power outage.  
  
The company needs a solution that avoids any single points of failure. The solution must give the application the ability to scale to meet user demand.  
  
Which solution will meet these requirements?

- A. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.
- B. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a single Availability Zone. Deploy the database on an EC2 instance. Enable EC2 Auto Recovery.
- C. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance with a read replica in a single Availability Zone. Promote the read replica to replace the primary DB instance if the primary DB instance fails.
- D. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Deploy the primary and secondary database servers on EC2 instances across multiple Availability Zones. Use Amazon Elastic Block Store (Amazon EBS) Multi-Attach to create shared storage between the instances.
```

정답 : `A`

- 여러 AZ에 걸친 EC2 Auto Scaling은 단일 AZ 장애를 회피하고 수요에 따라 자동으로 수평 확장
- RDS Multi-AZ는 동기식 복제로 스탠바이를 유지하여 데이터 손실을 최소화
	- 장애 시 자동 페일오버로 다운타임을 단축
	- 관리형으로 운영 오버헤드가 낮음

오답 이유

- **B**: 앱/DB가 **단일 AZ**에 있어 AZ 장애에 취약. EC2 Auto Recovery는 **인스턴스 수준** 복구일 뿐 **AZ 단절**이나 **데이터 손실 방지**를 보장하지 못합니다.
    
- **C**: RDS **읽기 복제본은 비동기식**이므로 승격 시 **데이터 손실(RPO>0)** 가능. 또한 “단일 AZ의 복제본”이라 AZ 장애 대비도 미흡. Multi-AZ가 정석.
    
- **D**: RDS가 아닌 **자체 운영 DB on EC2**는 고가용성/페일오버를 직접 구현해야 하므로 복잡. **EBS Multi-Attach는 동일 AZ 내 특정 Nitro 인스턴스 간 공유만** 지원하며 **AZ 간 공유 불가**이므로 설계 요건에 부적합하고 SPOF/일관성 문제 소지.


## #402
한 회사는 애플리케이션이 생성하는 대량의 스트리밍 데이터를 수집하고 처리해야 합니다.  
이 애플리케이션은 Amazon EC2 인스턴스에서 실행되며 Amazon Kinesis Data Streams로 데이터를 전송합니다.  
Kinesis Data Streams는 기본 설정으로 구성되어 있습니다.  
격일(이틀마다)로 애플리케이션이 데이터를 소비하여 Amazon S3 버킷에 데이터를 기록하고,  
이 데이터는 비즈니스 인텔리전스(BI) 처리를 위해 사용됩니다.  

회사는 애플리케이션이 Kinesis Data Streams로 전송한 모든 데이터가 Amazon S3에 도달하지 않는다는 것을 관찰했습니다.

이 문제를 해결하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?

A. Kinesis Data Streams의 기본 설정을 수정하여 데이터 보존 기간(data retention period)을 변경한다.  
B. 애플리케이션을 업데이트하여 Kinesis Producer Library(KPL)를 사용해 데이터를 Kinesis Data Streams로 전송한다.  
C. Kinesis 데이터 스트림의 샤드 수를 늘려 Kinesis Data Streams로 전송되는 데이터의 처리량을 처리할 수 있도록 한다.  
D. S3 버킷 내에서 S3 버전 관리를 활성화하여 버킷에 수집되는 각 객체의 모든 버전을 보존한다.

```
A company needs to ingest and handle large amounts of streaming data that its application generates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams, which is configured with default settings. Every other day, the application consumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) processing. The company observes that Amazon S3 is not receiving all the data that the application sends to Kinesis Data Streams.  
  
What should a solutions architect do to resolve this issue?

- A. Update the Kinesis Data Streams default settings by modifying the data retention period.
- B. Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.
- C. Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.
- D. Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the S3 bucket.
```

정답 : `A`

- 기본적으로 Amazon Kinesis Data Streams의 데이터 보존 기간은 24시간(기본값)
- 문제에서 "격일(이틀마다)"는 48시간 간격으로 데이터를 소비하므로 애플리케이션에서 데이터를 읽기전에 만료되어 삭제
- 따라서 보존 기간을 기본 24시간 → 최대 7일(또는 확장 설정 시 365일)로 늘려 데이터 손실을 방지

오답 이유

- **B. Kinesis Producer Library (KPL)**
    - KPL은 전송 효율(배치/압축/재시도)을 높이지만, **이미 전송된 데이터가 24시간 후 만료되는 문제**를 해결하지는 못합니다.
    - 현재 문제의 원인은 “데이터 유실”이므로, **보존 시간 부족**이 핵심입니다.
    
- **C. 샤드 수 확장**
    - 샤드는 **처리량(쓰기/읽기 TPS, MB/s)** 관련입니다.
    - 데이터 누락이 “처리량 초과로 인한 Write Throttling”이라면 가능성이 있지만, 문제는 “격일 소비 시점에 데이터가 일부 사라진다”이므로 **보존 시간 만료**가 근본 원인입니다.
    
- **D. S3 버전 관리 활성화**
    - S3에 기록된 객체 버전을 유지하는 기능으로, Kinesis → S3 전송 과정의 **데이터 누락** 문제와는 관련이 없습니다.
    - 누락은 **S3에 도달하기 전 단계(Kinesis Stream)** 에서 발생합니다.


## #403
한 개발자가 AWS Lambda 함수를 사용하여 파일을 Amazon S3에 업로드하는 애플리케이션을 가지고 있습니다.  
이 작업을 수행하기 위해 필요한 권한이 있어야 합니다.  
개발자는 이미 Amazon S3에 접근하기 위한 유효한 IAM 자격 증명이 포함된 IAM 사용자를 가지고 있습니다.

권한을 부여하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?

A. 필요한 IAM 권한을 Lambda 함수의 리소스 정책(resource policy)에 추가한다.  
B. Lambda 함수 내에서 기존 IAM 자격 증명을 사용하여 서명된 요청(signed request)을 생성한다.  
C. 새로운 IAM 사용자를 생성하고 Lambda 함수에서 기존 IAM 자격 증명을 사용한다.  
D. 필요한 권한을 가진 IAM 실행 역할(execution role)을 생성하고, 해당 IAM 역할을 Lambda 함수에 연결한다.

```
A developer has an application that uses an AWS Lambda function to upload files to Amazon S3 and needs the required permissions to perform the task. The developer already has an IAM user with valid IAM credentials required for Amazon S3.  
  
What should a solutions architect do to grant the permissions?

- A. Add required IAM permissions in the resource policy of the Lambda function.
- B. Create a signed request using the existing IAM credentials in the Lambda function.
- C. Create a new IAM user and use the existing IAM credentials in the Lambda function.
- D. Create an IAM execution role with the required permissions and attach the IAM role to the Lambda function.
```

정답 : `D`

- 람다 함수는 IAM 사용자 자격 증명을 코드 내에서 직접 사용하는 것이 보안상 금지된 방식
- 람다는 실행 시 자동으로 IAM Role(Execution Role)을 가정(Assume)하여 AWS 리소스에 접근
- 따라서 S3에 파일을 업로드하려면 람다 실행 역할에 S3 PutObject, GetObject 등의 권한을 부여해야 함

오답 이유

- **A. Lambda 리소스 정책에 권한 추가**
    - Lambda 함수의 리소스 정책은 **Lambda에 접근하는 외부 주체(예: API Gateway, 다른 계정)** 에 대한 허용 정책입니다.
    - Lambda가 S3를 호출하는 권한을 제어하지 않습니다. (**Outbound 권한 아님**)
    
- **B. 기존 IAM 자격 증명을 이용해 서명된 요청 생성**
    - Lambda 함수 내에서 **IAM 사용자의 Access Key/Secret Key를 코드에 넣는 것은 매우 위험한 보안 위반**입니다.
    - 또한 Lambda가 실행될 때마다 해당 자격 증명을 하드코딩해야 하므로 **운영 오버헤드 및 보안 리스크**가 큽니다.
    
- **C. 새로운 IAM 사용자 생성**
    - IAM 사용자는 사람(User) 또는 애플리케이션 외부 클라이언트에 적합하며, Lambda처럼 AWS 내부에서 실행되는 리소스에는 **IAM Role을 사용해야 함**.
    - 새로운 IAM 사용자 생성은 불필요하고 비보안적입니다.


## #404
한 회사가 서버리스 애플리케이션을 배포했습니다.  
이 애플리케이션은 Amazon S3 버킷에 새 문서가 업로드될 때마다 AWS Lambda 함수를 호출합니다.  
Lambda 함수는 해당 문서를 처리하는 역할을 합니다.  

최근 마케팅 캠페인 이후, 회사는 애플리케이션이 많은 문서를 처리하지 못했다는 사실을 발견했습니다.  

이 애플리케이션의 아키텍처를 개선하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?

A. Lambda 함수의 실행 제한 시간(runtime timeout)을 15분으로 설정한다.  
B. S3 버킷 복제 정책을 구성하여 문서를 S3 버킷에 임시로 저장하고 나중에 처리하도록 한다.  
C. 추가 Lambda 함수를 배포하고, 두 Lambda 함수 간에 문서 처리를 로드 밸런싱한다.  
D. Amazon Simple Queue Service (Amazon SQS) 큐를 생성하고, 요청을 큐로 전송하도록 한다.  
   그리고 해당 큐를 Lambda의 이벤트 소스로 구성한다.

```
A company has deployed a serverless application that invokes an AWS Lambda function when new documents are uploaded to an Amazon S3 bucket. The application uses the Lambda function to process the documents. After a recent marketing campaign, the company noticed that the application did not process many of the documents.  
  
What should a solutions architect do to improve the architecture of this application?

- A. Set the Lambda function's runtime timeout value to 15 minutes.
- B. Configure an S3 bucket replication policy. Stage the documents in the S3 bucket for later processing.
- C. Deploy an additional Lambda function. Load balance the processing of the documents across the two Lambda functions.
- D. Create an Amazon Simple Queue Service (Amazon SQS) queue. Send the requests to the queue. Configure the queue as an event source for Lambda.
```

정답 : `D`

- 문제의 핵심은 마케팅 캠페인으로 파일 업로드 트래픽이 급증했을 때 S3가 람다 호출을 보장하지 못하는 것
- S3 이벤트 알림은 "적어도 한 번(at least once)" 전달되지만, 대량의 업로드가 동시에 발생하면 일부 알림이 누락되거나 람다의 동시 실행 제한에 걸릴 수 있음
- 따라서 이벤트를 직접 람다에 연결하는 대신 SQS를 중간에 두어 비동기 버퍼링
	- S3 → SQS → Lambda 구성은 업로드 요청이 많아도 모든 이벤트가 큐에 안전하게 저장

오답 이유

- **A. Lambda timeout을 15분으로 설정**
    - Lambda 최대 실행 시간은 15분이 맞지만, 문제는 **처리되지 않은 이벤트**입니다.
        즉, **Lambda가 호출되지 않은 것**이지 **실행 중 중단된 것**이 아닙니다.
        Timeout을 늘려도 이벤트 유실 문제는 해결되지 않습니다.
    
- **B. S3 복제 정책 구성**
    - S3 복제(Replication)는 **데이터 복제용**이지 이벤트 전달이나 트래픽 버퍼링을 위한 기능이 아닙니다.
        문서를 “나중에 처리”할 수 있게 해주지 않으며, **Lambda 트리거 안정성 향상**과는 관련이 없습니다.
    
- **C. Lambda를 두 개로 늘려 로드 밸런싱**
    - Lambda는 **자동으로 수평 확장**됩니다. 직접 로드 밸런싱할 필요가 없습니다.
        또한 S3는 Lambda 함수 여러 개를 병렬로 호출하도록 로드밸런싱하지 않습니다.



## #405
한 솔루션스 아키텍트가 소프트웨어 데모 환경의 아키텍처를 설계하고 있습니다. 이 환경은 Application Load Balancer(ALB) 뒤의 Auto Scaling 그룹에 있는 Amazon EC2 인스턴스에서 실행됩니다. 시스템은 근무 시간 동안에는 트래픽이 크게 증가하지만, 주말에는 운영할 필요가 없습니다.

수요를 충족하도록 시스템이 확장될 수 있게 하기 위해 솔루션스 아키텍트는 어떤 작업 조합을 수행해야 합니까? (두 가지 선택)

A. 요청률에 따라 ALB 용량을 조정하도록 AWS Auto Scaling을 사용합니다.
B. VPC 인터넷 게이트웨이의 용량을 확장하도록 AWS Auto Scaling을 사용합니다.
C. 부하를 리전 간에 분산하기 위해 여러 AWS 리전에 EC2 인스턴스를 시작합니다.
D. 인스턴스 CPU 사용률을 기준으로 Auto Scaling 그룹을 확장하도록 대상 추적 스케일링 정책(target tracking scaling policy)을 사용합니다.
E. 주말에는 Auto Scaling 그룹의 최소, 최대 및 원하는 용량을 0으로 변경하는 예약 스케일링을 사용합니다. 주 초에 기본 값으로 되돌립니다.

```
A solutions architect is designing the architecture for a software demonstration environment. The environment will run on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The system will experience significant increases in traffic during working hours but is not required to operate on weekends.  
  
Which combination of actions should the solutions architect take to ensure that the system can scale to meet demand? (Choose two.)

- A. Use AWS Auto Scaling to adjust the ALB capacity based on request rate.
- B. Use AWS Auto Scaling to scale the capacity of the VPC internet gateway.
- C. Launch the EC2 instances in multiple AWS Regions to distribute the load across Regions.
- D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization.
- E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week.
```

정답 : `D, E`

- D: 근무 시간 동안의 변동 트래픽에 맞춰 CPU 사용률을 목표값으로 유지하도록 대상 추적 스케일링을 사용하면 자동으로 축/확장
- E: 주말에는 운영할 필요가 없으므로 예약 스케일링으로 ASG(Auto Scaling Group)의 용량을 0으로 내려 비용을 절감하고 주 초에 다시 원복

오답 이유

- **A**: ALB는 **완전관리형으로 자동 확장**되며, 사용자가 “ALB 용량”을 Auto Scaling으로 직접 조정하지 않습니다.
    
- **B**: **인터넷 게이트웨이(IGW)** 는 관리형으로 **용량 스케일 조정 개념이 없음**; Auto Scaling 대상이 아닙니다.
    
- **C**: 멀티 리전 배포는 요구되지 않았고 복잡성을 증가시킵니다. 단일 리전에서 ASG + ALB로 충분히 확장 가능.


## #406
솔루션스 아키텍트가 퍼블릭 서브넷과 데이터베이스 서브넷으로 구성된 2계층 아키텍처를 설계하고 있습니다. 퍼블릭 서브넷의 웹 서버는 포트 443으로 인터넷에 열려 있어야 합니다. 데이터베이스 서브넷의 Amazon RDS for MySQL DB 인스턴스는 포트 3306에서 오직 웹 서버에게만 액세스될 수 있어야 합니다.

이 요구 사항을 충족하기 위해 솔루션스 아키텍트는 어떤 단계 조합을 수행해야 합니까? (두 가지 선택)

A. 퍼블릭 서브넷에 대한 네트워크 ACL을 생성합니다. 포트 3306에서 0.0.0.0/0으로의 아웃바운드 트래픽을 거부하는 규칙을 추가합니다.
B. DB 인스턴스에 대한 보안 그룹을 생성합니다. 포트 3306에서 퍼블릭 서브넷 CIDR 블록으로부터의 트래픽을 허용하는 규칙을 추가합니다.
C. 퍼블릭 서브넷의 웹 서버에 대한 보안 그룹을 생성합니다. 포트 443에서 0.0.0.0/0으로부터의 트래픽을 허용하는 규칙을 추가합니다.
D. DB 인스턴스에 대한 보안 그룹을 생성합니다. 포트 3306에서 웹 서버의 보안 그룹으로부터의 트래픽을 허용하는 규칙을 추가합니다.
E. DB 인스턴스에 대한 보안 그룹을 생성합니다. 포트 3306에서 웹 서버의 보안 그룹으로부터의 트래픽을 제외한 모든 트래픽을 거부하는 규칙을 추가합니다.

```
A solutions architect is designing a two-tiered architecture that includes a public subnet and a database subnet. The web servers in the public subnet must be open to the internet on port 443. The Amazon RDS for MySQL DB instance in the database subnet must be accessible only to the web servers on port 3306.  
  
Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)

- A. Create a network ACL for the public subnet. Add a rule to deny outbound traffic to 0.0.0.0/0 on port 3306.
- B. Create a security group for the DB instance. Add a rule to allow traffic from the public subnet CIDR block on port 3306.
- C. Create a security group for the web servers in the public subnet. Add a rule to allow traffic from 0.0.0.0/0 on port 443.
- D. Create a security group for the DB instance. Add a rule to allow traffic from the web servers’ security group on port 3306.
- E. Create a security group for the DB instance. Add a rule to deny all traffic except traffic from the web servers’ security group on port 3306.
```

정답 : `C, D`

- C: 웹 서버가 인터넷에서 HTTPS(443)로 접근 가능해야 하므로, 웹 서버 SG 인바운드에 0.0.0.0/0:443 허용
- D: DB는 오직 웹 서버만 접근해야 하므로, DB SG 인바운드에 소스로 웹 서버 SG를 지정하여 3306 허용(최소 권한)

오답 이유

- **A**: NACL로 퍼블릭 서브넷의 아웃바운드 3306을 거부해도 DB 접근 제어를 정확히 보장하지 못하며, NACL은 본 요구에 불필요한 복잡성만 추가.
    
- **B**: 서브넷 CIDR을 허용하면 서브넷 내 **모든 리소스**가 DB에 접근 가능해져 최소 권한 원칙에 어긋남. SG 간 참조가 모범 사례.
    
- **E**: 보안 그룹은 **명시적 허용만** 지원하고 **명시적 거부(deny)** 규칙을 지원하지 않음. 따라서 “거부 규칙”을 추가할 수 없음.



## #407
한 회사가 AWS 클라우드에서 호스팅되는 게임 애플리케이션을 위해 공유 스토리지 솔루션을 구현하고 있습니다.  
회사는 Lustre 클라이언트를 사용하여 데이터에 액세스할 수 있는 기능이 필요합니다.  
해당 솔루션은 완전관리형(fully managed)이어야 합니다.

이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 데이터를 마운트 가능한 파일 시스템으로 공유하는 AWS DataSync 작업을 생성합니다.  
   이 파일 시스템을 애플리케이션 서버에 마운트합니다.  

B. AWS Storage Gateway 파일 게이트웨이를 생성합니다.  
   필요한 클라이언트 프로토콜을 사용하는 파일 공유를 생성하고,  
   애플리케이션 서버를 파일 공유에 연결합니다.  

C. Amazon Elastic File System (Amazon EFS) 파일 시스템을 생성하고 Lustre를 지원하도록 구성합니다.  
   파일 시스템을 원본 서버에 연결하고, 애플리케이션 서버를 파일 시스템에 연결합니다.  

D. Amazon FSx for Lustre 파일 시스템을 생성합니다.  
   파일 시스템을 원본 서버에 연결하고, 애플리케이션 서버를 파일 시스템에 연결합니다.

```
A company is implementing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use Lustre clients to access data. The solution must be fully managed.  
  
Which solution meets these requirements?

- A. Create an AWS DataSync task that shares the data as a mountable file system. Mount the file system to the application server.
- B. Create an AWS Storage Gateway file gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.
- C. Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system.
- D. Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.
```

정답 : `D`

- 요구사항: Lustre 클라이언트로 접근 가능, 완전관리형
- Amazon FSx for Lustre는 AWS에서 제공하는 완전관리형 병렬 파일 시스템 서비스
	- HPC(고성능 컴퓨팅), 머신러닝, 미디어 처리, 게임 등에서 대규모 데이터에 빠르게 접근해야 할 때 사용
- FSx for Lustre는 Lustre 클라이언트 프로토콜을 지원하므로, EC2 인스턴스가 Lustre 마운트를 통해 데이터를 읽고 쓸 수 있음

오답 이유

- **A. AWS DataSync**
    - DataSync는 **데이터 전송/복제용 서비스**이며, 파일 시스템을 “마운트” 가능한 형태로 제공하지 않습니다.
    - Lustre 클라이언트로 접근 불가능합니다.
    
- **B. AWS Storage Gateway (File Gateway)**
    - NFS/SMB 프로토콜을 통해 온프레미스 ↔ AWS 파일 접근을 지원하지만,
    - **Lustre 프로토콜은 지원하지 않음**. 또한 게임 워크로드용 병렬 고성능 파일 시스템에 적합하지 않음.
    
- **C. Amazon EFS**
    - EFS는 **NFS 기반**의 완전관리형 파일 시스템입니다.
    - Lustre 클라이언트를 사용할 수 없으며, 고성능 병렬 처리가 필요한 워크로드(HPC 등)에는 부적합.


## #408
한 회사가 지리적으로 분산된 수천 대의 원격 장치로부터 UDP를 사용해 데이터를 받는 애플리케이션을 운영합니다. 애플리케이션은 데이터를 즉시 처리하고 필요 시 장치로 메시지를 다시 보냅니다. 데이터는 저장하지 않습니다.

회사는 장치로부터의 데이터 전송에 대해 지연 시간을 최소화하는 솔루션이 필요합니다. 또한 다른 AWS 리전으로의 신속한 장애 조치(failover)가 가능해야 합니다.

다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. Amazon Route 53 장애 조치 라우팅 정책을 구성합니다. 두 리전에 각각 Network Load Balancer(NLB)를 생성합니다. NLB가 AWS Lambda 함수를 호출해 데이터를 처리하도록 구성합니다.
B. AWS Global Accelerator를 사용합니다. 두 리전에 각각 Network Load Balancer(NLB)를 엔드포인트로 생성합니다. AWS Fargate 런치 타입의 Amazon ECS 클러스터를 생성하고, 클러스터에 ECS 서비스를 만듭니다. ECS 서비스를 NLB의 대상(Target)으로 설정합니다. 데이터를 Amazon ECS에서 처리합니다.
C. AWS Global Accelerator를 사용합니다. 두 리전에 각각 Application Load Balancer(ALB)를 엔드포인트로 생성합니다. AWS Fargate 런치 타입의 Amazon ECS 클러스터를 생성하고, 클러스터에 ECS 서비스를 만듭니다. ECS 서비스를 ALB의 대상으로 설정합니다. 데이터를 Amazon ECS에서 처리합니다.
D. Amazon Route 53 장애 조치 라우팅 정책을 구성합니다. 두 리전에 각각 Application Load Balancer(ALB)를 생성합니다. AWS Fargate 런치 타입의 Amazon ECS 클러스터를 생성하고, 클러스터에 ECS 서비스를 만듭니다. ECS 서비스를 ALB의 대상으로 설정합니다. 데이터를 Amazon ECS에서 처리합니다.

```
A company runs an application that receives data from thousands of geographically dispersed remote devices that use UDP. The application processes the data immediately and sends a message back to the device if necessary. No data is stored.  
  
The company needs a solution that minimizes latency for the data transmission from the devices. The solution also must provide rapid failover to another AWS Region.  
  
Which solution will meet these requirements?

- A. Configure an Amazon Route 53 failover routing policy. Create a Network Load Balancer (NLB) in each of the two Regions. Configure the NLB to invoke an AWS Lambda function to process the data.
- B. Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the NLProcess the data in Amazon ECS.
- C. Use AWS Global Accelerator. Create an Application Load Balancer (ALB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.
- D. Configure an Amazon Route 53 failover routing policy. Create an Application Load Balancer (ALB) in each of the two Regions. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.
```

정답 : `B`

- 장치가 UDP를 사용하므로 UDP를 지원하는 L4 로드 밸런서(NLB)가 필요
- AWS Global Accelerator는 전 세계 Anycast 글로벌 엣지에 트래픽을 종단해 지연을 최소화
- NLB는 UDP를 지원하며, ECS(Fargate) 서비스를 대상으로 연결해 컨테이너에서 패킷을 즉시 처리하고 필요 시 응답 가능

오답 이유

- **A (Route 53 + NLB + Lambda)**: Route 53 장애 조치는 **DNS 기반**으로 캐시/TTL 영향으로 전환이 느릴 수 있어 **신속한 리전 페일오버**에 불리합니다. 또한 UDP 워크로드에 Lambda를 직접 매핑하는 것은 적합하지 않습니다(연결지향 아키텍처/응답 경로 제약).
    
- **C (Global Accelerator + ALB)**: **ALB는 UDP를 지원하지 않습니다**(L7 HTTP/HTTPS 전용).
    
- **D (Route 53 + ALB)**: ALB가 UDP 미지원이며, Route 53 기반 페일오버는 앞서 언급한 이유로 지연 최소화/빠른 장애 조치 요구에 불리합니다.


## #409
솔루션스 아키텍트는 Windows Internet Information Services(IIS) 웹 애플리케이션을 AWS로 마이그레이션해야 합니다. 현재 애플리케이션은 사용자의 온프레미스 NAS에 호스팅된 파일 공유에 의존합니다. 솔루션스 아키텍트는 IIS 웹 서버를 스토리지 솔루션에 연결된 다중 가용 영역의 Amazon EC2 인스턴스로 마이그레이션하고, 인스턴스에 연결된 Elastic Load Balancer를 구성할 것을 제안했습니다.

온프레미스 파일 공유를 대체하는 데 가장 탄력적(resilient)이고 내구성(durable)이 높은 옵션은 무엇입니까?

A. 파일 공유를 Amazon RDS로 마이그레이션한다.  
B. 파일 공유를 AWS Storage Gateway로 마이그레이션한다.  
C. 파일 공유를 Amazon FSx for Windows File Server로 마이그레이션한다.  
D. 파일 공유를 Amazon Elastic File System(Amazon EFS)으로 마이그레이션한다.

```
A solutions architect must migrate a Windows Internet Information Services (IIS) web application to AWS. The application currently relies on a file share hosted in the user's on-premises network-attached storage (NAS). The solutions architect has proposed migrating the IIS web servers to Amazon EC2 instances in multiple Availability Zones that are connected to the storage solution, and configuring an Elastic Load Balancer attached to the instances.  
  
Which replacement to the on-premises file share is MOST resilient and durable?

- A. Migrate the file share to Amazon RDS.
- B. Migrate the file share to AWS Storage Gateway.
- C. Migrate the file share to Amazon FSx for Windows File Server.
- D. Migrate the file share to Amazon Elastic File System (Amazon EFS).
```

정답 : `C`

- Amazon FSx for Windows File Server는 완전관리형 Windows 네이티브 파일 시스템으로 SMB 프로토콜을 지원하여 Windows 기반 애플리케이션에서 손쉽게 접근 가능
- Active Directory, NTFS 권한 관리, 멀티 AZ 배포, 자동 백업, 데이터 복제 기능을 제공해 가장 높은 수준의 내구성과 가용성 보장

오답 이유

**A. Amazon RDS**
- RDS는 관계형 데이터베이스 서비스입니다.
- 파일 공유를 저장하거나 SMB 프로토콜을 통한 파일 접근을 지원하지 않습니다.
- 파일 서버 대체 용도로 사용할 수 없습니다.

**B. AWS Storage Gateway**
- 온프레미스 애플리케이션이 AWS 스토리지(S3, FSx 등)에 접근할 수 있도록 하는 하이브리드 스토리지 서비스입니다.
- 온프레미스와 AWS 간 데이터 캐싱이나 마이그레이션에는 적합하지만, 완전한 AWS 네이티브 파일 서버로서의 내구성과 가용성을 제공하지 않습니다.
- 완전한 클라우드 기반으로 전환하는 이 시나리오에는 적합하지 않습니다.

**D. Amazon Elastic File System (EFS)**
- EFS는 NFS(Network File System) 프로토콜을 사용하는 리눅스 기반 파일 시스템입니다.
- Windows 기반 애플리케이션은 NFS를 직접 지원하지 않으며, SMB 통신이 필요합니다.
- 따라서 IIS(Windows Server) 환경에서는 호환되지 않아 사용할 수 없습니다.


## #410
한 회사가 Amazon EC2 인스턴스에 새 애플리케이션을 배포하고 있습니다.  
이 애플리케이션은 Amazon Elastic Block Store (Amazon EBS) 볼륨에 데이터를 기록합니다.  
회사는 EBS 볼륨에 기록되는 모든 데이터가 **저장 시 암호화(encrypted at rest)** 되도록 해야 합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. EBS 암호화를 지정하는 IAM 역할을 생성하고, 그 역할을 EC2 인스턴스에 연결합니다.  
B. EBS 볼륨을 암호화된 볼륨으로 생성하고, 해당 볼륨을 EC2 인스턴스에 연결합니다.  
C. EC2 인스턴스에 대해 키가 Encrypt이고 값이 True인 태그를 생성합니다. 암호화를 필요로 하는 모든 인스턴스에 이 태그를 추가합니다.  
D. AWS Key Management Service (AWS KMS) 키 정책을 생성하여 계정 내 EBS 암호화를 강제합니다. 해당 키 정책이 활성 상태인지 확인합니다.

```
A company is deploying a new application on Amazon EC2 instances. The application writes data to Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all data that is written to the EBS volumes is encrypted at rest.  
  
Which solution will meet this requirement?

- A. Create an IAM role that specifies EBS encryption. Attach the role to the EC2 instances.
- B. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.
- C. Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that require encryption at the EBS level.
- D. Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption in the account. Ensure that the key policy is active.
```

정답 : `B`

- Amazon EBS는 볼륨 단위로 암호화를 활성화할 수 있음
- 암호화된 EBS 볼륨을 생성하면, 해당 볼륨에 기록되거나 읽히는 모든 데이터가 자동으로 암호화/복호화
- EBS 암호화는 AWS KMS 키(기본적으로 aws/ebs 관리형 키)를 사용하며, 사용자는 별도의 코드 수정 없이 저장 시 데이터, 스냅샷, 백업 모두 암호화된 상태로 관리

오답 이유

**A. Create an IAM role that specifies EBS encryption.**
- IAM 역할에는 EBS 암호화 속성을 지정할 수 없습니다.
- IAM 역할은 권한 제어(예: ec2:CreateVolume, kms:Encrypt)를 담당하지만, 암호화를 “자동 적용”하는 기능은 없습니다.
- 암호화 여부는 볼륨 생성 시점에서 지정해야 합니다.

**C. Create an EC2 instance tag that has a key of Encrypt and a value of True.**
- 태그는 단순한 메타데이터로, EC2나 EBS 리소스의 **암호화 설정에 영향을 주지 않습니다.**
- 태그는 관리나 식별 용도로만 사용됩니다.

**D. Create an AWS KMS key policy that enforces EBS encryption.**
- KMS 키 정책은 **특정 키의 사용 권한**을 제어할 뿐, EBS 암호화를 강제할 수는 없습니다.
- EBS 암호화는 계정 수준 또는 볼륨 생성 시에만 설정할 수 있습니다.
- 만약 계정 전체에 암호화를 기본 적용하려면 “EBS Default Encryption” 기능을 활성화해야 합니다 (하지만 이는 키 정책이 아니라 EC2 설정).


## #411
한 회사는 사용 패턴이 불규칙한 웹 애플리케이션을 보유하고 있습니다. 매월 초에는 사용량이 많고, 매주 초에는 중간 수준의 사용량이 있으며, 주중에는 예측할 수 없는 사용량이 발생합니다.  
이 애플리케이션은 데이터 센터 내에서 실행 중인 웹 서버와 MySQL 데이터베이스 서버로 구성되어 있습니다.  
회사는 애플리케이션을 AWS 클라우드로 마이그레이션하려고 하며, 데이터베이스 수정을 필요로 하지 않으면서 비용 효율적인 데이터베이스 플랫폼을 선택해야 합니다.

어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. Amazon DynamoDB  
B. Amazon RDS for MySQL  
C. MySQL 호환 Amazon Aurora Serverless  
D. Auto Scaling 그룹 내에 배포된 Amazon EC2의 MySQL

```
A company has a web application with sporadic usage patterns. There is heavy usage at the beginning of each month, moderate usage at the start of each week, and unpredictable usage during the week. The application consists of a web server and a MySQL database server running inside the data center. The company would like to move the application to the AWS Cloud, and needs to select a cost-effective database platform that will not require database modifications.  
  
Which solution will meet these requirements?

- A. Amazon DynamoDB
- B. Amazon RDS for MySQL
- C. MySQL-compatible Amazon Aurora Serverless
- D. MySQL deployed on Amazon EC2 in an Auto Scaling group
```

정답 : `C`

- Aurora Serverless는 MySQL과 완전히 호환되며, 사용량 변화에 따라 자동으로 용량을 확장하거나 축소
- 또한, 실제 쿼리 요청이 있을 때만 데이터베이스 인스턴스를 프로비저닝하고, 사용량이 적을 때 자동으로 중지되어 비용을 절감

오답 이유

- **A. Amazon DynamoDB**    
    DynamoDB는 NoSQL 데이터베이스이므로 기존 MySQL용 SQL 기반 애플리케이션을 수정해야 합니다. 문제에서 “데이터베이스 수정이 필요 없어야 한다”고 명시되어 있어 부적합합니다.

- **B. Amazon RDS for MySQL**
    RDS for MySQL은 MySQL 호환성이 있지만, 인스턴스 크기를 수동으로 조정해야 하며, 트래픽이 불규칙할 때 비용 효율성이 떨어집니다. 자동 확장은 Aurora Serverless보다 제한적입니다.

- **D. MySQL on Amazon EC2 (Auto Scaling group)**
    EC2 인스턴스에 직접 MySQL을 설치하면 관리 부담이 큽니다. 또한 Auto Scaling 그룹은 스테이트리스 웹 서버에 적합하지만, 스테이트풀한 DB에는 적용이 어렵습니다.


## #412
한 이미지 호스팅 회사는 Amazon S3 버킷에 객체를 저장하고 있습니다.  
회사는 S3 버킷의 객체가 실수로 공개되는 것을 방지하고자 합니다.  
AWS 계정 전체의 모든 S3 객체가 비공개 상태로 유지되어야 합니다.  

어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. Amazon GuardDuty를 사용하여 S3 버킷 정책을 모니터링합니다. 객체를 공개 상태로 만드는 변경 사항을 수정하는 AWS Lambda 함수를 사용하는 자동 복구 작업 규칙을 생성합니다.  
B. AWS Trusted Advisor를 사용하여 공개적으로 액세스 가능한 S3 버킷을 찾습니다. 변경 사항이 감지되면 Trusted Advisor에서 이메일 알림을 구성합니다. S3 버킷 정책이 공개 액세스를 허용하는 경우 수동으로 수정합니다.  
C. AWS Resource Access Manager를 사용하여 공개적으로 액세스 가능한 S3 버킷을 찾습니다. 변경 사항이 감지되면 Amazon Simple Notification Service(Amazon SNS)를 사용하여 AWS Lambda 함수를 호출합니다. Lambda 함수를 배포하여 프로그래밍 방식으로 변경 사항을 복구합니다.  
D. 계정 수준에서 S3 Block Public Access 기능을 사용합니다. AWS Organizations를 사용하여 IAM 사용자가 해당 설정을 변경하지 못하도록 하는 서비스 제어 정책(SCP)을 생성합니다. SCP를 계정에 적용합니다.

```
An image-hosting company stores its objects in Amazon S3 buckets. The company wants to avoid accidental exposure of the objects in the S3 buckets to the public. All S3 objects in the entire AWS account need to remain private.  
  
Which solution will meet these requirements?

- A. Use Amazon GuardDuty to monitor S3 bucket policies. Create an automatic remediation action rule that uses an AWS Lambda function to remediate any change that makes the objects public.
- B. Use AWS Trusted Advisor to find publicly accessible S3 buckets. Configure email notifications in Trusted Advisor when a change is detected. Manually change the S3 bucket policy if it allows public access.
- C. Use AWS Resource Access Manager to find publicly accessible S3 buckets. Use Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function when a change is detected. Deploy a Lambda function that programmatically remediates the change.
- D. Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service control policy (SCP) that prevents IAM users from changing the setting. Apply the SCP to the account.
```

정답 : `D`

- S3의 Block Public Access(Account level) 기능은 AWS 계정 전체에서 모든 버킷과 객체에 대한 공개 액세스를 일괄 차단 가능
- 이를 통해 퍼블릭 ACL, 퍼블릭 정책 등으로 인해 의도치 않게 객체가 공개되는 상황을 근본적으로 차단
- SCP(Service Control Policy)를 사용하면 IAM 사용자나 역할이 이 설정을 변경하지 못하도록 강제 가능

오답 이유

- **A. Amazon GuardDuty**    
    GuardDuty는 위협 탐지 서비스로, S3 버킷 정책을 지속적으로 모니터링하거나 정책 변경을 자동으로 복구하는 용도가 아닙니다. 또한 Lambda를 통한 복구는 사후 대응 방식으로 근본적인 차단이 아닙니다.

- **B. AWS Trusted Advisor**    
    Trusted Advisor는 S3 퍼블릭 접근 경고를 제공할 수 있으나, 이는 단순히 **알림 및 수동 조치**만 가능하며, **자동 차단**이나 **계정 전체 적용**이 불가능합니다.

- **C. AWS Resource Access Manager (RAM)**
    RAM은 리소스 공유 서비스이며, 공개 접근 여부를 탐지하거나 Lambda로 복구하는 기능을 제공하지 않습니다. 선택지 내용 자체가 AWS 기능과 맞지 않습니다.


## #413
한 전자상거래 회사는 사용자 트래픽이 증가하고 있습니다. 이 회사의 스토어는 웹 계층과 별도의 데이터베이스 계층으로 구성된 2-계층 웹 애플리케이션으로 Amazon EC2 인스턴스에 배포되어 있습니다. 트래픽이 증가함에 따라, 회사는 이 아키텍처가 사용자에게 적시에 마케팅 및 주문 확인 이메일을 보내는 데 상당한 지연을 유발한다는 것을 알아차렸습니다. 회사는 복잡한 이메일 전달 문제를 해결하는 데 소요되는 시간을 줄이고 운영 오버헤드를 최소화하고자 합니다.

이 요구 사항을 충족하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?

A. 이메일 처리를 전담하는 EC2 인스턴스를 사용하여 별도의 애플리케이션 계층을 생성합니다.
B. 웹 인스턴스가 Amazon Simple Email Service(Amazon SES)를 통해 이메일을 보내도록 구성합니다.
C. 웹 인스턴스가 Amazon Simple Notification Service(Amazon SNS)를 통해 이메일을 보내도록 구성합니다.
D. 이메일 처리를 전담하는 EC2 인스턴스를 사용하여 별도의 애플리케이션 계층을 생성합니다. 인스턴스를 Auto Scaling 그룹에 배치합니다.

```
An ecommerce company is experiencing an increase in user traffic. The company’s store is deployed on Amazon EC2 instances as a two-tier web application consisting of a web tier and a separate database tier. As traffic increases, the company notices that the architecture is causing significant delays in sending timely marketing and order confirmation email to users. The company wants to reduce the time it spends resolving complex email delivery issues and minimize operational overhead.  
  
What should a solutions architect do to meet these requirements?

- A. Create a separate application tier using EC2 instances dedicated to email processing.
- B. Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).
- C. Configure the web instance to send email through Amazon Simple Notification Service (Amazon SNS).
- D. Create a separate application tier using EC2 instances dedicated to email processing. Place the instances in an Auto Scaling group.
```

정답 : `B`

- Amazon SES는 대규모 트랜잭션/마케팅 이메일 전송을 위한 완전관리형 서비스
- 애플리케이션에서 SES API 또는 SMTP 엔드포인트만 사용하면 돼 애플리케이션/인프라 변경 최소화와 운영 오버헤드 감소가 동시에 가능

오답 이유

- **A. EC2 전용 이메일 처리 계층 생성**    
    자체 메일 전송 스택을 운영해야 하므로 **MTA 관리, 큐/재시도, 바운스/불만 처리, 평판 관리** 등 운영 부담이 커집니다. 지연/전달 이슈의 근본 원인을 해소하지 못하고 오히려 복잡도가 증가합니다.

- **C. Amazon SNS로 이메일 전송**    
    SNS의 이메일 프로토콜은 **알림용 단순 메일 엔드포인트**로 전달률 관리나 대량 전송, 바운스/컴플레인트 처리 등 **이메일 전문 기능이 부족**합니다. 트랜잭션/마케팅 메일의 신뢰성·확장성 요구에 부적합합니다.

- **D. EC2 전용 계층 + Auto Scaling**
    Auto Scaling으로 컴퓨팅 확장은 가능하나, **메일 전송 인프라 자체의 운영(큐, 재시도, DNS 인증, 평판 관리 등)** 부담은 그대로이며 SES가 제공하는 **전달률/평판 관리** 이점을 대체할 수 없습니다. 비용·운영 측면에서 비효율적입니다.


## #414
한 회사에는 매일 수백 개의 보고서를 생성하는 비즈니스 시스템이 있습니다. 비즈니스 시스템은 보고서를 CSV 형식으로 네트워크 공유에 저장합니다. 회사는 이 데이터를 분석을 위해 거의 실시간(near-real time)으로 AWS 클라우드에 저장해야 합니다.

가장 적은 관리 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. AWS DataSync을 사용하여 파일을 Amazon S3로 전송합니다. 매일 하루가 끝날 때 실행되는 예약된 작업을 생성합니다.
B. Amazon S3 File Gateway를 생성합니다. 비즈니스 시스템이 S3 File Gateway의 새 네트워크 공유를 사용하도록 업데이트합니다.
C. AWS DataSync을 사용하여 파일을 Amazon S3로 전송합니다. 자동화 워크플로에서 DataSync API를 사용하는 애플리케이션을 생성합니다.
D. SFTP용 AWS Transfer 엔드포인트를 배포합니다. 네트워크 공유에서 새 파일을 확인하고 SFTP를 사용하여 새 파일을 업로드하는 스크립트를 생성합니다.

```
A company has a business system that generates hundreds of reports each day. The business system saves the reports to a network share in CSV format. The company needs to store this data in the AWS Cloud in near-real time for analysis.  
  
Which solution will meet these requirements with the LEAST administrative overhead?

- A. Use AWS DataSync to transfer the files to Amazon S3. Create a scheduled task that runs at the end of each day.
- B. Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File Gateway.
- C. Use AWS DataSync to transfer the files to Amazon S3. Create an application that uses the DataSync API in the automation workflow.
- D. Deploy an AWS Transfer for SFTP endpoint. Create a script that checks for new files on the network share and uploads the new files by using SFTP.
```

정답 : `B`

- S3 File Gateway는 온프레미스에서 SMB/NFS 네트워크 공유를 제공하고, 파일이 닫히는 즉시 S3로 비동기 업로드하여 거의 실시간으로 S3에 적재
- 기존 애플리케이션은 네트워크 드라이브 경로만 바꾸면 되므로 애플리케이션 수정 및 자동화 코드 작성 최소화
- 전송･재시도･캐시･메타데이터 관리가 서비스에서 처리되어 운영 오버헤드가 가장 낮음

오답 이유

- **A. DataSync + 일 1회 스케줄**    
    하루에 한 번 전송은 **near-real time 요건을 충족하지 못함**. 또한 지연이 길어 분석의 적시성이 떨어짐.

- **C. DataSync + API 기반 자동화 앱**    
    DataSync 자체는 적합하지만 **API를 호출하는 애플리케이션/오케스트레이션 구성**이 필요해 운영·개발 오버헤드가 증가. S3 File Gateway 대비 **관리 복잡도**가 큼.

- **D. AWS Transfer for SFTP + 스크립트**
    기존은 **SMB 네트워크 공유**인데 SFTP 업로드 스크립트를 별도로 작성·운영해야 하며, 폴링/재시도/오류 처리 등 **운영 부담**이 큼. 또한 추가 엔드포인트 비용/운영 고려 필요.


## #415
한 회사는 Amazon S3 Standard에 페타바이트 규모의 데이터를 저장하고 있습니다. 데이터는 여러 S3 버킷에 저장되어 있으며, 접근 빈도는 다양합니다. 회사는 모든 데이터에 대한 접근 패턴을 알지 못합니다. 회사는 각 S3 버킷에 대해 S3 사용 비용을 최적화할 수 있는 솔루션을 구현해야 합니다.

가장 운영 효율성이 높은 솔루션은 무엇입니까?

A. S3 수명 주기(Lifecycle) 구성을 생성하여 S3 버킷의 객체를 S3 Intelligent-Tiering으로 전환하는 규칙을 만듭니다.
B. S3 저장 클래스 분석 도구를 사용하여 S3 버킷의 각 객체에 대한 올바른 계층을 결정합니다. 각 객체를 식별된 저장 계층으로 이동합니다.
C. S3 수명 주기 구성을 생성하여 S3 버킷의 객체를 S3 Glacier Instant Retrieval로 전환하는 규칙을 만듭니다.
D. S3 수명 주기 구성을 생성하여 S3 버킷의 객체를 S3 One Zone-Infrequent Access(S3 One Zone-IA)로 전환하는 규칙을 만듭니다.

```
A company is storing petabytes of data in Amazon S3 Standard. The data is stored in multiple S3 buckets and is accessed with varying frequency. The company does not know access patterns for all the data. The company needs to implement a solution for each S3 bucket to optimize the cost of S3 usage.  
  
Which solution will meet these requirements with the MOST operational efficiency?

- A. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.
- B. Use the S3 storage class analysis tool to determine the correct tier for each object in the S3 bucket. Move each object to the identified storage tier.
- C. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Glacier Instant Retrieval.
- D. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 One Zone-Infrequent Access (S3 One Zone-IA).
```

정답 : `A`

- S3 Intelligent-Tiering은 접근 패턴을 모를 때 자동으로(모니터링 기반) 적절한 계층으로 이동시켜 비용을 최적화하는 관리형 스토리지 클래스
- 다수의 버킷과 페타바이트 규모에서 객체별 패턴을 사람이 판단할 필요가 없어 운영 오버헤드 최소
- 접근 패턴 변화에도 계층간 자동 전환으로 지속적으로 최적 비용을 유지

오답 이유

- **B. 저장 클래스 분석으로 객체별 수동 이동**    
    객체마다 적절한 계층을 **분석하고 일일이 이동**해야 하므로 규모가 클수록 **운영 부담이 매우 큼**. 접근 패턴이 시간이 지나 변해도 자동 재최적화가 되지 않습니다.

- **C. 일괄 Glacier Instant Retrieval 전환**    
    IR는 조회 지연과 최소 저장 기간/요금이 있어 **자주 접근되는 데이터**엔 비용이 증가할 수 있습니다. 접근 패턴을 모르는 상황에서 **일괄 아카이브 계층 전환은 위험**합니다.

- **D. 일괄 One Zone-IA 전환**
	단일 AZ 내 저장으로 **가용 영역 장애 시 위험**이 있으며, 모든 데이터에 적합하지 않습니다. 또한 접근 패턴 변화에 따른 자동 최적화가 없고, **내구성/복원력 요구를 저해**할 수 있습니다.


## #416
급속히 성장하는 글로벌 전자상거래 회사가 AWS에서 웹 애플리케이션을 호스팅하고 있습니다. 웹 애플리케이션에는 정적 콘텐츠와 동적 콘텐츠가 포함됩니다. 웹사이트는 온라인 트랜잭션 처리(OLTP) 데이터를 Amazon RDS 데이터베이스에 저장합니다. 웹사이트의 사용자는 페이지 로드 지연을 경험하고 있습니다.

이 문제를 해결하기 위해 솔루션스 아키텍트는 어떤 작업 조합을 수행해야 합니까? (두 가지를 선택하십시오.)

A. Amazon Redshift 클러스터를 구성합니다.
B. Amazon CloudFront 배포를 설정합니다.
C. 동적 웹 콘텐츠를 Amazon S3에 호스팅합니다.
D. RDS DB 인스턴스에 대한 읽기 전용 복제본을 생성합니다.
E. RDS DB 인스턴스에 대한 Multi-AZ 배포를 구성합니다.

```
A rapidly growing global ecommerce company is hosting its web application on AWS. The web application includes static content and dynamic content. The website stores online transaction processing (OLTP) data in an Amazon RDS database The website’s users are experiencing slow page loads.  
  
Which combination of actions should a solutions architect take to resolve this issue? (Choose two.)

- A. Configure an Amazon Redshift cluster.
- B. Set up an Amazon CloudFront distribution.
- C. Host the dynamic web content in Amazon S3.
- D. Create a read replica for the RDS DB instance.
- E. Configure a Multi-AZ deployment for the RDS DB instance.
```

정답 : `B, D`

- CloudFront는 전 세계 엣지 로케이션에서 정적 콘텐츠를 캐시해 TTFB(리소스 요청과 응답의 첫 번째 바이트가 도착하기 시작하는 시점 사이의 시간)를 낮추고 글로벌 사용자에게 지연 시간을 단축
	- 동적 콘텐츠에 대해서도 원본 보호와 최적화된 네트워크 전송(HTTP/2, 압축 등)으로 체감 속도를 개선
- RDS Read Replica는 읽기 트래픽 을 복제본으로 분산해 OLTP 주 인스턴스의 부하를 경감하고 트래픽 급증 시 읽기 지연을 줄여 페이지 로드 시간을 개선

오답 이유

- **A. Amazon Redshift 클러스터**    
    Redshift는 **OLAP(분석)** 용 데이터 웨어하우스입니다. 본문은 **OLTP** 워크로드로, 페이지 로드 지연 해소에 직접적 도움이 되지 않습니다.

- **C. 동적 콘텐츠를 S3에 호스팅**
    S3는 정적 웹 호스팅에 적합합니다. **동적 콘텐츠는 서버/컴퓨팅 계층(EC2, Lambda, 컨테이너 등)** 가 필요하므로 부적합합니다.

- **E. RDS Multi-AZ 배포**
    Multi-AZ는 **고가용성/복구 시간(HA/DR)** 향상이 목적이며, **읽기 성능 개선이나 읽기 스케일 아웃을 제공하지 않습니다**(대기 인스턴스는 읽기 엔드포인트로 사용 불가).


## #417
한 회사는 애플리케이션을 실행하기 위해 Amazon EC2 인스턴스와 AWS Lambda 함수를 사용합니다. 회사의 AWS 계정에는 퍼블릭 서브넷과 프라이빗 서브넷이 있는 VPC들이 있습니다. EC2 인스턴스는 한 VPC의 프라이빗 서브넷에서 실행됩니다. 애플리케이션이 동작하려면 Lambda 함수가 EC2 인스턴스에 직접 네트워크 액세스를 해야 합니다.

애플리케이션은 최소 1년 동안 실행될 것입니다. 회사는 그 기간 동안 애플리케이션에서 사용하는 Lambda 함수의 수가 증가할 것으로 예상합니다. 회사는 모든 애플리케이션 리소스에 대한 절감을 극대화하고 서비스 간 네트워크 지연 시간을 낮게 유지하고자 합니다.

어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. EC2 인스턴스 세이빙스 플랜을 구매합니다. Lambda 함수의 실행 시간과 메모리 사용량, 호출 횟수를 최적화합니다. Lambda 함수를 EC2 인스턴스가 있는 프라이빗 서브넷에 연결합니다.
B. EC2 인스턴스 세이빙스 플랜을 구매합니다. Lambda 함수의 실행 시간과 메모리 사용량, 호출 횟수, 전송되는 데이터 양을 최적화합니다. Lambda 함수를 EC2 인스턴스가 실행되는 동일한 VPC의 퍼블릭 서브넷에 연결합니다.
C. 컴퓨트 세이빙스 플랜을 구매합니다. Lambda 함수의 실행 시간과 메모리 사용량, 호출 횟수, 전송되는 데이터 양을 최적화합니다. Lambda 함수를 EC2 인스턴스가 있는 프라이빗 서브넷에 연결합니다.
D. 컴퓨트 세이빙스 플랜을 구매합니다. Lambda 함수의 실행 시간과 메모리 사용량, 호출 횟수, 전송되는 데이터 양을 최적화합니다. Lambda 함수를 Lambda 서비스 VPC에 그대로 둡니다.

```
A company uses Amazon EC2 instances and AWS Lambda functions to run its application. The company has VPCs with public subnets and private subnets in its AWS account. The EC2 instances run in a private subnet in one of the VPCs. The Lambda functions need direct network access to the EC2 instances for the application to work.  
  
The application will run for at least 1 year. The company expects the number of Lambda functions that the application uses to increase during that time. The company wants to maximize its savings on all application resources and to keep network latency between the services low.  
  
Which solution will meet these requirements?

- A. Purchase an EC2 Instance Savings Plan Optimize the Lambda functions’ duration and memory usage and the number of invocations. Connect the Lambda functions to the private subnet that contains the EC2 instances.
- B. Purchase an EC2 Instance Savings Plan Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to a public subnet in the same VPC where the EC2 instances run.
- C. Purchase a Compute Savings Plan. Optimize the Lambda functions’ duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.
- D. Purchase a Compute Savings Plan. Optimize the Lambda functions’ duration and memory usage, the number of invocations, and the amount of data that is transferred. Keep the Lambda functions in the Lambda service VPC.
```

정답 : `C`

- 컴퓨트 세이빙스 플랜은 EC2와 Lambda(그리고 Fargate)에 모두 적용되어, 증가가 예상되는 Lambda 비용까지 함께 절감 가능
- Lambda를 동일 VPC의 프라이빗 서브넷에 연결하면 ENI가 생성되어 사설 IP로 EC2에 직접 접근 가능
	- 같은 VPC/서브넷 대역에서 왕복 지연을 최소화하고, SG로 세밀 제어 가능

오답 이유

- **A. EC2 인스턴스 세이빙스 플랜 + 프라이빗 서브넷 연결**    
    네트워크 배치는 적절하지만, **세이빙스 플랜이 Lambda에 적용되지 않음** → “모든 애플리케이션 리소스 절감” 요구 불충족.

- **B. EC2 인스턴스 세이빙스 플랜 + 퍼블릭 서브넷 연결**    
    Lambda가 퍼블릭 서브넷에 붙어도 VPC 내부 통신은 가능하지만, **Lambda 비용 절감이 제외**되고, 보안/네트워크 설계상 불필요한 퍼블릭 서브넷 의존이 생길 수 있음.

- **D. 컴퓨트 세이빙스 플랜 + Lambda 서비스 VPC 유지**
    Lambda를 VPC에 연결하지 않으면 **프라이빗 서브넷의 EC2에 직접 접근 불가**(사설 IP로 통신 불가). 문제의 필수 요구 조건 위배.


## #418
한 솔루션스 아키텍트는 팀 구성원들이 두 개의 서로 다른 AWS 계정(개발 계정과 운영 계정)에 있는 Amazon S3 버킷에 접근할 수 있도록 해야 합니다.  
팀은 현재 개발 계정 내에서, 적절한 권한이 부여된 IAM 그룹에 속한 고유한 IAM 사용자 계정을 통해 개발 계정의 S3 버킷에 접근할 수 있습니다.

솔루션스 아키텍트는 운영 계정(Production Account)에 IAM 역할(Role)을 생성했습니다.  
이 역할에는 운영 계정의 S3 버킷에 접근할 수 있는 정책이 부여되어 있습니다.

**최소 권한 원칙(Principle of Least Privilege)**을 준수하면서 이러한 요구사항을 충족하는 솔루션은 무엇입니까?

A. 개발 계정의 사용자들에게 AdministratorAccess 정책을 연결합니다.  
B. 운영 계정의 역할 신뢰 정책(Trust Policy)에 개발 계정을 Principal로 추가합니다.  
C. 운영 계정의 S3 버킷에서 S3 Block Public Access 기능을 비활성화합니다.  
D. 운영 계정에 각 팀 구성원마다 고유한 자격 증명을 가진 사용자를 생성합니다.

```
A solutions architect needs to allow team members to access Amazon S3 buckets in two different AWS accounts: a development account and a production account. The team currently has access to S3 buckets in the development account by using unique IAM users that are assigned to an IAM group that has appropriate permissions in the account.  
  
The solutions architect has created an IAM role in the production account. The role has a policy that grants access to an S3 bucket in the production account.  
  
Which solution will meet these requirements while complying with the principle of least privilege?

- A. Attach the Administrator Access policy to the development account users.
- B. Add the development account as a principal in the trust policy of the role in the production account.
- C. Turn off the S3 Block Public Access feature on the S3 bucket in the production account.
- D. Create a user in the production account with unique credentials for each team member.
```

정답 : `B`

- 운영 계정에 있는 IAM Role 개발 계정의 IAM 사용자들이 AssumeRole을 통해 임시적으로 접근하도록 허용하는 것이 가장 안전하고 권장되는 방법
- 운영 계정(Role의 신뢰 정책)에 개발 계정을 Principal로 추가하면, 개발 계정의 사용자들이 그 역할을 위임받아 운영 계정의 S3 버킷에만 필요한 권한으로 접근 가능

오답 이유

- **A. AdministratorAccess 정책 부여**
    - 전체 AWS 리소스에 대한 **루트 수준 권한**을 부여하므로 **최소 권한 원칙에 위배**됩니다.
    - 보안상 매우 위험하며, 단일 S3 접근 용도에는 과도한 권한입니다.        

- **C. S3 Block Public Access 비활성화**
    - 이는 **공개 접근(Public Access)** 설정을 변경하는 것이며, 계정 간 권한 위임과는 무관합니다.
    - 오히려 보안 위험을 높이며, 내부 팀 접근 문제를 해결하지 못합니다.

- **D. 운영 계정에 사용자 생성**
    - 각 사용자마다 **별도의 IAM 자격 증명 관리**가 필요하므로 **운영 오버헤드 증가** 및 보안 위험(자격 증명 노출, 중복 관리) 발생.
    - Cross-Account Role 방식보다 **효율성과 보안성이 낮음**.


## #419
한 회사는 모든 기능이 활성화된 AWS Organizations를 사용하며 ap-southeast-2 리전에서 여러 Amazon EC2 워크로드를 운영하고 있습니다. 회사에는 다른 어떤 리전에서도 리소스를 생성하지 못하도록 하는 서비스 제어 정책(SCP)이 있습니다. 보안 정책에 따라 회사는 저장된 모든 데이터가 암호화된 상태여야 합니다.

감사 결과, 직원들이 Amazon Elastic Block Store(Amazon EBS) 볼륨을 암호화하지 않은 채 EC2 인스턴스용으로 생성한 사실이 발견되었습니다. 회사는 ap-southeast-2에서 어떤 IAM 사용자나 루트 사용자가 시작하는 새로운 EC2 인스턴스가 암호화된 EBS 볼륨을 사용하도록 하기를 원합니다. 회사는 EBS 볼륨을 생성하는 직원들에게 미치는 영향을 최소화하는 솔루션을 원합니다.

이 요구 사항을 충족하는 단계 조합은 무엇입니까? (두 가지 선택)

A. Amazon EC2 콘솔에서 EBS 암호화 계정 속성을 선택하고 기본 암호화 키를 정의합니다.
B. IAM 권한 경계를 생성합니다. 권한 경계를 루트 조직 단위(OU)에 연결합니다. ec2:Encrypted 조건이 false일 때 ec2:CreateVolume 작업을 거부하도록 경계를 정의합니다.
C. SCP를 생성합니다. SCP를 루트 조직 단위(OU)에 연결합니다. ec2:Encrypted 조건이 false일 때 ec2:CreateVolume 작업을 거부하도록 SCP를 정의합니다.
D. 각 계정의 IAM 정책을 업데이트하여 ec2:CreateVolume 작업을 ec2:Encrypted 조건이 false일 때 거부합니다.
E. Organizations 관리 계정에서 기본 EBS 볼륨 암호화 설정을 지정합니다.

```
A company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 workloads in the ap-southeast-2 Region. The company has a service control policy (SCP) that prevents any resources from being created in any other Region. A security policy requires the company to encrypt all data at rest.  
  
An audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) volumes for EC2 instances without encrypting the volumes. The company wants any new EC2 instances that any IAM user or root user launches in ap-southeast-2 to use encrypted EBS volumes. The company wants a solution that will have minimal effect on employees who create EBS volumes.  
  
Which combination of steps will meet these requirements? (Choose two.)

- A. In the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption key.
- B. Create an IAM permission boundary. Attach the permission boundary to the root organizational unit (OU). Define the boundary to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.
- C. Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false.
- D. Update the IAM policies for each account to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.
- E. In the Organizations management account, specify the Default EBS volume encryption setting.
```

정답 : `C, E`

- SCP를 루트 조직 단위에 연결하여 ec2:Encrypted=false 조건이 포함된 ec2:CreateVolume 작업을 조직 전체에서 거부하도록 설정
	- IAM 정책이나 개별 사용자 설정과 관계없이, 조직 내 어느 계정에서도 비암호화된 EBS 볼륨을 생성할 수 없음
- AWS Organizations의 계정 관리 기능을 통해 각 하위 계정에 대해 EBS 기본 암호화(EBS Encryption by Default) 설정을 중아에서 지정 가능
	- 각 계정에서 생성되는 새로운 EBS 볼륨은 자동으로 암호화되어 생성되며, 사용자가 별도로 암호화 여부를 선택할 필요 없음

오답 이유

- **A. EBS 암호화 기본값 설정 (EC2 콘솔에서 수동 설정)**
    이 설정은 **개별 계정 단위**에서만 적용되므로, 여러 계정을 사용하는 조직 환경에서는 **중앙 제어가 어렵습니다.**    
    Organizations를 통해 일괄 적용(E 선택지)하는 것이 더 효율적입니다.

- **B. 권한 경계**
    권한 경계는 **개별 IAM 사용자 또는 역할 수준에서만** 적용 가능합니다.    
    OU 단위나 조직 전체에는 적용할 수 없기 때문에 이 시나리오에서는 적절하지 않습니다.

- **D. 각 계정 IAM 정책 수정**
    모든 계정의 IAM 정책을 일일이 수정해야 하므로 **운영 오버헤드가 매우 큽니다.**
    SCP를 통해 중앙에서 제어하는 것이 훨씬 효율적입니다.


## #420
한 회사는 프로덕션 데이터베이스 워크로드에 대해 시간 소모적인 데이터베이스 관리 작업을 간소화하기 위해 Amazon RDS for PostgreSQL DB 클러스터를 사용하려고 합니다. 회사는 데이터베이스가 고가용성을 보장하고 대부분의 시나리오에서 40초 미만으로 자동 장애 조치를 제공하도록 하고자 합니다. 또한 기본 인스턴스에서 읽기 트래픽을 오프로드하고 비용을 가능한 한 낮게 유지하고자 합니다.

이러한 요구 사항을 충족할 솔루션은 무엇입니까?

A. Amazon RDS Multi-AZ DB 인스턴스 배포를 사용합니다. 읽기 복제본을 하나 생성하고 읽기 워크로드를 읽기 복제본으로 지정합니다.
B. Amazon RDS Multi-AZ DB duster 배포를 사용합니다. 읽기 복제본을 두 개 생성하고 읽기 워크로드를 읽기 복제본으로 지정합니다.
C. Amazon RDS Multi-AZ DB 인스턴스 배포를 사용합니다. 읽기 워크로드를 Multi-AZ 페어의 보조 인스턴스로 지정합니다.
D. Amazon RDS Multi-AZ DB 클러스터 배포를 사용합니다. 읽기 워크로드를 리더 엔드포인트로 지정합니다.

```
A company wants to use an Amazon RDS for PostgreSQL DB cluster to simplify time-consuming database administrative tasks for production database workloads. The company wants to ensure that its database is highly available and will provide automatic failover support in most scenarios in less than 40 seconds. The company wants to offload reads off of the primary instance and keep costs as low as possible.  
  
Which solution will meet these requirements?

- A. Use an Amazon RDS Multi-AZ DB instance deployment. Create one read replica and point the read workload to the read replica.
- B. Use an Amazon RDS Multi-AZ DB duster deployment Create two read replicas and point the read workload to the read replicas.
- C. Use an Amazon RDS Multi-AZ DB instance deployment. Point the read workload to the secondary instances in the Multi-AZ pair.
- D. Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint.
```

정답 : `D`

- RDS Multi-AZ DB 클러스터(Writer 1 + Reader 2 구성)는 대부분의 장애에서 ~35초 수준의 자동 장애 조치를 제공
- 클러스터에는 리더 엔드포인트가 제공되어 두 개의 읽기 가능 인스턴스로 자동 분산해 주므로, 기본 인스턴스의 부하를 효과적으로 오프로드
- 별도의 비동기 읽기 복제본을 추가 구매/운영하지 않아도 읽기 확장이 가능하므로 요구사항인 비용 최소화에도 부합

오답 이유

- **A. Multi-AZ DB 인스턴스 + 별도 Read Replica 1개**    
    전통적인 **Multi-AZ 인스턴스 배포의 보조(standby)는 읽기 불가**이며, 페일오버도 보통 수십 초~수분이 걸릴 수 있습니다. 읽기 오프로드를 위해선 추가로 읽기 복제본 운영이 필요해 **운영/비용 부담**이 증가합니다.

- **B. Multi-AZ DB duster + Read Replica 2개**    
    Multi-AZ **DB 클러스터 자체가 이미 읽기 가능한 리더 인스턴스를 포함**합니다. 여기에 별도의 읽기 복제본을 또 운영하는 것은 **과도한 비용/복잡도**를 초래합니다. (또한 선택지의 ‘duster’는 오타로 보임)

- **C. Multi-AZ DB 인스턴스의 보조 인스턴스로 읽기 지정**
    **인스턴스 기반 Multi-AZ의 보조 인스턴스는 읽기 엔드포인트가 아님**(읽기 불가). 읽기 오프로드가 되지 않으므로 요구사항 불충족입니다.