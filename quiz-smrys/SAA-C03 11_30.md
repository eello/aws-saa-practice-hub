---
created: 2025-09-24 09:29:52
last_modified: 2025-09-25 08:24:59
---
## #11
```
회사에는 Amazon EC2 인스턴스에서 실행되는 애플리케이션이 있으며 Amazon Aurora 데이터베이스를 사용합니다. EC2 인스턴스는 로컬 파일에 저장된 사용자 이름과 암호를 사용하여 데이터베이스에 연결합니다. 회사는 자격 증명 관리의 운영 오버헤드를 최소화하려고 합니다.
이 목표를 달성하기 위해 솔루션 아키텍트는 무엇을 해야 합니까?

A. AWS Secrets Manager를 사용합니다. 자동 로테이션을 켭니다.
B. AWS Systems Manager Parameter Store를 사용합니다. 자동 로테이션을 켭니다.
C. AWS Key Management Service(AWS KMS)로 암호화된 객체를 저장할 Amazon S3 버킷을 생성합니다. 자격 증명 파일을 S3로 마이그레이션합니다. 애플리케이션이 S3 버킷을 가리키도록 합니다.
D. 각 EC2 인스턴스에 대해 암호화된 Amazon EBS 볼륨을 생성합니다. 각 EC2 인스턴스에 새 EBS 볼륨을 연결합니다. 자격 증명 파일을 새 EBS 볼륨으로 마이그레이션합니다. 애플리케이션이 새 EBS 볼륨을 가리키도록 합니다.

---

A company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora database. The EC2 instances connect to the database by using user names and passwords that are stored locally in a file. The company wants to minimize the operational overhead of credential management.  
What should a solutions architect do to accomplish this goal?

- A. Use AWS Secrets Manager. Turn on automatic rotation.
- B. Use AWS Systems Manager Parameter Store. Turn on automatic rotation.
- C. Create an Amazon S3 bucket to store objects that are encrypted with an AWS Key Management Service (AWS KMS) encryption key. Migrate the credential file to the S3 bucket. Point the application to the S3 bucket.
- D. Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume for each EC2 instance. Attach the new EBS volume to each EC2 instance. Migrate the credential file to the new EBS volume. Point the application to the new EBS volume.
```

정답 : `A`
- AWS Secrets Manager는 데이터베이스 자격증명을 안전하게 저장하도록 설계
- RDS/Aurora와 통합된 자동 비밀번호(자격 증명) 로테이션을 기본 제공
- 자동 로테이션을 활성화하면 Secrets Manager가 람다(또는 내장기능)를 통해 주기적으로 비밀번호를 안전하게 교체
- 애플리케이션은 Secrets Manager API 또는 SDK를 통해 최신 자격 증명을 가져오도록 구성하면 운영자가 수동으로 파일을 관리하거나 배포할 필요가 줄어듦

오답 이유
**B**
- Systems Manager Parameter Store(특히 SecureString)는 자격 증명을 안전하게 저장할 수는 있지만, Secrets Manager처럼 RDS/Aurora용 **내장 자동 로테이션 기능을 제공하지 않습니다**. 로테이션을 구현하려면 Lambda와 추가 오케스트레이션을 직접 만들어야 하므로 운영 오버헤드가 증가합니다. 따라서 “자동 로테이션을 켠다”는 문구를 충족하지 못합니다.

**C**
- S3에 암호화된 객체로 자격 증명을 저장하는 것은 암호화 측면에서는 가능하지만, **자동 로테이션, 버전 관리에 따른 안전한 교체, IAM 기반의 비밀 전용 기능(예: 임시 자격증명 제공) 등 비밀 관리의 핵심 기능이 부족**합니다. 또한 애플리케이션이 S3에서 자격증명을 읽으려면 추가 코드(권한, 캐시, 재시도, 보안 주의)가 필요하고 비밀번호 교체 시 배포/동기화 문제가 발생합니다.

**D**
- 각 EC2 인스턴스에 EBS 볼륨을 만들어 자격증명을 저장하는 방식은 **비밀이 인스턴스 바깥으로 안전하게 회전되지 않으며**, 인스턴스별로 파일을 관리해야 하므로 운영 오버헤드와 보안 위험(예: 스냅샷 노출, 볼륨 복제)이 높아집니다. 또한 자동 로테이션을 제공하지 않아 요구사항을 만족하지 않습니다.


## #12
```
글로벌 회사가 Amazon EC2 인스턴스와 Application Load Balancer(ALB)를 사용하여 웹 애플리케이션을 호스팅하고 있습니다.  
웹 애플리케이션에는 정적 데이터와 동적 데이터가 있습니다. 회사는 정적 데이터를 Amazon S3 버킷에 저장하고 있습니다.  
회사는 정적 데이터와 동적 데이터 모두에 대해 성능을 향상시키고 지연 시간을 줄이고자 합니다.  
또한 회사는 Amazon Route 53에 등록된 자체 도메인 이름을 사용하고 있습니다.  

이러한 요구사항을 충족하기 위해 솔루션 아키텍트는 무엇을 해야 합니까?

A. S3 버킷과 ALB를 오리진으로 갖는 Amazon CloudFront 배포를 생성합니다. Route 53을 구성하여 CloudFront 배포로 트래픽을 라우팅합니다.  
B. ALB를 오리진으로 갖는 Amazon CloudFront 배포를 생성합니다. S3 버킷을 엔드포인트로 갖는 AWS Global Accelerator 표준 가속기를 생성합니다. Route 53을 구성하여 CloudFront 배포로 트래픽을 라우팅합니다.  
C. S3 버킷을 오리진으로 갖는 Amazon CloudFront 배포를 생성합니다. ALB와 CloudFront 배포를 엔드포인트로 갖는 AWS Global Accelerator 표준 가속기를 생성합니다. 커스텀 도메인 이름을 생성하여 가속기 DNS 이름을 가리키게 합니다. 해당 커스텀 도메인을 웹 애플리케이션의 엔드포인트로 사용합니다.  
D. ALB를 오리진으로 갖는 Amazon CloudFront 배포를 생성합니다. S3 버킷을 엔드포인트로 갖는 AWS Global Accelerator 표준 가속기를 생성합니다. 두 개의 도메인 이름을 생성합니다. 하나는 CloudFront DNS 이름(동적 콘텐츠용), 다른 하나는 가속기 DNS 이름(정적 콘텐츠용)을 가리키게 합니다. 두 도메인 이름을 웹 애플리케이션 엔드포인트로 사용합니다.  

---

A global company hosts its web application on Amazon EC2 instances behind an Application Load Balancer (ALB). The web application has static data and dynamic data. The company stores its static data in an Amazon S3 bucket. The company wants to improve performance and reduce latency for the static data and dynamic data. The company is using its own domain name registered with Amazon Route 53.  
What should a solutions architect do to meet these requirements?

- A. Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins. Configure Route 53 to route traffic to the CloudFront distribution.
- B. Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint Configure Route 53 to route traffic to the CloudFront distribution.
- C. Create an Amazon CloudFront distribution that has the S3 bucket as an origin. Create an AWS Global Accelerator standard accelerator that has the ALB and the CloudFront distribution as endpoints. Create a custom domain name that points to the accelerator DNS name. Use the custom domain name as an endpoint for the web application.
- D. Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint. Create two domain names. Point one domain name to the CloudFront DNS name for dynamic content. Point the other domain name to the accelerator DNS name for static content. Use the domain names as endpoints for the web application.
```

정답 : `A`
- CloudFront 는 전 세계 엣지 로케이션을 통해 정적 콘텐츠(S3)와 동적 콘텐츠(ALB의 EC2)를 모두 캐싱/가속할 수 있음
- 단일 CloudFront 배포에서 다중 오리진(S3 + ALB) 설정이 가능하므로, 정적/동적 콘텐츠를 모두 처리할 수 있음
- Route 53은 커스텀 도메인에서 CloudFront 배포로 트래픽을 전달하므로, 사용자는 단일 도메인에서 정적/동적 데이터를 모두 빠르게 접근할 수 있음
- Global Accelerator는 다중 리전 애플리케이션을 위한 최적화 도구이지만, 문제의 요구사항은 단일 글로벌 애플리케이션에 대해 지연 시간 개선 + 캐싱을 요구하고 있으므로 CloudFront가 정답

오답 이유
**B - ALB만 CloudFront로 가속하고, 정적 데이터(S3)는 Global Accelerator를 통해 접근**
- Global Accelerator는 캐싱을 제공하지 않습니다. 따라서 정적 콘텐츠는 여전히 원본 S3에서 가져와야 하므로 성능 최적화 부족.
- S3 정적 콘텐츠는 CloudFront를 사용하는 것이 훨씬 효율적입니다.  

**C - CloudFront와 ALB를 [[AWS Services#AWS Global Accelerator|Gloval Accelerator]]와 함께 혼합 사용**
- 불필요하게 복잡합니다. 단순히 CloudFront 하나로도 전 세계 지연 시간 단축 및 캐싱이 가능합니다.
- Global Accelerator는 다중 리전 액티브-액티브 아키텍처에 더 적합한 서비스입니다.

**D - 두 개의 별도 도메인 사용 (정적/동적 분리)**
- 사용자 경험이 나빠집니다. 정적/동적 콘텐츠를 따로 다른 도메인으로 호출해야 하므로 애플리케이션 통합성이 깨짐.
- CloudFront 하나로 통합 처리 가능하므로 불필요한 복잡성.


## #13
```
한 회사는 AWS 인프라에서 월간 유지보수를 수행합니다. 이러한 유지보수 동안, 회사는 여러 AWS 리전에서 Amazon RDS for MySQL 데이터베이스의 자격 증명을 회전할 필요가 있습니다.  
가장 적은 운영 오버헤드로 이 요구사항을 충족하는 솔루션은 무엇입니까?

A. AWS Secrets Manager에 자격 증명을 비밀로 저장합니다. 필요한 리전에 대해 다중 리전 비밀 복제를 사용합니다. Secrets Manager가 일정에 따라 비밀을 회전하도록 구성합니다.  
B. AWS Systems Manager에 보안 문자열 매개변수를 생성하여 자격 증명을 저장합니다. 필요한 리전에 대해 다중 리전 비밀 복제를 사용합니다. Systems Manager가 일정에 따라 비밀을 회전하도록 구성합니다.  
C. 서버 측 암호화(SSE)가 활성화된 Amazon S3 버킷에 자격 증명을 저장합니다. Amazon EventBridge(Amazon CloudWatch Events)를 사용하여 AWS Lambda 함수를 호출하여 자격 증명을 회전합니다.  
D. AWS Key Management Service(AWS KMS) 다중 리전 고객 관리 키를 사용하여 자격 증명을 암호화합니다. Amazon DynamoDB 글로벌 테이블에 비밀을 저장합니다. Lambda 함수를 사용하여 DynamoDB에서 비밀을 검색하고 RDS API를 사용하여 비밀을 회전합니다.

---

A company performs monthly maintenance on its AWS infrastructure. During these maintenance activities, the company needs to rotate the credentials for its Amazon RDS for MySQL databases across multiple AWS Regions.  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Store the credentials as secrets in AWS Secrets Manager. Use multi-Region secret replication for the required Regions. Configure Secrets Manager to rotate the secrets on a schedule.
- B. Store the credentials as secrets in AWS Systems Manager by creating a secure string parameter. Use multi-Region secret replication for the required Regions. Configure Systems Manager to rotate the secrets on a schedule.
- C. Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke an AWS Lambda function to rotate the credentials.
- D. Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-Region customer managed keys. Store the secrets in an Amazon DynamoDB global table. Use an AWS Lambda function to retrieve the secrets from DynamoDB. Use the RDS API to rotate the secrets.
```

정답 : `A`
- 시크릿 매니저는 RDS와 통합되어 자동으로 데이터베이스 자격 증명을 회전할 수 있음
- 다중 리전 비밀 복제를 지원하여 여러 리전에서 동일한 비밀을 관리 가능
- 관리형 서비스이므로 직접 람다나 EventBridge를 설정할 필요 없이 최소한의 운영 오버헤드로 자격 증명 회전 가능
- RDS 전용 회전 기능을 제공하므로 안전하고 신뢰성 높은 자격 증명 회전 가능

오답 이유
**B - 이유**:
- Systems Manager Parameter Store는 비밀 저장 및 회전을 지원하지만, RDS 회전과의 통합이 Secrets Manager만큼 원활하지 않음. 다중 리전 자동 복제 기능도 제한적이며 운영 오버헤드가 증가함.

**C - 이유**:
- S3와 Lambda/EventBridge 조합은 수동 스크립트를 만들어야 하고, RDS와의 직접 통합이 없으므로 회전 로직을 직접 구현해야 함. 운영 오버헤드가 가장 높음.

**D - 이유**:
- KMS + DynamoDB + Lambda + RDS API를 조합해야 하며, 모든 회전 로직과 다중 리전 동기화를 직접 구현해야 함. 관리 부담이 큼.

## #14
```
한 회사는 Amazon EC2 인스턴스 뒤에 Application Load Balancer를 두고 전자상거래 애플리케이션을 운영합니다. 인스턴스는 여러 가용 영역에 걸쳐 있는 EC2 Auto Scaling 그룹에서 실행됩니다. Auto Scaling 그룹은 CPU 사용률 지표를 기준으로 확장됩니다.  
전자상거래 애플리케이션은 MySQL 8.0 데이터베이스에 거래 데이터를 저장하며, 이 데이터베이스는 대형 EC2 인스턴스에서 호스팅됩니다.  
애플리케이션 부하가 증가하면 데이터베이스 성능이 빠르게 저하됩니다. 애플리케이션은 쓰기보다는 읽기 요청이 많습니다.  
회사는 예측할 수 없는 읽기 작업 부하를 처리하면서 **자동으로 데이터베이스를 확장**하고, **고가용성**을 유지하는 솔루션을 원합니다.  

A. 단일 노드로 leader와 compute 기능을 갖춘 Amazon Redshift 사용  
B. Single-AZ 배포로 Amazon RDS 사용, 다른 가용 영역에 리더 인스턴스 추가  
C. Multi-AZ 배포로 Amazon Aurora 사용, Aurora Replica와 함께 Aurora Auto Scaling 구성  
D. EC2 Spot 인스턴스로 Amazon ElastiCache for Memcached 사용

---

A company runs an ecommerce application on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales based on CPU utilization metrics. The ecommerce application stores the transaction data in a MySQL 8.0 database that is hosted on a large EC2 instance.  
The database's performance degrades quickly as application load increases. The application handles more read requests than write transactions. The company wants a solution that will automatically scale the database to meet the demand of unpredictable read workloads while maintaining high availability.  
Which solution will meet these requirements?

- A. Use Amazon Redshift with a single node for leader and compute functionality.
- B. Use Amazon RDS with a Single-AZ deployment Configure Amazon RDS to add reader instances in a different Availability Zone.
- C. Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora Replicas.
- D. Use Amazon ElastiCache for Memcached with EC2 Spot Instances.
```

정답 : `C`
- Aurora는 읽기 전용 복제본(오로라 레플리카)을 쉽게 생성 가능
- 오로라 오토 스케일링을 사용하면 읽기 부하 증가 시 자동으로 복제본을 추가하고 부하 감소시 제거 가능
- 멀티 AZ 배포로 기본 인스턴스가 실패해도 자동 장애 조치 지원
- 기존 MySQL 8.0과 호환 가능하고 마이그레이션이 비교적 용이

오답 이유
**A - 이유**:
- Amazon Redshift는 OLAP(분석용 데이터웨어하우스)용으로, 트랜잭션 중심(ecommerce MySQL) 워크로드에는 적합하지 않음.

**B - 이유**:
- Single-AZ 배포는 기본 인스턴스 장애 시 다운타임 발생 가능.
- RDS Reader 인스턴스 추가는 가능하지만, 자동 확장(Auto Scaling) 기능이 제한적임.

**D - 이유**:
- ElastiCache는 캐시 솔루션으로, DB 자체를 확장하는 것이 아님.
- 읽기 성능 향상에는 도움이 되지만, 트랜잭션 데이터 저장과 고가용성을 제공하지 못함.


## #15
```
한 회사는 최근 AWS로 마이그레이션을 완료했으며, 프로덕션 VPC로 들어오고 나가는 트래픽을 보호하는 솔루션을 구현하고자 합니다.  
이 회사는 온프레미스 데이터 센터에 검사 서버를 운영했었는데, 해당 서버는 트래픽 흐름 검사와 트래픽 필터링과 같은 특정 작업을 수행했습니다.  
회사는 AWS 클라우드에서도 동일한 기능을 수행하고 싶어합니다.  

A. Amazon GuardDuty를 사용하여 프로덕션 VPC에서 트래픽 검사 및 필터링 수행  
B. Traffic Mirroring을 사용하여 프로덕션 VPC의 트래픽을 복제해 트래픽 검사 및 필터링 수행  
C. AWS Network Firewall을 사용하여 프로덕션 VPC에서 트래픽 검사 및 필터링을 위한 규칙 생성  
D. AWS Firewall Manager를 사용하여 프로덕션 VPC에서 트래픽 검사 및 필터링을 위한 규칙 생성

---

A company recently migrated to AWS and wants to implement a solution to protect the traffic that flows in and out of the production VPC. The company had an inspection server in its on-premises data center. The inspection server performed specific operations such as traffic flow inspection and traffic filtering. The company wants to have the same functionalities in the AWS Cloud.  
Which solution will meet these requirements?

- A. Use Amazon GuardDuty for traffic inspection and traffic filtering in the production VPC.
- B. Use Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering.
- C. Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering for the production VPC.
- D. Use AWS Firewall Manager to create the required rules for traffic inspection and traffic filtering for the production VPC.
```

정답 : `C`
- AWS Network Firewall은 VPC 수준에서 들어오고 나가는 트래픽을 검사하고 필터링할 수 있는 완전 관리형 방화벽 서비스
- 온프레미스에서 수행하던 트래픽 검사와 트래픽 필터링을 그대로 구현 가능
- 상태 기반 방화벽과 패킷 필터링 규칙을 지원하며 보안 정책 관리가 편리
- 프로덕션 VPC의 모든 서브넷 트래픽을 대상으로 적용 가능, 고가용성과 자동 확장 지원

오답 이유
**A - 이유**:
- GuardDuty는 **위협 탐지(Threat Detection)** 서비스이며, 트래픽을 직접 차단하거나 필터링하는 기능은 없음.
- 트래픽 분석용은 가능하지만 방화벽 역할을 하진 않음.

**B - 이유**:
- Traffic Mirroring은 트래픽을 **복제해서 모니터링/분석**하는 용도
- 트래픽 필터링이나 차단 기능은 제공하지 않음.  

**D - 이유**:
- Firewall Manager는 **조직 전체 방화벽 정책 관리** 서비스
- 개별 VPC에서 트래픽 검사/필터링 규칙을 직접 적용하는 서비스는 아니고, Network Firewall이나 WAF 정책을 중앙에서 관리하는 역할임.

## #16
```
한 회사는 AWS에 데이터 레이크를 호스팅하고 있습니다. 데이터 레이크는 Amazon S3와 Amazon RDS for PostgreSQL에 저장된 데이터를 포함합니다.  
회사는 **데이터 시각화 기능이 포함된 보고 솔루션**이 필요하며, 데이터 레이크 내 모든 데이터 소스를 포함해야 합니다.  
회사의 경영진만 모든 시각화에 대한 **전체 액세스**를 갖고, 나머지 직원은 제한된 액세스만 갖도록 해야 합니다.  

A. Amazon QuickSight에서 분석을 생성합니다. 모든 데이터 소스를 연결하고 새로운 데이터셋을 생성합니다. 대시보드를 게시하여 데이터를 시각화하고, 적절한 IAM 역할과 대시보드를 공유합니다.  
B. Amazon QuickSight에서 분석을 생성합니다. 모든 데이터 소스를 연결하고 새로운 데이터셋을 생성합니다. 대시보드를 게시하여 데이터를 시각화하고, 적절한 사용자와 그룹과 대시보드를 공유합니다.  
C. Amazon S3의 데이터를 위해 AWS Glue 테이블과 크롤러를 생성합니다. AWS Glue ETL 작업으로 보고서를 생성하고 S3에 게시합니다. S3 버킷 정책을 사용해 보고서 접근을 제한합니다.  
D. Amazon S3의 데이터를 위해 AWS Glue 테이블과 크롤러를 생성합니다. Amazon Athena Federated Query를 사용해 RDS PostgreSQL 데이터에 접근합니다. Athena로 보고서를 생성하고 S3에 게시합니다. S3 버킷 정책으로 접근을 제한합니다.

--

A company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations. The rest of the company should have only limited access.  
Which solution will meet these requirements?

- A. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles.
- B. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.
- C. Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.
- D. Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL. Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.
```

정답 : `B`
- QuickSight는 S3, RDS 등 다양한 데이터 소스를 통합하여 시각화 가능
- QuickSight에서 사용자와 그룹 단위로 대시보드 공유 가능 -> 경영진에게 전체 엑세스, 나머지 직원은 제한된 액세스 설정 가능
- IAM 역할로 공유하면 일부 제약이 있어 관리가 복잡할 수 있음 -> 사용자/그룹 단위 공유 권장

오답 이유
**A - 이유**:
- IAM 역할을 기반으로 공유하면 QuickSight 사용자 계정과 직접 연결되지 않아 세밀한 접근 제어가 어렵고 관리가 복잡함

**C - 이유**:
- Glue ETL + S3 접근 제어는 보고서를 생성하고 공유할 수 있지만 **대시보드 수준의 인터랙티브 시각화**를 제공하지 않음
- 경영진과 직원 간 접근 제한을 실시간으로 관리하기 어려움

**D - 이유**:
- Athena + S3 접근 제어 역시 보고서 생성 및 배포는 가능하지만, QuickSight처럼 **인터랙티브 대시보드** 제공이 제한적임
- 시각화 및 접근 제어가 QuickSight보다 유연하지 않음


## #17
```
한 회사가 새로운 비즈니스 애플리케이션을 구현하고 있습니다.  
애플리케이션은 두 개의 Amazon EC2 인스턴스에서 실행되며, 문서 저장을 위해 Amazon S3 버킷을 사용합니다.  
솔루션 아키텍트는 EC2 인스턴스가 S3 버킷에 접근할 수 있도록 해야 합니다.  

A. S3 버킷 접근 권한을 부여하는 IAM 역할을 생성하고, EC2 인스턴스에 연결  
B. S3 버킷 접근 권한을 부여하는 IAM 정책을 생성하고, EC2 인스턴스에 연결  
C. S3 버킷 접근 권한을 부여하는 IAM 그룹을 생성하고, EC2 인스턴스에 연결  
D. S3 버킷 접근 권한을 부여하는 IAM 사용자를 생성하고, EC2 인스턴스에 연결

---

A company is implementing a new business application. The application runs on two Amazon EC2 instances and uses an Amazon S3 bucket for document storage. A solutions architect needs to ensure that the EC2 instances can access the S3 bucket.  
What should the solutions architect do to meet this requirement?

- A. Create an IAM role that grants access to the S3 bucket. Attach the role to the EC2 instances.
- B. Create an IAM policy that grants access to the S3 bucket. Attach the policy to the EC2 instances.
- C. Create an IAM group that grants access to the S3 bucket. Attach the group to the EC2 instances.
- D. Create an IAM user that grants access to the S3 bucket. Attach the user account to the EC2 instances.
```

정답 : `A`
- IAM 역할(Role)은 EC2 인스턴스에 연결하여 해당 인스턴스에서 AWS 서비스(S3 등)에 안전하게 접근 가능하게 하는 권한 부여 방식
- EC2 인스턴스에 IAM 역할을 연결하면 인스턴스 프로파일을 통해 임시 보안 자격 증명이 자동으로 제공됨
- 사용자 인증 정보(Access Key/Secret Key)를 코드에 하드코딩할 필요 없음 -> 보안과 관리 측면에서 안전

오답 이유
**B - 이유**:
- IAM 정책은 권한을 정의하는 문서일 뿐, **직접 EC2 인스턴스에 연결할 수 없음**
- 정책은 **역할(Role)이나 사용자(User) 또는 그룹(Group)**에 붙여야 함

**C - 이유**:
- IAM 그룹은 사용자(User)를 관리하기 위한 단위이며, **EC2 인스턴스에 직접 연결할 수 없음**

**D - 이유**:
- IAM 사용자를 생성하고 Access Key를 EC2에 하드코딩하면 가능하지만, **보안상 권장되지 않음**
- IAM 역할을 사용하는 것이 가장 안전하고 관리가 용이함

## #18
```
애플리케이션 개발팀은 대용량 이미지를 작은 크기의 압축 이미지로 변환하는 마이크로서비스를 설계하고 있습니다.  
사용자가 웹 인터페이스를 통해 이미지를 업로드하면, 마이크로서비스는 이미지를 Amazon S3 버킷에 저장하고, AWS Lambda 함수로 처리 및 압축한 후 다른 S3 버킷에 압축된 이미지를 저장해야 합니다.  
솔루션 아키텍트는 **내구성이 있으며 상태가 없는(stateless) 구성 요소**를 사용하여 이미지를 자동으로 처리하는 솔루션을 설계해야 합니다.
* 두 개 선택

A. Amazon Simple Queue Service(Amazon SQS) 큐를 생성합니다. 이미지가 S3 버킷에 업로드될 때 S3 버킷에서 SQS 큐로 알림을 전송하도록 구성합니다.  
B. Lambda 함수를 Amazon SQS 큐를 호출 소스로 사용하도록 구성합니다. SQS 메시지가 성공적으로 처리되면 큐에서 메시지를 삭제합니다.  
C. Lambda 함수를 구성하여 S3 버킷에서 새로운 업로드를 모니터링합니다. 업로드된 이미지를 감지하면 메모리에 텍스트 파일을 작성하고 처리된 이미지를 추적합니다.  
D. Amazon EC2 인스턴스를 시작하여 Amazon SQS 큐를 모니터링합니다. 큐에 항목이 추가되면 EC2 인스턴스에서 텍스트 파일에 파일 이름을 기록하고 Lambda 함수를 호출합니다.  
E. Amazon EventBridge(Amazon CloudWatch Events) 이벤트를 구성하여 S3 버킷을 모니터링합니다. 이미지가 업로드되면 애플리케이션 소유자의 이메일 주소로 알림을 보내는 Amazon SNS 주제로 전송합니다.

---

An application development team is designing a microservice that will convert large images to smaller, compressed images. When a user uploads an image through the web interface, the microservice should store the image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function, and store the image in its compressed form in a different S3 bucket.  
A solutions architect needs to design a solution that uses durable, stateless components to process the images automatically.  
Which combination of actions will meet these requirements? (Choose two.)

- A. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded to the S3 bucket.
- B. Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfully processed, delete the message in the queue.
- C. Configure the Lambda function to monitor the S3 bucket for new uploads. When an uploaded image is detected, write the file name to a text file in memory and use the text file to keep track of the images that were processed.
- D. Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queue. When items are added to the queue, log the file name in a text file on the EC2 instance and invoke the Lambda function.
- E. Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket. When an image is uploaded, send an alert to an Amazon ample Notification Service (Amazon SNS) topic with the application owner's email address for further processing.
```

정답 : `A, B`
- 내구성(durable): SQS 큐는 메시지를 안전하게 저장, 람다가 실패하더라도 메시지가 손실되지 않음
- 상태 없는 처리(stateless): 람다 함수는 상태를 유지하지 않고 메시지를 받아 이미지 처리 후 완료
- 자동처리 : S3 업로드 시 SQS 알림 -> 람다 자동 트리거 -> 이미지 처리 및 다른 S3 버킷 저장
- 메시지 삭제는 정상 처리 후 큐에서 제거하여 중복 방지

오답 이유
**C - 이유**:
- Lambda가 메모리에 텍스트 파일을 만들어 처리 상태를 추적하면 **상태 유지(stateful)**
- 서버리스 원칙에 맞지 않고 내구성이 낮음

**D - 이유**:
- EC2를 사용하면 **상태가 있는 구성 요소**가 필요하고 관리 부담 증가
- 서버리스, stateless 아키텍처 원칙과 맞지 않음

**E - 이유**:
- SNS는 알림용이며, **이미지 처리나 자동 Lambda 트리거** 목적에는 부적합


## #19
```
한 회사는 AWS에 세 계층 웹 애플리케이션을 배포했습니다.  
- 웹 서버: 퍼블릭 서브넷  
- 애플리케이션 서버와 데이터베이스 서버: 프라이빗 서브넷  
회사는 AWS Marketplace에서 제공되는 타사 가상 방화벽 어플라이언스를 검사 VPC에 배포했습니다.  
어플라이언스는 IP 패킷을 수신할 수 있는 IP 인터페이스로 구성되어 있습니다.  

솔루션 아키텍트는 **웹 서버에 도달하기 전에 모든 트래픽이 어플라이언스를 통해 검사되도록** 웹 애플리케이션을 통합해야 합니다.  

A. 애플리케이션 VPC의 퍼블릭 서브넷에 Network Load Balancer를 생성하여 트래픽을 어플라이언스로 라우팅  
B. 애플리케이션 VPC의 퍼블릭 서브넷에 Application Load Balancer를 생성하여 트래픽을 어플라이언스로 라우팅  
C. Inspection VPC에 Transit Gateway를 배포하고, 라우팅 테이블을 구성하여 들어오는 패킷을 Transit Gateway로 라우팅  
D. Inspection VPC에 Gateway Load Balancer를 배포하고, Gateway Load Balancer 엔드포인트를 생성하여 들어오는 패킷을 어플라이언스로 전달

---

A company has a three-tier web application that is deployed on AWS. The web servers are deployed in a public subnet in a VPC. The application servers and database servers are deployed in private subnets in the same VPC. The company has deployed a third-party virtual firewall appliance from AWS Marketplace in an inspection VPC. The appliance is configured with an IP interface that can accept IP packets.  
A solutions architect needs to integrate the web application with the appliance to inspect all traffic to the application before the traffic reaches the web server.  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.
- B. Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.
- C. Deploy a transit gateway in the inspection VPConfigure route tables to route the incoming packets through the transit gateway.
- D. Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance.
```

정답 : `D`
- Gateway Load Balancer(GWLB)는 네트워크 패킷을 가상 어플라이언스로 투명하게 전달할 수 있는 AWS 서비스
- 타사 방화벽 어플라이언스와 통합 시 최소한의 운영 부담으로 트래픽 검사 가능
- 엔드포인트와 함께 사용하면 기존 라우팅을 크게 변경하지 않고도 트래픽을 어플라이언스로 전달 가능
- 상태 유지(stateful) 패킷 검사, 확장성, 고가용성 지원

오답 이유
**A - 이유**:
- Network Load Balancer는 **L4 TCP/UDP 로드 밸런싱**에 적합하지만, 패킷을 투명하게 어플라이언스로 전달하는 기능은 부족
- 방화벽 통합 시 GWLB만큼 간편하지 않음

**B - 이유**:
- Application Load Balancer는 **L7 HTTP/HTTPS 로드 밸런싱**
- IP 패킷 단위 검사에는 적합하지 않음

**C - 이유**:
- Transit Gateway는 VPC 간 라우팅에 적합
- 방화벽 검사 기능 자체는 제공하지 않고, 직접 라우팅/운영 관리 부담이 큼

## #20
```
한 회사는 동일한 AWS 리전 내 테스트 환경으로 **대량의 프로덕션 데이터를 복제(clone)** 하고자 합니다.  
데이터는 **Amazon EC2 인스턴스의 Amazon EBS 볼륨**에 저장되어 있습니다.  
- 복제된 데이터의 수정은 프로덕션 환경에 영향을 주지 않아야 함  
- 데이터를 접근하는 소프트웨어는 **일관되게 높은 I/O 성능** 필요  

솔루션 아키텍트는 **프로덕션 데이터를 테스트 환경으로 복제하는 데 소요되는 시간을 최소화**해야 합니다.  

A. 프로덕션 EBS 볼륨의 스냅샷을 생성하고, 테스트 환경의 EC2 인스턴스에 **인스턴스 스토어**로 복원  
B. 프로덕션 EBS 볼륨에 **EBS Multi-Attach**를 구성. 스냅샷 생성 후, 테스트 환경 EC2에 프로덕션 EBS 볼륨을 바로 연결  
C. 프로덕션 EBS 볼륨의 스냅샷을 생성하고, 새 EBS 볼륨을 만들고 초기화 후 테스트 환경 EC2에 연결 → 스냅샷 복원  
D. 프로덕션 EBS 볼륨의 스냅샷을 생성 후 **EBS Fast Snapshot Restore(FSR)** 활성화 → 새 EBS 볼륨으로 복원 후 테스트 환경 EC2에 연결

---

A company wants to improve its ability to clone large amounts of production data into a test environment in the same AWS Region. The data is stored in Amazon EC2 instances on Amazon Elastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the production environment. The software that accesses this data requires consistently high I/O performance.  
A solutions architect needs to minimize the time that is required to clone the production data into the test environment.  
Which solution will meet these requirements?

- A. Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes in the test environment.
- B. Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of the production EBS volumes. Attach the production EBS volumes to the EC2 instances in the test environment.
- C. Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots.
- D. Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment.
```

정답 : `D`
- EBS FSR(Fast Snapshot Restore)는 스탭샷을 볼륨으로 복원할 때 즉시 고성능 볼륨 제공
	- 일반 스냅샷 복원 시 초기 I/O는 느리지만 FSR 사용 시 첫 번째 I/O부터 프로덕션 수준의 성능 보장
		- 일반 스냅샷은 S3에 증분 백업 형태로 저장됨 -> 새 EBS 볼륨을 스냅샷에서 복원할 때는 아직 실제 EBS 볼륨 블록에 로드되지 않은 상태
		- AWS 복원된 블록을 Lazy Loading 방식으로 처리
			- 처음 접근하는 블록에 대해서만 S3에서 EBS로 로드하기 때문에 초기 I/O 성능이 느림
		- FSR을 활성화하면 스냅샷의 모든 블록을 미리 EBS 볼륨에 로드 -> 첫 번째 I/O부터 EBS 본래 성능 제공
- 테스트 볼륨이 프로덕션과 독립 -> 수정해도 프로덕션 영향 없음
- 대량 데이터 복제 시간 최소화 -> FSR로 초기화 없이 바로 사용 가능

오답 이유

**A - 이유**:
- EC2 인스턴스 스토어는 **임시 스토리지**, 인스턴스 종료 시 데이터 손실
- 테스트 환경에서 독립적 데이터 유지 불가, I/O 성능 보장 어려움

**B - 이유**:
- EBS Multi-Attach는 **하나의 볼륨을 여러 EC2에서 공유**
- 테스트 환경에서 수정하면 **프로덕션 데이터에 영향** → 요구사항 불충족

**C - 이유**:
- 새 EBS 볼륨 생성 후 스냅샷 복원 → **초기 I/O가 느림** (성능 저하)
- 대량 데이터 복제 시간 최소화 목적 미달

## #21
```
한 전자상거래 회사는 AWS에서 **하루 한 제품만 판매하는 사이트**를 운영하려고 합니다.  
- 매일 24시간 동안 정확히 한 제품 판매  
- 최대 시간대에는 시간당 **수백만 건의 요청** 처리 필요  
- 요청 지연(latency)은 **밀리초 단위**여야 함  
- **운영 부담 최소화** 원함  

A. Amazon S3에 전체 웹사이트를 여러 버킷으로 호스팅. CloudFront 배포 추가. S3를 오리진으로 설정. 주문 데이터도 S3에 저장  
B. Amazon EC2 + Auto Scaling (다중 AZ). ALB로 웹 트래픽 분산. 백엔드 API용 ALB 추가. 데이터는 RDS MySQL  
C. 애플리케이션을 컨테이너로 마이그레이션. Amazon EKS에서 컨테이너 호스팅. Kubernetes Cluster Autoscaler로 트래픽 변동 처리. 데이터는 RDS MySQL  
D. S3 버킷에 웹사이트 정적 콘텐츠 호스팅. CloudFront 배포. S3를 오리진으로 설정. API Gateway + Lambda로 백엔드 API 구현. 데이터는 DynamoDB

---

An ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will feature exactly one product on sale for a period of 24 hours. The company wants to be able to handle millions of requests each hour with millisecond latency during peak hours.

Which solution will meet these requirements with the LEAST operational overhead?

  

A. Use Amazon S3 to host the full website in different S3 buckets. Add Amazon CloudFront distributions. Set the S3 buckets as origins for the distributions. Store the order data in Amazon S3.

B. Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across multiple Availability Zones. Add an Application Load Balancer (ALB) to distribute the website traffic. Add another ALB for the backend APIs. Store the data in Amazon RDS for MySQL.

C. Migrate the full application to run in containers. Host the containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use the Kubernetes Cluster Autoscaler to increase and decrease the number of pods to process bursts in traffic. Store the data in Amazon RDS for MySQL.

D. Use an Amazon S3 bucket to host the website's static content. Deploy an Amazon CloudFront distribution. Set the S3 bucket as the origin. Use Amazon API Gateway and AWS Lambda functions for the backend APIs. Store the data in Amazon DynamoDB.
```

정답 : `D`
- 수백만 건의 요청 처리와 밀리초 지연 보장 : CloudFront + Lambda + DynamoDB는 서버리스 구조로 자동 확장, DynamoDB는 밀리초 단위 읽기 쓰기 성능 제공
- 운영 부담 최소화 : 서버리스 구성으로 EC2, EKS, 클러스터, 오토스케일ㄹ링, 패치, OS 관리 등이 불필요
- 비용 효율성 : 트래픽 급증 시에도 서버 미리 프로비저닝할 필요 없음, 사용한 만큼만 지불

오답 이유

**A - 이유**
- S3 + CloudFront는 정적 콘텐츠에 적합
- 하지만 주문 처리 같은 **동적 API 기능 제공 불가** → 데이터 저장/처리 어려움

**B - 이유**
- EC2 + ALB + RDS는 전통적인 웹 아키텍처
- 수백만 요청 처리 시 **Auto Scaling, DB 스케일링, 운영 부담** 많음

**C - 이유**
- EKS + RDS는 컨테이너 기반 확장 가능
- 하지만 **클러스터/노드 관리, 스케일링 정책, DB 운영 부담** 큼
- 서버리스 대비 운영 부담 높음

## #22
```
한 솔루션 아키텍트가 새로운 디지털 미디어 애플리케이션을 위한 **스토리지 아키텍처**를 Amazon S3로 설계하려고 합니다.  
- 미디어 파일은 **가용 영역(Availability Zone) 손실에도 내구성을 유지**해야 함  
- 일부 파일은 자주 접근, 일부는 드물게 접근 (예측 불가)  
- **저장 비용과 접근 비용**을 최소화해야 함  

A. S3 Standard  
B. S3 Intelligent-Tiering  
C. S3 Standard-Infrequent Access (S3 Standard-IA)  
D. S3 One Zone-Infrequent Access (S3 One Zone-IA)

---

A solutions architect is using Amazon S3 to design the storage architecture of a new digital media application. The media files must be resilient to the loss of an Availability Zone. Some files are accessed frequently while other files are rarely accessed in an unpredictable pattern. The solutions architect must minimize the costs of storing and retrieving the media files.  
Which storage option meets these requirements?

- A. S3 Standard
- B. S3 Intelligent-Tiering
- C. S3 Standard-Infrequent Access (S3 Standard-IA)
- D. S3 One Zone-Infrequent Access (S3 One Zone-IA)
```

정답 : `B`
- 자동 계층 전환
	- 자주 접근되는 객체는 Standard 계층
	- 드물게 접근되는 객체는 Infrequent Access 계층으로 자동 이동
	- 접근 패턴이 예측 불가할 때 최적 비용 유지
- 가용 영역 내구성
	- Intelligent Tiering은 다중 AZ에 데이터 복제 -> AZ 장애에도 데이터 안전
- 비용 최소화
	- 접근 패턴에 따라 자동으로 비용 효율적 계층 사용
	- 관리자가 직접 접근 패턴 예측하거나 수동 전환할 필요 없음

오답 이유

**A - S3 Standard**
- 장점: 다중 AZ 내구성, 고속 접근
- 단점: **드물게 접근하는 데이터에도 높은 저장 비용** → 비용 최적화 불가

**C - S3 Standard-IA**
- 장점: 다중 AZ 내구성, 낮은 저장 비용
- 단점: **접근 비용 높음**, 접근 패턴 예측 불가 시 비용 효율 낮음

**D - S3 One Zone-IA**
- 장점: 저장 비용 가장 낮음
- 단점: **한 AZ만 저장** → AZ 장애 발생 시 데이터 손실 가능
- 요구 사항(“AZ 손실에도 내구성”) 불충족

## #23
```
한 회사가 Amazon S3 Standard 스토리지를 사용하여 백업 파일을 저장하고 있습니다.  
파일은 1개월 동안 자주 접근됩니다. 그러나 파일은 1개월 이후에는 접근되지 않습니다.  
회사는 파일을 무기한 보관해야 합니다.  
어떤 스토리지 솔루션이 이러한 요구사항을 가장 비용 효율적으로 충족할 수 있습니까?

A. S3 Intelligent-Tiering을 구성하여 객체를 자동으로 이동하도록 설정합니다.  
B. S3 Lifecycle 구성을 생성하여 객체를 1개월 후 S3 Glacier Deep Archive로 전환합니다.  
C. S3 Lifecycle 구성을 생성하여 객체를 1개월 후 S3 Standard-Infrequent Access(S3 Standard-IA)로 전환합니다.  
D. S3 Lifecycle 구성을 생성하여 객체를 1개월 후 S3 One Zone-Infrequent Access(S3 One Zone-IA)로 전환합니다.

---

A company is storing backup files by using Amazon S3 Standard storage. The files are accessed frequently for 1 month. However, the files are not accessed after 1 month. The company must keep the files indefinitely.  
Which storage solution will meet these requirements MOST cost-effectively?

- A. Configure S3 Intelligent-Tiering to automatically migrate objects.
- B. Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month.
- C. Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 1 month.
- D. Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 month.
```

정답 : `B`
- 접근 패턴 기반 비용 최적화
	- 처음 1개월은 자주 접근 -> S3 Standard 유지
	- 이후 접근 없음 -> 저비용 장기 보관 스토리지인 Glacier Deep Archive로 이동
- 무기한 보관 가능
	- Glacier Deep Archive는 장기 보관용 최저 비용 스토리지
	- 데이터 복원 시 몇 시간 -> 자주 접근하지 않는 백업에 적합
- Lifecycle 정책 활용
	- 자동으로 스토리지 전환으로 운영 부담을 최소화

오답 이유

**A - S3 Intelligent-Tiering**
- 자동 계층 이동 가능하지만, **최저 비용 스토리지(Glacier Deep Archive)까지 이동하지는 않음**
- 접근 패턴 예측 가능하면 비용 절감 효과 제한적

**C - S3 Standard-IA**
- 낮은 접근 빈도의 스토리지지만 Glacier 대비 **저장 비용 높음**
- 장기 보관 비용 최적화 불리

**D - S3 One Zone-IA**
- 한 AZ만 저장 → AZ 장애 시 데이터 손실 가능
- 요구 사항(무기한 안전 보관) 불충족


## #24
```
한 회사가 최근 청구서에서 Amazon EC2 비용 증가를 관찰했습니다.  
청구팀은 몇 개의 EC2 인스턴스에서 **원치 않는 수직 확장(Vertical Scaling)**이 발생했음을 확인했습니다.  
솔루션 아키텍트는 지난 2개월 동안의 EC2 비용을 비교하는 그래프를 만들고, **수직 확장의 근본 원인(root cause)**을 심층 분석해야 합니다.  
솔루션 아키텍트가 **운영 부담을 최소화**하면서 이 정보를 생성하려면 어떻게 해야 합니까?

A. AWS Budgets를 사용하여 예산 보고서를 생성하고 인스턴스 유형별 EC2 비용을 비교합니다.  
B. Cost Explorer의 세부 필터링 기능을 사용하여 인스턴스 유형별 EC2 비용을 심층 분석합니다.  
C. AWS Billing and Cost Management 대시보드의 그래프를 사용하여 지난 2개월 동안 인스턴스 유형별 EC2 비용을 비교합니다.  
D. AWS Cost and Usage Reports를 사용하여 보고서를 생성하고 Amazon S3 버킷으로 전송합니다. S3를 소스로 Amazon QuickSight를 사용하여 인스턴스 유형별 인터랙티브 그래프를 생성합니다.

---

A company observes an increase in Amazon EC2 costs in its most recent bill. The billing team notices unwanted vertical scaling of instance types for a couple of EC2 instances. A solutions architect needs to create a graph comparing the last 2 months of EC2 costs and perform an in-depth analysis to identify the root cause of the vertical scaling.  
How should the solutions architect generate the information with the LEAST operational overhead?

- A. Use AWS Budgets to create a budget report and compare EC2 costs based on instance types.
- B. Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs based on instance types.
- C. Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on instance types for the last 2 months.
- D. Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket. Use Amazon QuickSight with Amazon S3 as a source to generate an interactive graph based on instance types.
```

정답 : `B`
- **AWS Cost Explorer**: AWS 비용 및 사용량 분석 도구. 세부 필터링, 그래프, 시간별 비교 가능
- Cost Explorer는 웹 UI에서 즉시 분석 가능, 추가 구성 불필요 -> 운영 부담 최소화
- 인스턴스 유형, AZ, 태그 등 다양한 기준으로 비용 세분화 가능, 수직 확장 원인 분석에 적합 -> 세부 필터링 가능
- 지난 2개월 비용 데이터를 그래프 형태로 바로 시각화 가능 -> 그래프 비교 용이

오답 이유

**A - AWS Budgets**
- **AWS Budgets**: 비용 및 사용량 목표 설정, 초과 시 알림
- 예산 초과 여부 모니터링 용도
- 세부 비용 분석 기능 제한 → 근본 원인 분석에는 부적합


**C - Billing & Cost Management 대시보드**
- **Billing and Cost Management Dashboard**: 기본 비용 그래프 및 청구 정보 제공, 세부 분석 기능 제한
- 단순 비교 그래프 제공, 필터링 기능 제한
- 인스턴스 유형별 심층 분석 어려움

**D - Cost and Usage Report + QuickSight**
- **AWS Cost and Usage Reports (CUR)**: 상세 사용량 및 비용 데이터 제공, 분석 위해 S3/QuickSight와 연계 가능
- 강력하지만 구성 및 운영 부담 큼
- **가장 최소 운영 부담** 요구 조건과 맞지 않음


## #25
```
한 회사가 애플리케이션을 설계하고 있습니다.  
애플리케이션은 AWS Lambda 함수를 사용하여 Amazon API Gateway를 통해 정보를 받고, Amazon Aurora PostgreSQL 데이터베이스에 정보를 저장합니다.  

POC(개념 검증) 단계에서, 회사는 데이터베이스로 로드해야 하는 **대량 데이터 처리**를 위해 Lambda 할당량(quotas)을 크게 증가시켜야 했습니다.  
솔루션 아키텍트는 **확장성을 개선하고 구성 작업을 최소화**할 새로운 설계를 추천해야 합니다.  

A. Lambda 함수 코드를 Apache Tomcat 코드로 리팩터링하여 Amazon EC2 인스턴스에서 실행합니다. 데이터베이스는 JDBC 드라이버를 사용하여 연결합니다.  
B. Aurora에서 Amazon DynamoDB로 플랫폼을 변경합니다. DynamoDB Accelerator(DAX) 클러스터를 프로비저닝하고, DAX 클라이언트 SDK를 사용하여 기존 DynamoDB API 호출을 DAX 클러스터로 향하게 합니다.  
C. 두 개의 Lambda 함수를 설정합니다. 하나는 정보를 수신하고, 다른 하나는 데이터를 데이터베이스에 로드하도록 구성합니다. Lambda 함수들은 Amazon SNS를 사용하여 통합합니다.  
D. 두 개의 Lambda 함수를 설정합니다. 하나는 정보를 수신하고, 다른 하나는 데이터를 데이터베이스에 로드하도록 구성합니다. Lambda 함수들은 Amazon SQS 큐를 사용하여 통합합니다.

---

A company is designing an application. The application uses an AWS Lambda function to receive information through Amazon API Gateway and to store the information in an Amazon Aurora PostgreSQL database.  
During the proof-of-concept stage, the company has to increase the Lambda quotas significantly to handle the high volumes of data that the company needs to load into the database. A solutions architect must recommend a new design to improve scalability and minimize the configuration effort.  
Which solution will meet these requirements?

- A. Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances. Connect the database by using native Java Database Connectivity (JDBC) drivers.
- B. Change the platform from Aurora to Amazon DynamoDProvision a DynamoDB Accelerator (DAX) cluster. Use the DAX client SDK to point the existing DynamoDB API calls at the DAX cluster.
- C. Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using Amazon Simple Notification Service (Amazon SNS).
- D. Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using an Amazon Simple Queue Service (Amazon SQS) queue.
```

정답 : `D`
- 스케일링 문제 해결
	- 람다 단일 함수에서 직접 오로라에 데이터를 로드하면 동시 호출 제한과 데이터베이스 연결 제한에 걸림
	- SQS 큐를 사용하면 람다가 메시지를 비동기적으로 처리해 데이터베이스 과부하 방지
	- **SQS 메시지 수를 기준으로 처리 속도 조절**
		Lambda가 SQS를 이벤트 소스로 사용하면 다음과 같은 방식으로 처리됩니다.
		1. **SQS 큐에 메시지가 들어감**
		    - API Gateway → Lambda(수신) → SQS
		    - 메시지 = 처리할 데이터 하나
		2. **Lambda(SQS 처리용) 자동 호출**
		    - AWS가 SQS 메시지를 **배치 단위로 Lambda에 전달**
		    - 배치 크기(batch size)를 설정할 수 있음 → 한 번에 몇 개 메시지를 처리할지 조절		    
		3. **동시 처리 수 제어**
		    - Lambda는 **최대 동시 실행 수**(concurrent executions) 설정 가능
		    - 예: 동시 실행 수를 10으로 제한 → 동시에 Aurora에 연결되는 Lambda 인스턴스도 최대 10개
		4. **결과**
		    - 메시지가 너무 많아도 Lambda 동시 실행 제한 + 배치 단위 조절로 **DB 과부하 방지**
		    - 큐에 쌓인 메시지는 Lambda가 여유가 생기면 순서대로 처리 → 안정적인 데이터베이스 연결
- 운영 및 구성 부담 최소화
	- EC2 또는 DynamoDB로 변경 불필요 -> 기존 아키텍처 유지
	- 큐 기반 설계 -> 높은 데이터량 처리 가능, 확장성 확보
- 람다 함수 분리
	- 수신 함수 : API Gateway 이벤트 수신 -> SQS 메시지 작성
	- 처리 함수 : SQS 메시지 처리 -> 오로라 로드
	- 각 함수는 독립적, 오류 재시도, 재처리 가능

오답 이유

**A - EC2 + Tomcat**
- 서버 관리 부담이 크게 증가
- 확장성 확보 위해 서버 수 관리 필요 → 구성 부담 큼

**B - Aurora → DynamoDB + DAX**
- 데이터베이스 변경 필요 → POC 기반 기존 설계 파괴
- 관계형 트랜잭션 요구사항(Aurora PostgreSQL) 만족 불가
  
**C - Lambda + SNS**
- SNS는 **푸시 기반**으로 메시지를 전송, 순서 보장 및 메시지 재처리 관리가 어려움
- 고용량 데이터 처리에 적합하지 않음


## #26
```
한 회사가 AWS Cloud 배포를 검토하여 Amazon S3 버킷에 **승인되지 않은 구성 변경**이 없는지 확인해야 합니다.  
솔루션 아키텍트는 이 목표를 달성하기 위해 무엇을 해야 합니까?

A. 적절한 규칙을 사용하여 AWS Config를 활성화합니다.  
B. 적절한 검사를 사용하여 AWS Trusted Advisor를 활성화합니다.  
C. 적절한 평가 템플릿을 사용하여 Amazon Inspector를 활성화합니다.  
D. Amazon S3 서버 액세스 로깅을 활성화하고, Amazon EventBridge(CloudWatch Events)를 구성합니다.

---

A company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do not have unauthorized configuration changes.  
What should a solutions architect do to accomplish this goal?

- A. Turn on AWS Config with the appropriate rules.
- B. Turn on AWS Trusted Advisor with the appropriate checks.
- C. Turn on Amazon Inspector with the appropriate assessment template.
- D. Turn on Amazon S3 server access logging. Configure Amazon EventBridge (Amazon Cloud Watch Events).
```

정답 : `A`
- AWS Config
	- AWS 리소스 구성 변경을 자동으로 기록 및 평가
	- S3 버킷 정책, 버킷 ACL, 퍼블릭 접근 설정 등 구성 변경 감지 가능
	- 사전 정의된 규칙을 사용해 승인되지 않은 변경 자동 감지
- 자동 감사 및 알림
	- 구성 변경 시 알림 전송 가능
	- 규정 준수 상태를 추적하고 보고서 생성 가능

오답 이유

**B - AWS Trusted Advisor**
- 비용 최적화, 보안 권장사항, 성능, 내구성 점검    
- 실시간 구성 변경 감지 및 감사 기능 **제한적** → S3 변경 모니터링 목적에는 부적합

**C - Amazon Inspector**
- EC2, 컨테이너 등 취약점 및 보안 평가 도구    
- S3 버킷 구성 변경 감지에는 **사용되지 않음**

**D - S3 서버 액세스 로깅 + EventBridge**
- 접근 로그 기록 가능 → 누가 객체에 접근했는지 추적
- **구성 변경** 감지는 자동으로 수행되지 않음 → 추가 분석 필요

## #27
```
한 회사가 새로운 애플리케이션을 출시하며, 애플리케이션 지표를 Amazon CloudWatch 대시보드에 표시하려고 합니다.  
제품 관리자는 주기적으로 이 대시보드에 접근해야 합니다. 제품 관리자는 AWS 계정을 가지고 있지 않습니다.  
솔루션 아키텍트는 최소 권한 원칙(Principle of Least Privilege)을 준수하면서 제품 관리자에게 접근 권한을 제공해야 합니다.  

A. CloudWatch 콘솔에서 대시보드를 공유합니다. 제품 관리자의 이메일 주소를 입력하고 공유 단계를 완료합니다. 공유 가능한 링크를 제품 관리자에게 제공합니다.  
B. 제품 관리자를 위해 IAM 사용자를 생성합니다. 해당 사용자에게 CloudWatchReadOnlyAccess AWS 관리형 정책을 연결합니다. 새 로그인 자격 증명을 제품 관리자와 공유합니다. 올바른 대시보드의 브라우저 URL도 공유합니다.  
C. 회사 직원용 IAM 사용자를 생성합니다. 해당 사용자에게 ViewOnlyAccess AWS 관리형 정책을 연결합니다. 새 로그인 자격 증명을 제품 관리자와 공유합니다. 제품 관리자에게 CloudWatch 콘솔에 접속하여 Dashboards 섹션에서 대시보드를 이름으로 찾도록 안내합니다.  
D. 퍼블릭 서브넷에 Bastion 서버를 배포합니다. 제품 관리자가 대시보드에 접근할 필요가 있을 때 서버를 시작하고 RDP 자격 증명을 공유합니다. Bastion 서버에서 브라우저가 적절한 권한을 가진 캐시된 AWS 자격 증명으로 대시보드 URL을 열도록 구성합니다.

---

A company is launching a new application and will display application metrics on an Amazon CloudWatch dashboard. The company's product manager needs to access this dashboard periodically. The product manager does not have an AWS account. A solutions architect must provide access to the product manager by following the principle of least privilege.  
Which solution will meet these requirements?

- A. Share the dashboard from the CloudWatch console. Enter the product manager's email address, and complete the sharing steps. Provide a shareable link for the dashboard to the product manager.
- B. Create an IAM user specifically for the product manager. Attach the CloudWatchReadOnlyAccess AWS managed policy to the user. Share the new login credentials with the product manager. Share the browser URL of the correct dashboard with the product manager.
- C. Create an IAM user for the company's employees. Attach the ViewOnlyAccess AWS managed policy to the IAM user. Share the new login credentials with the product manager. Ask the product manager to navigate to the CloudWatch console and locate the dashboard by name in the Dashboards section.
- D. Deploy a bastion server in a public subnet. When the product manager requires access to the dashboard, start the server and share the RDP credentials. On the bastion server, ensure that the browser is configured to open the dashboard URL with cached AWS credentials that have appropriate permissions to view the dashboard.
```

정답 : `A`
- 제품 관리자는 AWS 계정이 없기 때문에 IAM 사용자 생성(B,C)는 적합하지 않음
- CloudWatch 대시보드 공유 기능은 읽기 전용 링크 제공
- 링크를 통해 제품 관리자는 대시보드 보기만 가능 -> 추가 권한 불필요
- 별도 IAM 계정, Bastion 서버, RDP 세션 설정 불필요

오답 이유

**B - IAM 사용자 생성 + CloudWatchReadOnlyAccess**
- AWS 계정이 없는 제품 관리자는 로그인 불가 → 요구사항 불충족

**C - ViewOnlyAccess IAM 사용자 공유**
- 마찬가지로 제품 관리자가 AWS 계정이 없으므로 접근 불가

**D - [[용어 모음#**Bastion 서버**|Bastion 서버]] + [[용어 모음#**RDP (Remote Desktop Protocol)**|RDP]]**
- 지나치게 복잡하고 운영 부담 큼
- 최소 권한 원칙 위반 가능 (서버 접속, 브라우저 사용 등)

## #28
```
한 회사가 애플리케이션을 AWS로 마이그레이션하고 있습니다. 애플리케이션은 서로 다른 AWS 계정에 배포되어 있습니다.  
회사는 AWS Organizations를 사용하여 계정을 중앙에서 관리합니다.  
회사의 보안 팀은 모든 AWS 계정에서 단일 로그인(SSO) 솔루션이 필요합니다.  
회사는 온프레미스에서 자체 관리되는 Microsoft Active Directory에서 사용자와 그룹을 계속 관리해야 합니다.  
어떤 솔루션이 이러한 요구사항을 충족할 수 있을까요?

A. AWS SSO 콘솔에서 AWS Single Sign-On(AWS SSO)을 활성화합니다. AWS Directory Service for Microsoft Active Directory를 사용하여 회사의 자체 관리 Microsoft Active Directory와 AWS SSO를 연결하기 위해 단방향 포리스트 신뢰(one-way forest trust) 또는 단방향 도메인 신뢰(one-way domain trust)를 생성합니다.

B. AWS SSO 콘솔에서 AWS Single Sign-On(AWS SSO)을 활성화합니다. AWS Directory Service for Microsoft Active Directory를 사용하여 회사의 자체 관리 Microsoft Active Directory와 AWS SSO를 연결하기 위해 양방향 포리스트 신뢰(two-way forest trust)를 생성합니다.

C. AWS Directory Service를 사용합니다. 회사의 자체 관리 Microsoft Active Directory와 양방향 신뢰(two-way trust) 관계를 생성합니다.

D. 온프레미스에 아이덴티티 제공자(IdP)를 배포합니다. AWS SSO 콘솔에서 AWS Single Sign-On(AWS SSO)을 활성화합니다.

---

A company is migrating applications to AWS. The applications are deployed in different accounts. The company manages the accounts centrally by using AWS Organizations. The company's security team needs a single sign-on (SSO) solution across all the company's accounts. The company must continue managing the users and groups in its on-premises self-managed Microsoft Active Directory.  
Which solution will meet these requirements?

- A. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a one-way forest trust or a one-way domain trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.
- B. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a two-way forest trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.
- C. Use AWS Directory Service. Create a two-way trust relationship with the company's self-managed Microsoft Active Directory.
- D. Deploy an identity provider (IdP) on premises. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console.
```

정답 : `A. AWS SSO + 단방향 포리스트/도메인 신뢰`

- **AWS [[용어 모음#**SSO (Single Sign-On)**|SSO]]와 온프레미스 AD 통합 가능**
	- AWS SSO는 AWS Organizations 내 여러 계정에서 단일 로그인(SSO)을 제공
    - 온프레미스 AD 사용자를 계속 관리 가능
- **AWS Directory Service for Microsoft AD 사용**
    - AWS SSO와 연결 가능
    - 단방향 신뢰(one-way trust) 설정 → 온프레미스 AD 사용자로 AWS SSO 로그인 가능
    - 복잡성과 보안 위험 최소화
- **운영 및 보안 효율성**
    - 사용자와 그룹 관리는 온프레미스 AD에서 중앙화
    - 모든 계정에 대한 단일 로그인 제공

오답 이유

**B - 양방향 포리스트 신뢰**
- AWS SSO 관점에서 필요 없는 복잡성 추가
- 최소 권한 원칙에 맞지 않음

**C - AWS Directory Service만 사용 + 양방향 신뢰**
- 단일 로그인(SSO) 기능 제공 불가
- 요구사항 불충족

**D - 온프레미스 IdP 배포 + AWS SSO**
- 가능은 하지만, 관리 부담 증가
- AWS SSO와 직접 연동하는 것보다 운영 효율성 떨어짐


## #29
```
한 회사가 UDP 연결을 사용하는 VoIP(Voice over Internet Protocol) 서비스를 제공합니다.  
이 서비스는 Auto Scaling 그룹에서 실행되는 Amazon EC2 인스턴스로 구성됩니다.  
회사는 여러 AWS 리전에 배포를 하고 있습니다.  

회사는 사용자를 **지연(latency)이 가장 낮은 리전으로 라우팅**해야 합니다.  
또한, 리전 간 자동 장애 조치(failover)가 필요합니다.  
어떤 솔루션이 이러한 요구사항을 충족할 수 있을까요?

A. Network Load Balancer(NLB)와 연관된 타겟 그룹을 배포합니다. 타겟 그룹을 Auto Scaling 그룹과 연결합니다. 각 리전에서 NLB를 AWS Global Accelerator 엔드포인트로 사용합니다.

B. Application Load Balancer(ALB)와 연관된 타겟 그룹을 배포합니다. 타겟 그룹을 Auto Scaling 그룹과 연결합니다. 각 리전에서 ALB를 AWS Global Accelerator 엔드포인트로 사용합니다.

C. Network Load Balancer(NLB)와 연관된 타겟 그룹을 배포합니다. 타겟 그룹을 Auto Scaling 그룹과 연결합니다. Amazon Route 53 레이턴시 레코드를 생성하여 각 NLB의 앨리어스를 가리키게 합니다. Amazon CloudFront 배포를 생성하고 레이턴시 레코드를 오리진으로 사용합니다.

D. Application Load Balancer(ALB)와 연관된 타겟 그룹을 배포합니다. 타겟 그룹을 Auto Scaling 그룹과 연결합니다. Amazon Route 53 가중치 레코드를 생성하여 각 ALB의 앨리어스를 가리키게 합니다. Amazon CloudFront 배포를 생성하고 가중치 레코드를 오리진으로 사용합니다.

---

A company provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. The service consists of Amazon EC2 instances that run in an Auto Scaling group. The company has deployments across multiple AWS Regions.  
The company needs to route users to the Region with the lowest latency. The company also needs automated failover between Regions.  
Which solution will meet these requirements?

- A. Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Use the NLB as an AWS Global Accelerator endpoint in each Region.
- B. Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Use the ALB as an AWS Global Accelerator endpoint in each Region.
- C. Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53 latency record that points to aliases for each NLB. Create an Amazon CloudFront distribution that uses the latency record as an origin.
- D. Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53 weighted record that points to aliases for each ALB. Deploy an Amazon CloudFront distribution that uses the weighted record as an origin.
```

정답 : `A. Network Load Balancer + AWS Global Accelerator`

- UDP 지원
	- [[용어 모음#**VoIP(Voice over Internet Protocol)**|VoIP]] 서비스는 UDP 프로토콜 사용
	- ALB는 HTTP/HTTPS 전용 -> UDP 지원 X
	- NLB는 TCP 및 UDP 지원
- 글로벌 지연 시간 기반 라우팅 + 자동 장애 조치
	- [[AWS Services#AWS Global Accelerator|AWS Global Accelerator]] 사용 -> 사용자 트래픽을 가장 가까운 리전으로 라우팅
	- 특정 리전 장애 시 자동 장애 조치 제공
- 오토 스케일링 그룹과 통합 용이
	- NLB 타겟 그룹과 연결해 인스턴스 수에 따라 자동 확장 가능

오답 이유

**B - ALB 사용**
- UDP를 지원하지 않음 → VoIP 서비스 요구사항 불충족

**C - Route 53 레이턴시 + CloudFront**
- CloudFront는 UDP를 지원하지 않음 → VoIP 트래픽 처리 불가
- NLB는 적합하지만 CloudFront를 통해 전달하면 UDP 통신 불가능

**D - ALB + Route 53 가중치 + CloudFront**
- ALB는 UDP 미지원, CloudFront도 UDP 미지원 → VoIP 트래픽 처리 불가


## #30
```
한 개발팀이 매월 48시간 동안 리소스를 많이 사용하는 테스트를 일반 목적 Amazon RDS for MySQL DB 인스턴스에서 실행하며, Performance Insights를 활성화합니다. 이 테스트는 매월 한 번 실행되며 데이터베이스를 사용하는 유일한 프로세스입니다. 팀은 DB 인스턴스의 컴퓨팅 및 메모리 사양을 줄이지 않고 테스트 실행 비용을 절감하고자 합니다.
어떤 솔루션이 가장 비용 효율적으로 이 요구사항을 충족할 수 있습니까?

A. 테스트가 완료되면 DB 인스턴스를 중지하고, 필요할 때 DB 인스턴스를 다시 시작합니다.  
B. DB 인스턴스에 Auto Scaling 정책을 적용하여 테스트가 완료되면 자동으로 확장하도록 합니다.  
C. 테스트가 완료되면 스냅샷을 생성하고 DB 인스턴스를 종료한 다음, 필요할 때 스냅샷에서 복원합니다.  
D. 테스트가 완료되면 DB 인스턴스를 낮은 용량으로 수정하고, 필요할 때 다시 수정합니다.

---

A development team runs monthly resource-intensive tests on its general purpose Amazon RDS for MySQL DB instance with Performance Insights enabled. The testing lasts for 48 hours once a month and is the only process that uses the database. The team wants to reduce the cost of running the tests without reducing the compute and memory attributes of the DB instance.  
Which solution meets these requirements MOST cost-effectively?

- A. Stop the DB instance when tests are completed. Restart the DB instance when required.
- B. Use an Auto Scaling policy with the DB instance to automatically scale when tests are completed.
- C. Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot when required.
- D. Modify the DB instance to a low-capacity instance when tests are completed. Modify the DB instance again when required.
```

정답 : `A`

- RDS는 Start/Stop 기능을 제공 -> 인스턴스를 중지하면 컴퓨팅 비용만 중지되고, 스토리지와 백업은 유지
- 테스트가 한 달에 한 번이고 장시간만 필요하므로, 테스트가 끝난 후 인스턴스 중지로 비용 절감
- 인스턴스 사양은 그대로 유지 -> 테스트 성능을 떨어뜨리지 않음

오답 이유

**B - Auto Scaling**
- RDS는 EC2처럼 CPU 기반 자동 스케일링을 지원하지 않음.
- 테스트 후 비용 절감에는 적합하지 않음.

**C - 스냅샷 후 종료/복원**
- 복원 과정이 시간이 오래 걸리며, 운영 효율성이 떨어짐.    
- 테스트에 필요한 인스턴스 사양을 유지하는데 비효율적임.

**D - 인스턴스 용량 변경**
- 수동 조정 필요, 변경 과정에서 테스트 지연 가능.
- 비용 절감 효과가 Stop/Start보다 낮음.