---
created: 2025-09-22 15:39:45
last_modified: 2025-09-25 07:38:50
---
## #1
```
한 회사가 여러 대륙의 도시에서 온도, 습도, 기압 데이터를 수집하고 있습니다.  
각 사이트에서 하루 평균 수집하는 데이터 양은 500GB입니다. 각 사이트는 고속 인터넷 연결을 보유하고 있습니다.  

회사는 이러한 글로벌 사이트의 데이터를 **가능한 한 빨리 단일 Amazon S3 버킷으로 집계**하고자 합니다.  
솔루션은 **운영 복잡성을 최소화**해야 합니다.  

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. 대상 S3 버킷에서 **S3 Transfer Acceleration**을 활성화합니다. 멀티파트 업로드를 사용하여 각 사이트에서 대상 S3 버킷으로 직접 데이터를 업로드합니다.  

B. 각 사이트에서 데이터를 가장 가까운 리전의 S3 버킷으로 업로드합니다. **S3 크로스 리전 복제(CRR)**를 사용하여 객체를 대상 S3 버킷으로 복사합니다. 그런 다음 원본 S3 버킷의 데이터를 삭제합니다.  

C. 매일 **AWS Snowball Edge Storage Optimized** 장치를 사용하여 각 사이트에서 가장 가까운 리전으로 데이터를 전송합니다. S3 크로스 리전 복제를 사용하여 객체를 대상 S3 버킷으로 복사합니다.  

D. 각 사이트에서 데이터를 가장 가까운 리전의 Amazon EC2 인스턴스로 업로드합니다. 데이터를 Amazon EBS 볼륨에 저장합니다. 정기적으로 EBS 스냅샷을 생성하고 대상 S3 버킷이 있는 리전으로 복사합니다. 해당 리전에서 EBS 볼륨을 복원합니다.

---

A company collects data for temperature, humidity, and atmospheric pressure in cities across multiple continents. The average volume of data that the company collects from each site daily is 500 GB. Each site has a high-speed Internet connection.

The company wants to aggregate the data from all these global sites as quickly as possible in a single Amazon S3 bucket. The solution must minimize operational complexity.

Which solution meets these requirements?

  

A. Turn on S3 Transfer Acceleration on the destination S3 bucket. Use multipart uploads to directly upload site data to the destination S3 bucket.

B. Upload the data from each site to an S3 bucket in the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket. Then remove the data from the origin S3 bucket.

C. Schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each site to the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket.

D. Upload the data from each site to an Amazon EC2 instance in the closest Region. Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. At regular intervals, take an EBS snapshot and copy it to the Region that contains the destination S3 bucket. Restore the EBS volume in that Region.
```

정답: `A`

- **S3 Transfer Acceleration (TA)** 는 전 세계 AWS 엣지 로케이션([[AWS Services#AWS CloudFront|AWS CloudFront]] 엣지)을 통해 업로드를 받아 AWS 글로벌 백본으로 전달하므로, 장거리(geographically distributed clients → single S3 버킷) 업로드 성능을 크게 개선
- 각 사이트에 이미 **고속 인터넷**이 있고 매일 500 GB씩 전송해야 하는 반복적 워크플로우에 적합
- **multipart upload**를 사용하면 큰 파일/데이터를 분할해 병렬 업로드해 전송 시간을 단축하고 안정성을 높일 수 있음
- **운영 복잡성도 낮음**
- 각 사이트에서 S3에 직접 업로드하면 별도의 디바이스 운영·배송·중계서버 같은 관리가 필요 없음

오답 이유

**B. 각 사이트에서 가장 가까운 리전의 S3로 업로드 → S3 Cross-Region Replication(CRR)으로 목적지로 복제 → 원본 삭제**
- 운영 복잡성과 지연이 늘어남 -> 여러 리전별 버킷과 복제 규칙을 관리해야 하고, CRR 복제 지연과 비용(데이터 복제 요금) 발생
- 요구사항은 `최대한 빨리 + 운영 복잡성 최소화`를 요구하므로 직접 단일 버킷에 가속 업로드보다 덜 적합

**C. 매일 Snowball Edge Storage Optimized 디바이스를 스케줄링해서 각 사이트에서 가장 가까운 리전으로 전송 → CRR로 목적지 복사**
- Snowball 은 물리 디바이스 배송 기반이므로 네트워크 제약 시 사용하거나 대규모 초기 마이그레이션에 사용
- 전 세계 여러 사이트에서 매일 500 GB씩 반복적으로 처리하기에 디바이스 배송, 반납, 물리적 운영(운영 복잡성)과 소요시간 문제가 커짐

**D. 각 사이트 → 가장 가까운 리전의 EC2에 업로드 → EBS에 저장 → 정기적으로 EBS 스냅샷 찍어 목적지 리전으로 복사 → 복원**
- 운영 및 관리가 매우 복잡 - EC2 관리, 스냅샷/복사/복원 작업, 비용 및 오류지점 증가
- EBS 스냅샷은 객체 수준의 데이터 집계(로그/파일 업로드)에는 부적절하고 지연이 큼
- 단일 S3 버킷에 직접 업로드하는 것이 훨씬 단순하고 적합

## #2
```
한 회사는 자체 애플리케이션의 로그 파일을 분석할 수 있는 기능이 필요합니다.  
로그는 JSON 형식으로 Amazon S3 버킷에 저장되어 있습니다.  
쿼리는 단순하며, 필요할 때(on-demand) 실행됩니다.  

솔루션 설계자는 **기존 아키텍처를 최소한으로 변경**하면서 로그 분석을 수행해야 합니다.  

이 요구사항을 **최소한의 운영 부담으로** 충족할 수 있는 방법은 무엇입니까?

A. Amazon Redshift를 사용하여 모든 내용을 한 곳에 로드하고, 필요할 때 SQL 쿼리를 실행합니다.  
B. Amazon CloudWatch Logs에 로그를 저장하고, Amazon CloudWatch 콘솔에서 필요할 때 SQL 쿼리를 실행합니다.  
C. Amazon Athena를 Amazon S3와 직접 사용하여 필요할 때 쿼리를 실행합니다.  
D. AWS Glue를 사용하여 로그를 카탈로그화하고, Amazon EMR에서 임시 Apache Spark 클러스터를 사용하여 필요할 때 SQL 쿼리를 실행합니다.

---

A company needs the ability to analyze the log files of its proprietary application. The logs are stored in JSON format in an Amazon S3 bucket. Queries will be simple and will run on-demand. A solutions architect needs to perform the analysis with minimal changes to the existing architecture.  
What should the solutions architect do to meet these requirements with the LEAST amount of operational overhead?

- A. Use Amazon Redshift to load all the content into one place and run the SQL queries as needed.
- B. Use Amazon CloudWatch Logs to store the logs. Run SQL queries as needed from the Amazon CloudWatch console.
- C. Use Amazon Athena directly with Amazon S3 to run the queries as needed.
- D. Use AWS Glue to catalog the logs. Use a transient Apache Spark cluster on Amazon EMR to run the SQL queries as needed.
```

정답 : `C`

- Athena는 S3에 있는 파일(JSON)을 직접 일고 SQL로 쿼리하므로 로그를 S3에 둔 채 아키텍처 변경 없이 바로 분석 가능 -> 운영/관리할 클러스터가 필요 없음
- 온디맨드에 적합: 쿼리는 필요할 때만 실행하면 되고(서버리스/페이퍼쿼리), 반복적이지 않은 간단 온디맨드 분석에 비용/운영 면에서 효율적
- JSON 지원: Athena(및 그 하부의 Presto/Trino 스타일 엔진)는 Json/Parquet 같은 반정형 데이터에 대해 쿼리 가능하고 스키마를 정의해 쉽게 조회 가능

오답 이유

**A. [[AWS Services#Amazon Redshift|Amazon Redshift]]에 모두 로드 → 쿼리**
- Redshift는 강력한 DWH지만 **데이터를 먼저 로드(COPY 등)** 해야 하고 클러스터 운영·관리(프로비저닝, 유지보수, 비용)가 필요하다. 요청이 **온디맨드·간단**하고 아키텍처 변경을 최소화해야 하는 상황에서는 과도한 솔루션이다.   

**B. [[AWS Services#Amazon CloudWatch Logs / Logs Insights|Amazon CloudWatch Logs]]로 옮기고 CloudWatch Logs Insights에서 쿼리**
- CloudWatch Logs Insights는 로그 분석용 기능이 있지만, **현재 로그가 S3에 저장**되어 있다면 로그를 CloudWatch로 다시 이관(또는 애플리케이션 레벨 변경)해야 하고 이 또한 아키텍처 변경이 발생한다. 또한 CloudWatch Logs는 로그를 직접 수집/저장하는 용도로 설계되어 있어, 이미 S3에 저장된 대규모 로그를 단순히 온디맨드 SQL로 분석하려면 적합하지 않을 수 있다. (즉, “최소 변경” 요구사항에 부합하지 않음). 

**D. [[AWS Services#AWS Glue + Amazon EMR (Spark)|AWS Glue + Amazon EMR (Spark)]]으로 쿼리**
- Glue + EMR은 매우 유연하고 대규모 배치 처리에 적합하지만 **클러스터 설정·관리(또는 Glue Job 관리)** 같은 운영 오버헤드가 생긴다. 간단한 온디맨드 쿼리 목적이라면 과잉 설계이며 운영 복잡성이 큼.

## #3
```
한 회사는 부서별 여러 AWS 계정을 관리하기 위해 AWS Organizations를 사용하고 있습니다. 관리 계정에는 프로젝트 보고서를 저장하는 Amazon S3 버킷이 있습니다. 회사는 이 S3 버킷에 대한 액세스를 AWS Organizations 내 계정의 사용자로만 제한하려고 합니다.  

다음 중 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?  

A. S3 버킷 정책에 조직 ID를 참조하는 aws:PrincipalOrgID 글로벌 조건 키를 추가한다.  
B. 각 부서별로 조직 단위(OU)를 생성한다. S3 버킷 정책에 aws:PrincipalOrgPaths 글로벌 조건 키를 추가한다.  
C. AWS CloudTrail을 사용하여 CreateAccount, InviteAccountToOrganization, LeaveOrganization, RemoveAccountFromOrganization 이벤트를 모니터링한다. 그리고 S3 버킷 정책을 해당 이벤트에 맞게 업데이트한다.  
D. S3 버킷에 액세스가 필요한 각 사용자에게 태그를 추가한다. 그리고 S3 버킷 정책에 aws:PrincipalTag 글로벌 조건 키를 추가한다.

---

A company uses AWS Organizations to manage multiple AWS accounts for different departments. The management account has an Amazon S3 bucket that contains project reports. The company wants to limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations.  
Which solution meets these requirements with the LEAST amount of operational overhead?

- A. Add the aws PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy.
- B. Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.
- C. Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly.
- D. Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the S3 bucket policy. 
```

정답 : `A`
- aws:PrincipalOrgID 조건 키는 특정 [[AWS Services#AWS Organizations|AWS Organizations]] 전체를 기준으로 액세스를 제한할 수 있는 가장 단순하고 효과적인 방법
- 버킷 정책에 추가하면 해당 Organization ID 내의 모든 계정에서 오는 요청만 허용
- 운영 오버헤드가 거의 없으며 OU나 개별 사용자/계정 단위로 별도 관리할 필요가 없음

오답 이유

**B - aws:PrincipalOrgPaths 사용**
- aws:PrincipalOrgPaths는 특정 OU(조직 단위)까지 제한할 수 있어 더 세밀한 제어가 가능합니다.
- 하지만 문제 요구사항은 단순히 **Organizations 내부 계정**만 허용하는 것이므로 OU까지 관리하는 것은 불필요한 오버헤드입니다.

**C - [[AWS Services#AWS CloudTrail|CloudTrail]]로 이벤트 모니터링**
- CloudTrail을 통해 계정 이동/생성 이벤트를 추적하고 그때마다 정책을 업데이트하는 방식은 관리가 매우 번거롭습니다.
- “최소한의 운영 오버헤드” 요구사항에 맞지 않습니다.

**D - 사용자별 태그 기반 제어**
- 각 사용자에 대해 태그를 관리해야 하고, 새로운 사용자가 생길 때마다 태그 작업이 필요합니다.
- 이는 계정/조직 단위의 단순한 정책보다 훨씬 관리 비용이 큽니다.

## #4
```
한 애플리케이션이 VPC 내의 Amazon EC2 인스턴스에서 실행되고 있습니다.  
이 애플리케이션은 Amazon S3 버킷에 저장된 로그를 처리해야 합니다.  
EC2 인스턴스는 인터넷 연결 없이 S3 버킷에 접근해야 합니다.  

다음 중 Amazon S3로의 **프라이빗 네트워크 연결**을 제공하는 솔루션은 무엇입니까?  

A. S3 버킷에 대한 게이트웨이 VPC 엔드포인트를 생성한다.  
B. 로그를 Amazon CloudWatch Logs로 스트리밍한 후 S3 버킷으로 내보낸다.  
C. EC2 인스턴스에 인스턴스 프로파일을 생성하여 S3 액세스를 허용한다.  
D. S3 엔드포인트에 접근하기 위해 프라이빗 링크가 연결된 Amazon API Gateway API를 생성한다.

---

An application runs on an Amazon EC2 instance in a VPC. The application processes logs that are stored in an Amazon S3 bucket. The EC2 instance needs to access the S3 bucket without connectivity to the internet.  
Which solution will provide private network connectivity to Amazon S3?

- A. Create a gateway VPC endpoint to the S3 bucket.
- B. Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket.
- C. Create an instance profile on Amazon EC2 to allow S3 access.
- D. Create an Amazon API Gateway API with a private link to access the S3 endpoint.
```

정답 : `A. S3 버킷에 대한 게이트웨이 VPC 엔드포인트를 생성한다.`
- Amazon S3는 **게이트웨이 VPC 엔드포인트(Gateway Endpoint)**를 지원하며 이를 통해 프라이빗 네트워크 경로로 S3 버킷에 접근 가능
- 인터넷 게이트웨이, NAT 게이트웨이, 프록시 없이 바로 S3와 통신 가능

오답 이유

**B. CloudWatch Logs → S3 Export**
- CloudWatch Logs를 거쳐 S3에 데이터를 내보내는 방식은 S3 접근을 대체하지 못함
- EC2 인스턴스가 직접 S3 버킷에서 로그를 읽어야 하는 요구사항과 맞지 않음

**C. Instance Profile (IAM Role)**
- 인스턴스 프로파일(IAM Role)은 권한(Authorization)**을 제공하는 것이지 네트워크 경로를 제공하지 않음
- 따라서 S3에 접근하려면 인터넷/NAT 게이트웨이나 VPC 엔드포인트 필요

**D. API Gateway + PrivateLink**
- API Gateway를 사용해 S3를 프록시할 수는 있지만 불필요하게 복잡하며 비용 증가
- S3에 이미 최적화된 게이트웨이 엔드포인트 솔루션이 있기 때문에 잘못된 선택

## #5
```
한 회사는 단일 Amazon EC2 인스턴스에서 웹 애플리케이션을 호스팅하고 있으며, 사용자가 업로드한 문서를 Amazon EBS 볼륨에 저장하고 있습니다.  
확장성과 가용성을 높이기 위해, 회사는 아키텍처를 복제하여 다른 가용 영역(AZ)에 두 번째 EC2 인스턴스와 EBS 볼륨을 생성하고, 두 인스턴스를 모두 Application Load Balancer(ALB) 뒤에 배치했습니다.  

변경 후, 사용자는 웹사이트를 새로 고칠 때마다 **문서의 일부 집합만 보이고 전체 문서를 한 번에 볼 수 없다**고 보고했습니다.  

사용자가 항상 모든 문서를 볼 수 있도록 하기 위해 솔루션 아키텍트가 제안해야 하는 것은 무엇입니까?  

A. 데이터를 복사하여 두 EBS 볼륨에 모든 문서를 포함시킨다.  
B. Application Load Balancer를 구성하여 사용자를 문서가 있는 서버로 직접 연결한다.  
C. 두 EBS 볼륨의 데이터를 Amazon EFS로 복사한다. 애플리케이션을 수정하여 새 문서를 Amazon EFS에 저장하도록 한다.  
D. Application Load Balancer를 구성하여 요청을 두 서버 모두로 보낸다. 각 문서를 올바른 서버에서 반환한다.  

---

A company is hosting a web application on AWS using a single Amazon EC2 instance that stores user-uploaded documents in an Amazon EBS volume. For better scalability and availability, the company duplicated the architecture and created a second EC2 instance and EBS volume in another Availability Zone, placing both behind an Application Load Balancer. After completing this change, users reported that, each time they refreshed the website, they could see one subset of their documents or the other, but never all of the documents at the same time.  
What should a solutions architect propose to ensure users see all of their documents at once?

- A. Copy the data so both EBS volumes contain all the documents
- B. Configure the Application Load Balancer to direct a user to the server with the documents
- C. Copy the data from both EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS
- D. Configure the Application Load Balancer to send the request to both servers. Return each document from the correct server
```

정답 : `C. 두 EBS 볼륨의 데이터를 Amazon EFS로 복사한다. 애플리케이션을 수정하여 새 문서를 Amazon EFS에 저장하도록 한다.`
- EBS는 단일 인스턴스 전용 볼륨이므로 여러 인스턴스에서 동시에 접근할 수 없음.
- 이를 해결하기 위해 공유 파일 시스템을 사용해야함
	- [[AWS Services#Amazon EFS (Elastic File System)|Amazon EFS]]는 여러 EC2 인스턴스에서 동시에 접근 가능한 NFS 기반 파일 시스템
	- 문서를 EFS로 이동하고 애플리케이션이 새 문서를 EFS에 저장하도록 하면, 모든 인스턴스에서 같은 데이터를 볼 수 있음.

오답 이유
**A - EBS 볼륨 복사**
- 초기 복사 후 새로운 문서가 생기면 **동기화 문제** 발생.
- 사용자가 새로 업로드한 문서가 한 인스턴스에만 저장되므로 동일 문제 재발.

**B - ALB를 특정 서버로 연결**
- 로드 밸런서는 인스턴스 간 트래픽을 분산하도록 설계됨.
- 특정 서버로 연결하면 **고가용성/로드 분산** 효과가 사라짐.
- 또한, 사용자가 업로드한 문서가 다른 서버에 존재할 수 있음 → 여전히 일부 문서만 보임.

**D - 요청을 두 서버 모두로 보내기**
- ALB는 단일 요청을 **동시에 여러 서버에 보내는 기능 없음**.
- 구현이 복잡하며 불필요한 트래픽과 데이터 정합성 문제 발생.

## #6
```
한 회사는 NFS를 사용하여 온프레미스 네트워크 스토리지에 대용량 비디오 파일을 저장하고 있습니다.  
각 비디오 파일의 크기는 1 MB에서 500 GB까지 다양하며, 총 저장 용량은 70 TB이고 더 이상 증가하지 않습니다.  

회사는 비디오 파일을 Amazon S3로 **최대한 빠르게** 마이그레이션하면서 **최소한의 네트워크 대역폭**을 사용해야 합니다.  

다음 중 이 요구 사항을 충족하는 솔루션은 무엇입니까?  

A. S3 버킷을 생성하고, 해당 버킷에 쓰기 권한이 있는 IAM 역할을 생성합니다. AWS CLI를 사용하여 모든 파일을 로컬에서 S3 버킷으로 복사합니다.  
B. AWS Snowball Edge 작업을 생성합니다. 온프레미스에서 Snowball Edge 장치를 수령하고, Snowball Edge 클라이언트를 사용하여 데이터를 장치로 전송합니다. 장치를 반환하면 AWS가 데이터를 Amazon S3로 가져옵니다.  
C. 온프레미스에 S3 File Gateway를 배포합니다. S3 File Gateway에 연결할 공개 서비스 엔드포인트를 생성합니다. S3 버킷을 생성합니다. S3 File Gateway에서 새 NFS 파일 공유를 생성하고, 이를 S3 버킷에 연결합니다. 기존 NFS 파일 공유의 데이터를 S3 File Gateway로 전송합니다.  
D. 온프레미스 네트워크와 AWS 간에 AWS Direct Connect 연결을 설정합니다. 온프레미스에 S3 File Gateway를 배포합니다. S3 File Gateway에 연결할 공개 가상 인터페이스(VIF)를 생성합니다. S3 버킷을 생성합니다. S3 File Gateway에서 새 NFS 파일 공유를 생성하고, 이를 S3 버킷에 연결합니다. 기존 NFS 파일 공유의 데이터를 S3 File Gateway로 전송합니다.  

---

A company uses NFS to store large video files in on-premises network attached storage. Each video file ranges in size from 1 MB to 500 GB. The total storage is 70 TB and is no longer growing. The company decides to migrate the video files to Amazon S3. The company must migrate the video files as soon as possible while using the least possible network bandwidth.  
Which solution will meet these requirements?

- A. Create an S3 bucket. Create an IAM role that has permissions to write to the S3 bucket. Use the AWS CLI to copy all files locally to the S3 bucket.
- B. Create an AWS Snowball Edge job. Receive a Snowball Edge device on premises. Use the Snowball Edge client to transfer data to the device. Return the device so that AWS can import the data into Amazon S3.
- C. Deploy an S3 File Gateway on premises. Create a public service endpoint to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway.
- D. Set up an AWS Direct Connect connection between the on-premises network and AWS. Deploy an S3 File Gateway on premises. Create a public virtual interface (VIF) to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway.
```

정답 : `B. AWS Snowball Edge 작업을 생성합니다. 온프레미스에서 Snowball Edge 장치를 수령하고, Snowball Edge 클라이언트를 사용하여 데이터를 장치로 전송합니다. 장치를 반환하면 AWS가 데이터를 Amazon S3로 가져옵니다.`

- 문제의 요구사항은 빠른 마이그레이션과 최소환의 네트워크 대역폭 사용
- 데이터의 크기는 70TB, 파일 최대 크기는 500GB로 인터넷 전송 시 수일~수주 소요 가능
- AWS Snowball Edge는 대규모 데이터를 물리적으로 AWS로 이동시키는 오프라인 데이터 전송 장치
	- 장치를 수령 후 데이터 복사 -> 장치를 AWS에 반환하여 S3로 데이터 업로드
- 네트워크를 거의 사용하지 않으면서 대규모 데이터를 빠르게 마이그레이션 가능

오답 이유
**A - AWS CLI를 통한 직접 복사**
- 70 TB 데이터를 인터넷을 통해 전송하면 **대역폭 소모가 크고, 전송 속도가 매우 느림**
- 요구사항 “최소 네트워크 대역폭”을 충족하지 못함

**C - S3 File Gateway + 퍼블릭 엔드포인트**
- File Gateway는 온프레미스에서 S3로 마이그레이션 가능
- 하지만 퍼블릭 엔드포인트를 사용하면 **인터넷 대역폭을 모두 사용**
- 70 TB 전송 시 시간이 오래 걸림 → 요구사항 “빠른 마이그레이션” 불충족

**D - Direct Connect + S3 File Gateway**
- Direct Connect 사용 시 전송 속도는 빨라지지만, **70 TB 전송에는 여전히 상당한 시간 소요**
- Snowball Edge 대비 오프라인 전송보다 느리고, 장치 비용 효율이 떨어짐

## #7
```
한 회사는 들어오는 메시지를 수집하는 애플리케이션을 가지고 있습니다.  
수십 개의 다른 애플리케이션과 마이크로서비스가 이러한 메시지를 **빠르게 소비**합니다.  
메시지 수는 크게 변동하며, 때때로 **초당 100,000개**까지 급증하기도 합니다.  

회사는 솔루션을 **분리(decouple)**하고 **확장성**을 높이고자 합니다.  

다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?  

A. 메시지를 Amazon Kinesis Data Analytics에 저장합니다. 소비자 애플리케이션이 메시지를 읽고 처리하도록 구성합니다.  
B. 수집 애플리케이션을 Auto Scaling 그룹에 있는 Amazon EC2 인스턴스에 배포하고, CPU 지표에 따라 EC2 인스턴스 수를 조절합니다.  
C. 메시지를 단일 샤드가 있는 Amazon Kinesis Data Streams에 작성합니다. AWS Lambda 함수를 사용하여 메시지를 전처리하고 Amazon DynamoDB에 저장합니다. 소비자 애플리케이션이 DynamoDB에서 메시지를 읽고 처리하도록 구성합니다.  
D. 메시지를 Amazon SNS 주제로 게시하고, 여러 Amazon SQS 구독을 연결합니다. 소비자 애플리케이션이 큐에서 메시지를 처리하도록 구성합니다.  

---

A company has an application that ingests incoming messages. Dozens of other applications and microservices then quickly consume these messages. The number of messages varies drastically and sometimes increases suddenly to 100,000 each second. The company wants to decouple the solution and increase scalability.  
Which solution meets these requirements?

- A. Persist the messages to Amazon Kinesis Data Analytics. Configure the consumer applications to read and process the messages.
- B. Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group to scale the number of EC2 instances based on CPU metrics.
- C. Write the messages to Amazon Kinesis Data Streams with a single shard. Use an AWS Lambda function to preprocess messages and store them in Amazon DynamoDB. Configure the consumer applications to read from DynamoDB to process the messages.
- D. Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SOS) subscriptions. Configure the consumer applications to process the messages from the queues.
```

정답 : `D. 메시지를 Amazon SNS 주제로 게시하고, 여러 Amazon SQS 구독을 연결합니다. 소비자 애플리케이션이 큐에서 메시지를 처리하도록 구성합니다.`
- 문제의 요구사항은 메시지 수집과 소비를 분리(decoupling), 높은 확장성, 급격한 메시지 증가(최대 100,000/s) 처리
- SNS + SQS 패턴은 Pub/Sub 모델을 구현
	- SNS -> 여러 SQS 큐 구독 가능 -> 각 소비자가 독립적으로 처리 가능
	- 메시지 수 급증에도 SQS가 버퍼 역할 -> 소비자 처리 속도와 독립적
	- 완전 관리형 서비스 -> 서버 관리 필요 없음

오답 이유
**A - Kinesis Data Analytics**
- Kinesis Data Analytics는 **스트리밍 데이터를 분석**하기 위한 서비스
- 단순 메시지 전달과 소비자 분리용으로는 적합하지 않음

**B - EC2 Auto Scaling**
- Auto Scaling을 사용해 EC2 수를 늘리면 메시지 처리량은 늘어나지만
- **애플리케이션과 소비자가 직접 연결**되어 있어 decoupling 되지 않음
- 급격한 트래픽 증가 시 스케일링 지연 발생 가능

**C - Kinesis Data Streams 단일 샤드 + Lambda + DynamoDB**
- 단일 샤드 제한 → 처리량 한계
    - 샤드 1개당 최대 **1,000건/초 쓰기, 2,000건/초 읽기**
- 초당 100,000건 메시지를 처리할 수 없음 → 확장성 부족
- DynamoDB를 메시지 버퍼로 사용 → 복잡성 증가

## #8
```
한 회사가 분산 애플리케이션을 AWS로 마이그레이션하고 있습니다.  
애플리케이션은 **가변적인 워크로드**를 처리합니다.  
기존 플랫폼은 **여러 컴퓨트 노드에 작업을 분배하는 주(primary) 서버**로 구성되어 있습니다.  
회사는 애플리케이션을 **복원력과 확장성을 최대화하는 방식**으로 현대화하고자 합니다.  

다음 중 이러한 요구 사항을 충족하도록 아키텍처를 설계하는 방법은 무엇입니까?  

A. 작업 대상(job destination)으로 Amazon SQS 큐를 구성합니다.  
   컴퓨트 노드를 Auto Scaling 그룹으로 관리되는 EC2 인스턴스로 구현합니다.  
   EC2 Auto Scaling을 스케줄링 기반으로 구성합니다.  

B. 작업 대상(job destination)으로 Amazon SQS 큐를 구성합니다.  
   컴퓨트 노드를 Auto Scaling 그룹으로 관리되는 EC2 인스턴스로 구현합니다.  
   EC2 Auto Scaling을 큐의 크기를 기준으로 구성합니다.  

C. 주 서버(primary server)와 컴퓨트 노드를 Auto Scaling 그룹으로 관리되는 EC2 인스턴스로 구현합니다.  
   작업 대상(job destination)으로 AWS CloudTrail을 구성합니다.  
   EC2 Auto Scaling을 주 서버의 로드를 기준으로 구성합니다.  

D. 주 서버(primary server)와 컴퓨트 노드를 Auto Scaling 그룹으로 관리되는 EC2 인스턴스로 구현합니다.  
   작업 대상(job destination)으로 Amazon EventBridge를 구성합니다.  
   EC2 Auto Scaling을 컴퓨트 노드의 로드를 기준으로 구성합니다.  

---

A company is migrating a distributed application to AWS. The application serves variable workloads. The legacy platform consists of a primary server that coordinates jobs across multiple compute nodes. The company wants to modernize the application with a solution that maximizes resiliency and scalability.  
How should a solutions architect design the architecture to meet these requirements?

- A. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling to use scheduled scaling.
- B. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling based on the size of the queue.
- C. Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure AWS CloudTrail as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the primary server.
- D. Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure Amazon EventBridge (Amazon CloudWatch Events) as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the compute nodes.
```

정답 : `B. 작업 대상(job destination)으로 Amazon SQS 큐를 구성합니다. 컴퓨트 노드를 Auto Scaling 그룹으로 관리되는 EC2 인스턴스로 구현합니다. EC2 Auto Scaling을 큐의 크기를 기준으로 구성합니다.`

- 문제의 요구사항
	- 분산 처리
	- 가변 워크로드 처리 -> 워크로드가 급격히 늘어나거나 줄어도 대응할 수 있어야 함
	- 복원력과 확장성 극대화
- SQS + Auto Scailing 패턴
	- SQS를 통해 작업 큐를 분리(decoupling) -> 주 서버 장애에도 작업 손실 최소화
	- 컴퓨트 노드는 오토 스케일링 그룹에서 동적으로 스케일 아웃/인
	- 큐 사이즈를 기준으로 오토 스케일링 설정 -> 워크로드 급증 시 자동 확장

오답 이유
**A - 스케줄링 기반 Auto Scaling**
- 스케줄링은 정해진 시간에만 인스턴스 수를 조정
- 워크로드 변동(급증/급감)에 대응 불가 → 확장성 부족

**C - CloudTrail 사용**
- CloudTrail은 **AWS API 호출 로깅 및 감사**용 서비스
- 작업 분배 및 스케일링과 관련 없음 → 부적합

**D - EventBridge 사용**
- EventBridge는 이벤트 중심 처리 가능하지만, **작업 큐 기반 스케일링**과 직접 연계되지 않음
- Auto Scaling이 컴퓨트 노드의 로드 기준이면 급격한 메시지 폭증 대응에 한계

## #9
```
한 회사가 데이터 센터에서 SMB 파일 서버를 운영하고 있습니다.  
이 파일 서버는 **대용량 파일**을 저장하며, 생성 후 **처음 며칠간 자주 접근**됩니다.  
7일 이후에는 파일 접근이 거의 없습니다.  

총 데이터 크기는 계속 증가하여 회사의 **전체 저장 용량에 근접**하고 있습니다.  

솔루션 아키텍트는 **가용 저장 공간을 늘리면서 최근 자주 접근된 파일에 대한 낮은 지연(latency) 접근을 유지**해야 합니다.  
또한, **파일 라이프사이클 관리를 제공**하여 향후 저장 공간 문제를 방지해야 합니다.  

다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?  

A. AWS DataSync를 사용하여 SMB 파일 서버에서 7일 이상 된 데이터를 AWS로 복사합니다.  
B. Amazon S3 File Gateway를 생성하여 회사의 저장 공간을 확장합니다. S3 Lifecycle 정책을 만들어 데이터를 7일 후 S3 Glacier Deep Archive로 전환합니다.  
C. Amazon FSx for Windows File Server 파일 시스템을 생성하여 회사의 저장 공간을 확장합니다.  
D. 각 사용자 컴퓨터에 유틸리티를 설치하여 Amazon S3에 접근합니다. S3 Lifecycle 정책을 만들어 데이터를 7일 후 S3 Glacier Flexible Retrieval로 전환합니다.  

---

A company is running an SMB file server in its data center. The file server stores large files that are accessed frequently for the first few days after the files are created. After 7 days the files are rarely accessed.  
The total data size is increasing and is close to the company's total storage capacity. A solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files. The solutions architect must also provide file lifecycle management to avoid future storage issues.  
Which solution will meet these requirements?

- A. Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.
- B. Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.
- C. Create an Amazon FSx for Windows File Server file system to extend the company's storage space.
- D. Install a utility on each user's computer to access Amazon S3. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days.
```

정답 : `B. Amazon S3 File Gateway를 생성하여 회사의 저장 공간을 확장합니다. S3 Lifecycle 정책을 만들어 데이터를 7일 후 S3 Glacier Deep Archive로 전환합니다.`

- 요구사항
	- 최근 파일에 대해 낮은 지연 접근 유지
	- 저장 공간 확장
	- 파일 라이프사이클 관리
- [[AWS Services#AWS S3 File Gateway|AWS S3 File Gateway]]는 기존 SMB/NFS 파일 서버를 클라우드 확장 스토리지로 연결
	- 사용자는 온프레미스처럼 로컬 파일 서버처럼 접근 가능 -> 최근 파일은 온프레미스 캐시에서 낮은 지연 접근 가능
	- 백엔드는 S3에 저장 -> 용량 확장 가능
- S3 라이프사이클 정책을 통해 7일 후 Glacier Deep Archive로 자동 전환 -> 비용 절감 및 장기 보관
- 

오답 이유
**A - DataSync 단순 복사**
- DataSync는 파일을 **AWS로 이동/복사**만 하는 도구
- 최근 파일에 대한 낮은 지연 접근을 보장하지 못함
- 라이프사이클 관리 자동화 기능 부족


**C - FSx for Windows File Server**
- FSx는 확장 가능한 파일 시스템 제공
- 그러나 비용이 높고, Glacier와 같은 **자동 라이프사이클 정책** 지원하지 않음
- 최근/오래된 파일 접근 패턴 최적화 어려움
  

**D - 사용자 유틸리티 + S3**
- 각 사용자에 설치 필요 → 관리 복잡
- S3 직접 접근은 **온프레미스 SMB/NFS 서버처럼 동작하지 않음** → 기존 워크플로우와 호환성 문제
- Glacier Flexible Retrieval는 **액세스 지연 존재** → 최근 파일 접근 성능 요구사항 불충족

## #10
```
한 회사가 AWS에서 전자상거래 웹 애플리케이션을 구축하고 있습니다.  
애플리케이션은 **새 주문 정보를 처리하기 위해 Amazon API Gateway REST API**로 전송합니다.  

회사는 **주문이 수신된 순서대로 처리되도록 보장**하고자 합니다.  

다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?  

A. 애플리케이션이 주문을 수신할 때, API Gateway 통합을 사용하여 Amazon SNS 주제로 메시지를 게시합니다.  
   주제에 AWS Lambda 함수를 구독시켜 처리하도록 구성합니다.  

B. 애플리케이션이 주문을 수신할 때, API Gateway 통합을 사용하여 Amazon SQS **FIFO 큐**에 메시지를 전송합니다.  
   SQS FIFO 큐를 구성하여 AWS Lambda 함수를 호출하도록 설정합니다.  

C. API Gateway 인증자를 사용하여 애플리케이션이 주문을 처리하는 동안 모든 요청을 차단합니다.  

D. 애플리케이션이 주문을 수신할 때, API Gateway 통합을 사용하여 Amazon SQS **Standard 큐**에 메시지를 전송합니다.  
   SQS Standard 큐를 구성하여 AWS Lambda 함수를 호출하도록 설정합니다.  
   
---

A company is building an ecommerce web application on AWS. The application sends information about new orders to an Amazon API Gateway REST API to process. The company wants to ensure that orders are processed in the order that they are received.  
Which solution will meet these requirements?

- A. Use an API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when the application receives an order. Subscribe an AWS Lambda function to the topic to perform processing.
- B. Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue when the application receives an order. Configure the SQS FIFO queue to invoke an AWS Lambda function for processing.
- C. Use an API Gateway authorizer to block any requests while the application processes an order.
- D. Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue when the application receives an order. Configure the SQS standard queue to invoke an AWS Lambda function for processing.
```

정답 : `B`
- 요구사항 - 주문이 수신된 순서대로 처리되도록 보장
- SQS FIFO 큐는 메시지 순서를 보장
	- 메시지 그룹 ID를 사용해 그룹 내 순서를 유지
	- 중복 방지
- 람다 함수를 SQS FIFO 큐와 연동하면 주문이 들어온 순서대로 처리 가능

오답 이유
**A - SNS 사용**
- SNS는 **Pub/Sub 모델**    
- 여러 구독자가 병렬로 메시지를 처리하므로 **메시지 순서 보장 불가**

**C - API Gateway 요청 차단**
- 모든 요청을 차단하면 처리 순서를 맞출 수 있지만, **애플리케이션 가용성이 크게 떨어지고 비효율적**    
- 실질적인 순서 보장 방법 아님

**D - SQS Standard 큐 사용**
- Standard 큐는 **순서를 보장하지 않음**
- 대량 처리 및 높은 처리량에 적합하지만, **FIFO 요구사항 충족 불가**