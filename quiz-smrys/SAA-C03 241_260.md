---
created: 2025-10-01 15:24:56
last_modified: 2025-10-02 11:04:05
---
## #241
한 온라인 교육 회사가 AWS 클라우드로 마이그레이션하고 있습니다. 회사는 학생 기록을 PostgreSQL 데이터베이스에 보관하고 있습니다. 회사는 데이터가 항상 여러 AWS 리전에 걸쳐 사용 가능하며 온라인 상태로 유지되는 솔루션이 필요합니다.

이 요구사항을 가장 적은 운영 오버헤드로 충족하는 솔루션은 무엇입니까?

A. PostgreSQL 데이터베이스를 Amazon EC2 인스턴스의 PostgreSQL 클러스터로 마이그레이션합니다.
B. PostgreSQL 데이터베이스를 Amazon RDS for PostgreSQL DB 인스턴스로 마이그레이션하고 Multi-AZ 기능을 켭니다.
C. PostgreSQL 데이터베이스를 Amazon RDS for PostgreSQL DB 인스턴스로 마이그레이션합니다. 다른 리전에 읽기 복제본을 생성합니다.
D. PostgreSQL 데이터베이스를 Amazon RDS for PostgreSQL DB 인스턴스로 마이그레이션합니다. 다른 리전에 복사되도록 DB 스냅샷을 설정합니다.

```
An online learning company is migrating to the AWS Cloud. The company maintains its student records in a PostgreSQL database. The company needs a solution in which its data is available and online across multiple AWS Regions at all times.  
  
Which solution will meet these requirements with the LEAST amount of operational overhead?

- A. Migrate the PostgreSQL database to a PostgreSQL cluster on Amazon EC2 instances.
- B. Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance with the Multi-AZ feature turned on.
- C. Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Create a read replica in another Region.
- D. Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Set up DB snapshots to be copied to another Region.
```

정답 : `C`

- RDS for PostgreSQL의 크로스 리전 읽기 복제본은 주 리전의 쓰기 인스턴스와 다른 리전의 읽기 엔드포인트를 항상 온라인으로 유지
- 장애 시 프로모션으로 다른 리전의 읽기 복제본을 빠르게 쓰기 인스턴스로 전환 가능 → DR 용이
- 완전관리형 RDS이므로 운영 오버헤드가 낮고, EC2 자가 운영 대비 패치/백업/모니터링 부담이 적음

오답 이유

- **A. EC2 자가 운영**: 클러스터 구성, 복제, 장애 조치, 패치 등 **운영 부담이 큼**. “최소한의 운영 오버헤드” 요구에 부적합.
    
- **B. RDS Multi-AZ**: **단일 리전 내 고가용성**만 제공합니다. 다중 리전 “항상 온라인” 요구(지역 간 가용)는 충족하지 못합니다.
    
- **D. 스냅샷의 리전 간 복사**: **정기 백업**일 뿐 **온라인 읽기 엔드포인트**를 제공하지 않습니다. 실시간 가용/동기화 요구에 부적합.


## #242
한 회사가 웹 애플리케이션을 AWS에서 7개의 Amazon EC2 인스턴스를 사용하여 호스팅하고 있습니다. 회사는 모든 정상(healthy) EC2 인스턴스의 IP 주소가 DNS 쿼리에 대한 응답으로 반환되기를 요구합니다.

이 요구사항을 충족하기 위해 어떤 정책을 사용해야 합니까?

A. Simple 라우팅 정책
B. Latency 라우팅 정책
C. Multivalue 라우팅 정책
D. Geolocation 라우팅 정책

```
A company hosts its web application on AWS using seven Amazon EC2 instances. The company requires that the IP addresses of all healthy EC2 instances be returned in response to DNS queries.  
  
Which policy should be used to meet this requirement?

- A. Simple routing policy
- B. Latency routing policy
- C. Multivalue routing policy
- D. Geolocation routing policy
```

정답 : `C`

- Multivalue Answer Routing Policy는 Route 53에서 다수의 레코드를 반환하며, 헬스체크를 통과한 레코드만 응답에 포함
- 이 정책을 사용하면 질의 시 여러 정상 IP 주소가 응답에 포함되어, 클라이언트가 분산된 인스턴스에 연결 가능

오답 이유

- **A. Simple routing policy**
    - 단일 레코드만 반환. 여러 인스턴스를 동시에 반환하지 않음. 헬스체크 기능도 없음.
    
- **B. Latency routing policy**
    - 지리적으로 가장 낮은 지연 시간을 가진 리소스를 선택해 반환. 모든 정상 인스턴스를 반환하지 않음.
    
- **D. Geolocation routing policy**
    - 사용자의 지리적 위치에 따라 특정 리소스를 반환. 역시 모든 인스턴스를 반환하지 않음.


## #243
의료 연구소가 새로운 연구와 관련된 데이터를 생성합니다. 연구소는 전국의 클리닉에서 온프레미스 파일 기반 애플리케이션으로 이 데이터를 최소 지연(latency)으로 사용할 수 있게 하길 원합니다. 데이터 파일은 각 클리닉에 대해 읽기 전용 권한이 부여된 Amazon S3 버킷에 저장되어 있습니다.

이 요구사항을 충족하기 위해 솔루션스 아키텍트는 무엇을 권장해야 합니까?

A. 각 클리닉의 온프레미스에 가상 머신(VM)으로 AWS Storage Gateway 파일 게이트웨이를 배포합니다.
B. 처리 목적을 위해 AWS DataSync를 사용하여 파일을 각 클리닉의 온프레미스 애플리케이션으로 마이그레이션합니다.
C. 각 클리닉의 온프레미스에 가상 머신(VM)으로 AWS Storage Gateway 볼륨 게이트웨이를 배포합니다.
D. 각 클리닉의 온프레미스 서버에 Amazon Elastic File System(Amazon EFS) 파일 시스템을 연결합니다.

```
A medical research lab produces data that is related to a new study. The lab wants to make the data available with minimum latency to clinics across the country for their on-premises, file-based applications. The data files are stored in an Amazon S3 bucket that has read-only permissions for each clinic.  
  
What should a solutions architect recommend to meet these requirements?

- A. Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) on premises at each clinic
- B. Migrate the files to each clinic’s on-premises applications by using AWS DataSync for processing.
- C. Deploy an AWS Storage Gateway volume gateway as a virtual machine (VM) on premises at each clinic.
- D. Attach an Amazon Elastic File System (Amazon EFS) file system to each clinic’s on-premises servers.
```

정답 : `A`

- 파일 게이트웨이(File Gateway)는 S3 버킷을 백엔드로 해 SMB/NFS 파일 공유를 온프레미스에 제공하고, 로컬 캐시를 통해 자주 접근하는 데이터를 저지연으로 제공
- 각 클리닉은 기존 파일 기반 애플리케이션을 그대로 사용(프로토콜 호환)하면서, S3의 단일 원본 읽기 전용으로 안전하게 접근 가능
- 운영은 게이트웨이 장비/VM만 관리하면 되며, 중앙의 S3에 저장된 데이터가 곧바로 각 지점에 캐시되어 최소 지연으로 제공

오답 이유

- **B. DataSync 마이그레이션**
    - DataSync는 **일회성/주기적 전송 및 동기화**에 적합하며, 애플리케이션이 필요로 하는 **지속적인 파일 공유 + 저지연 접근**을 제공하지 않습니다. 각 지점에 데이터를 계속 복제하면 운영/비용 부담 증가.
    
- **C. 볼륨 게이트웨이**
    - iSCSI **블록 스토리지**(캐시드/스토어드 볼륨) 제공이 목적입니다. S3 객체를 **파일 공유**로 노출하려는 본 요구와 부합하지 않습니다.
    
- **D. EFS를 온프레미스에 연결**
    - EFS는 VPC 내 NFS 서비스이며, 온프레미스에서 사용하려면 **Direct Connect/VPN** 등 네트워크 구성이 필요하고 **지연이 커질 수** 있습니다. 또한 현재 데이터 원본이 S3이므로 EFS로의 **이관/동기화 작업**이 추가됩니다.


## #244
한 회사가 단일 Amazon EC2 인스턴스에서 실행되는 콘텐츠 관리 시스템을 사용하고 있습니다. 이 EC2 인스턴스에는 웹 서버와 데이터베이스 소프트웨어가 모두 포함되어 있습니다. 회사는 웹사이트 플랫폼을 고가용성으로 만들어야 하며, 사용자 수요를 충족하기 위해 웹사이트가 확장 가능해야 합니다.

이 요구사항을 충족하기 위해 솔루션스 아키텍트는 무엇을 권장해야 합니까?

A. 데이터베이스를 Amazon RDS로 이동하고 자동 백업을 활성화합니다. 동일한 가용 영역에서 다른 EC2 인스턴스를 수동으로 시작합니다. 해당 가용 영역에 Application Load Balancer를 구성하고 두 인스턴스를 타깃으로 설정합니다.
B. 데이터베이스를 기존 EC2 인스턴스와 같은 가용 영역의 읽기 복제본을 가진 Amazon Aurora 인스턴스로 마이그레이션합니다. 동일한 가용 영역에서 다른 EC2 인스턴스를 수동으로 시작합니다. Application Load Balancer를 구성하고 두 EC2 인스턴스를 타깃으로 설정합니다.
C. 데이터베이스를 다른 가용 영역에 읽기 복제본을 둔 Amazon Aurora로 이동합니다. EC2 인스턴스에서 Amazon 머신 이미지(AMI)를 생성합니다. 두 개의 가용 영역에 Application Load Balancer를 구성합니다. 두 가용 영역에 걸쳐 해당 AMI를 사용하는 Auto Scaling 그룹을 연결합니다.
D. 데이터베이스를 별도의 EC2 인스턴스로 이동하고 Amazon S3로 백업을 스케줄합니다. 원래 EC2 인스턴스에서 AMI를 생성합니다. 두 개의 가용 영역에 Application Load Balancer를 구성합니다. 두 가용 영역에 걸쳐 해당 AMI를 사용하는 Auto Scaling 그룹을 연결합니다.

```
A company is using a content management system that runs on a single Amazon EC2 instance. The EC2 instance contains both the web server and the database software. The company must make its website platform highly available and must enable the website to scale to meet user demand.  
  
What should a solutions architect recommend to meet these requirements?

- A. Move the database to Amazon RDS, and enable automatic backups. Manually launch another EC2 instance in the same Availability Zone. Configure an Application Load Balancer in the Availability Zone, and set the two instances as targets.
- B. Migrate the database to an Amazon Aurora instance with a read replica in the same Availability Zone as the existing EC2 instance. Manually launch another EC2 instance in the same Availability Zone. Configure an Application Load Balancer, and set the two EC2 instances as targets.
- C. Move the database to Amazon Aurora with a read replica in another Availability Zone. Create an Amazon Machine Image (AMI) from the EC2 instance. Configure an Application Load Balancer in two Availability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones.
- D. Move the database to a separate EC2 instance, and schedule backups to Amazon S3. Create an Amazon Machine Image (AMI) from the original EC2 instance. Configure an Application Load Balancer in two Availability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones.
```

정답 : `C`

- 다중 AZ 고가용성: Aurora 기본 인스턴스 + 다른 AZ의 읽기 복제본은 AZ 장애 시 빠른 승격으로 DB 가용성 유지
- 기존 EC2에서 AMI를 만들고, 두 AZ에 걸친 ALB + Auto Scaling 그룹으로 배치하면 수평 확장과 인스턴스 장애 복구가 자동화
- 애플리케이션 코드는 그대로 두고(AMI 기반), 데이터베이스만 관리형 서비스(Aurora)로 분리해 성능･내구성･운영 편의성을 확보

오답 이유

- **A**: RDS로 이전은 좋지만 **웹 인스턴스 두 대 모두 한 AZ**에 있고 ALB도 **단일 AZ** → AZ 장애 시 다운타임 발생.
    
- **B**: Aurora 읽기 복제본이 **같은 AZ**이므로 DB 고가용성 미흡. 웹 인스턴스도 한 AZ에만 있어 단일 장애점 존재.
    
- **D**: 데이터베이스를 **EC2 자체 운영**으로 두면 백업/페일오버/패치 등 운영 오버헤드와 위험이 큼. 관리형 DB를 쓰는 것이 바람직.


## #245
한 회사가 AWS에서 애플리케이션을 출시하려고 합니다. 애플리케이션은 Application Load Balancer(ALB)를 사용하여 트래픽을 단일 타깃 그룹의 최소 두 개 이상의 Amazon EC2 인스턴스로 라우팅합니다. 인스턴스는 각 환경별 Auto Scaling 그룹에 있습니다. 회사는 개발 환경과 운영(프로덕션) 환경이 필요합니다. 운영 환경은 트래픽이 높은 기간이 있을 것입니다.

다음 중 개발 환경을 가장 비용 효율적으로 구성하는 솔루션은 무엇입니까?

A. 개발 환경의 타깃 그룹을 재구성하여 타깃으로 단 하나의 EC2 인스턴스만 갖도록 합니다.
B. ALB의 로드 밸런싱 알고리즘을 'least outstanding requests'로 변경합니다.
C. 두 환경 모두에서 EC2 인스턴스의 크기를 줄입니다.
D. 개발 환경의 Auto Scaling 그룹에서 EC2 인스턴스의 최대 수를 줄입니다.

```
A company is launching an application on AWS. The application uses an Application Load Balancer (ALB) to direct traffic to at least two Amazon EC2 instances in a single target group. The instances are in an Auto Scaling group for each environment. The company requires a development environment and a production environment. The production environment will have periods of high traffic.  
  
Which solution will configure the development environment MOST cost-effectively?

- A. Reconfigure the target group in the development environment to have only one EC2 instance as a target.
- B. Change the ALB balancing algorithm to least outstanding requests.
- C. Reduce the size of the EC2 instances in both environments.
- D. Reduce the maximum number of EC2 instances in the development environment’s Auto Scaling group.
```

정답 : `A`

- 개발 환경은 일반적으로 트래픽이 낮고 가용성 요구가 낮으므로 최소 한 대(Desired/Min=1) 로 운영해 비용 절감
- ALB는 하나의 정상 타깃만 있어도 동작하므로, 개발 환경 타깃 그룹을 단일 EC2 인스턴스로 줄이는 것이 직접적이고 비용 절감 효과가 큼

오답 이유

- **B. ALB 알고리즘 변경**: 라우팅 방식 변경은 **비용에 영향이 거의 없음**. 인스턴스 수/크기 감소가 아닌 이상 비용 절감 효과가 없습니다.
    
- **C. 두 환경 모두 인스턴스 크기 축소**: 개발 환경만이 아니라 **프로덕션까지 성능에 악영향**을 줄 수 있습니다. 요구는 “개발 환경을 비용 효율적으로”이므로 과도한 변경입니다.
    
- **D. 개발 환경 ASG의 최대 수 감소**: 비용은 주로 **현재(Desired/Min) 용량**에 의해 결정됩니다. 개발 트래픽이 낮아 어차피 스케일 아웃하지 않는다면 최대치만 낮춰도 **실질 비용 절감이 거의 없습니다**.


## #246
한 회사가 여러 가용 영역에 Amazon EC2 인스턴스에서 웹 애플리케이션을 실행하고 있습니다. EC2 인스턴스는 프라이빗 서브넷에 있습니다. 솔루션스 아키텍트는 인터넷 연결형(Application Load Balancer, ALB)을 구현하고 EC2 인스턴스를 타깃 그룹으로 지정했습니다. 그러나 인터넷 트래픽이 EC2 인스턴스에 도달하지 않습니다.

이 문제를 해결하기 위해 솔루션스 아키텍트는 아키텍처를 어떻게 재구성해야 합니까?

A. ALB를 네트워크 로드 밸런서로 교체합니다. 인터넷 트래픽을 허용하기 위해 퍼블릭 서브넷에 NAT 게이트웨이를 구성합니다.
B. EC2 인스턴스를 퍼블릭 서브넷으로 이동합니다. EC2 인스턴스의 보안 그룹에 0.0.0.0/0으로의 아웃바운드 트래픽을 허용하는 규칙을 추가합니다.
C. EC2 인스턴스 서브넷의 라우팅 테이블을 업데이트하여 0.0.0.0/0 트래픽을 인터넷 게이트웨이 경로로 보냅니다. EC2 인스턴스의 보안 그룹에 0.0.0.0/0으로의 아웃바운드 트래픽을 허용하는 규칙을 추가합니다.
D. 각 가용 영역에 퍼블릭 서브넷을 생성합니다. 퍼블릭 서브넷을 ALB에 연결합니다. 퍼블릭 서브넷의 라우팅 테이블을 프라이빗 서브넷으로의 경로로 업데이트합니다.

```
A company runs a web application on Amazon EC2 instances in multiple Availability Zones. The EC2 instances are in private subnets. A solutions architect implements an internet-facing Application Load Balancer (ALB) and specifies the EC2 instances as the target group. However, the internet traffic is not reaching the EC2 instances.  
  
How should the solutions architect reconfigure the architecture to resolve this issue?

- A. Replace the ALB with a Network Load Balancer. Configure a NAT gateway in a public subnet to allow internet traffic.
- B. Move the EC2 instances to public subnets. Add a rule to the EC2 instances’ security groups to allow outbound traffic to 0.0.0.0/0.
- C. Update the route tables for the EC2 instances’ subnets to send 0.0.0.0/0 traffic through the internet gateway route. Add a rule to the EC2 instances’ security groups to allow outbound traffic to 0.0.0.0/0.
- D. Create public subnets in each Availability Zone. Associate the public subnets with the ALB. Update the route tables for the public subnets with a route to the private subnets.
```

정답 : `D`

- 인터넷 연결형 ALB는 반드시 퍼블릭 서브넷(라우트 테이블에 인터넷 게이트웨이(IGW) 경로가 있는 서브넷)에 배치되어야 함
- 타깃 EC2 인스턴스는 프라이빗 서브넷에 그대로 두고, 타깃 그룹으로 등록하면 ALB가 사설 IP로 백엔드와 통신
- 따라서 각 AZ에 퍼블릭 서브넷을 생성해 ALB 서브넷으로 지정하면 인터넷이 ALB까지 트래픽이 들어오고 ALB는 VPC 내부로 프라이빗 서브넷의 인스턴스에 연결

오답 이유

- **A. NLB + NAT 게이트웨이**
    - NAT 게이트웨이는 **프라이빗 인스턴스의 아웃바운드 인터넷** 용도이며, **인바운드 인터넷 트래픽 수신**에 사용되지 않습니다. 또한 NLB로 교체할 필요 없음.
    
- **B. 인스턴스를 퍼블릭 서브넷으로 이동**
    - 백엔드 인스턴스를 퍼블릭으로 노출할 필요가 없습니다. **보안상 비권장**이며, ALB 뒤 프라이빗 서브넷이 모범 사례입니다.
    
- **C. 프라이빗 서브넷에 IGW 경로 추가**
    - 프라이빗 서브넷은 IGW로 직접 라우팅하지 않습니다(그렇게 하면 퍼블릭 서브넷이 됩니다). 문제의 핵심은 **ALB가 퍼블릭 서브넷에 있어야 한다는 점**입니다.


## #247
한 회사가 Amazon RDS for MySQL에 데이터베이스를 배포했습니다. 트랜잭션이 증가함에 따라, 데이터베이스 지원 팀은 DB 인스턴스에서 읽기 속도가 느려지고 있으며 읽기 복제본 추가를 권장했습니다.

이 변경을 구현하기 전에 솔루션스 아키텍트가 수행해야 할 작업은 무엇입니까? (두 가지 선택)

A. RDS 기본 노드에서 binlog 복제를 활성화합니다.
B. 소스 DB 인스턴스에 대한 장애 조치(failover) 우선 순위를 선택합니다.
C. 소스 DB 인스턴스에서 장기 실행 트랜잭션이 완료되도록 허용합니다.
D. 글로벌 테이블을 생성하고 테이블이 사용 가능할 AWS 리전을 지정합니다.
E. 백업 보존 기간을 0이 아닌 값으로 설정하여 소스 인스턴스에서 자동 백업을 활성화합니다.

```
A company has deployed a database in Amazon RDS for MySQL. Due to increased transactions, the database support team is reporting slow reads against the DB instance and recommends adding a read replica.  
  
Which combination of actions should a solutions architect take before implementing this change? (Choose two.)

- A. Enable binlog replication on the RDS primary node.
- B. Choose a failover priority for the source DB instance.
- C. Allow long-running transactions to complete on the source DB instance.
- D. Create a global table and specify the AWS Regions where the table will be available.
- E. Enable automatic backups on the source instance by setting the backup retention period to a value other than 0.
```

정답 : `A, E`

- A - binlog 복제 활성화 : MySQL 기반 RDS에서 읽기 복제본은 binlog(binary log) 기반 복제를 사용하기 때문에 소스 인스턴스에서 binlog 복제를 활성화해야 읽기 복제본 생성 가능
- E - 자동 백업 활성화 : 읽기 복제본을 생성하려면 자동 백업이 활성화되어 있어야 하며, 이를 위해 BackupRetentionPeriod > 0 으로 설정

오답 이유

- **B. failover priority**: 이는 **Aurora**에서 사용되는 기능으로, 표준 RDS MySQL 읽기 복제본과는 관계없습니다.
    
- **C. 장기 실행 트랜잭션 허용**: 트랜잭션 종료는 좋은 운영 관행이지만, 읽기 복제본 생성의 **필수 조건은 아님**.
    
- **D. 글로벌 테이블 생성**: 이는 **Amazon DynamoDB** 기능이며 RDS MySQL과 무관합니다.


## #248
한 회사가 Amazon EC2 인스턴스에서 분석 소프트웨어를 실행합니다. 이 소프트웨어는 사용자가 Amazon S3에 업로드한 데이터를 처리하기 위한 작업 요청을 받습니다. 사용자는 제출한 일부 데이터가 처리되지 않고 있음을 보고합니다. Amazon CloudWatch는 EC2 인스턴스의 CPU 사용률이 지속적으로 100%에 가깝거나 그에 근접함을 보여줍니다. 회사는 시스템 성능을 개선하고 사용자 부하에 따라 시스템을 확장하고자 합니다.

이 요구사항을 충족하려면 솔루션스 아키텍트는 무엇을 해야 합니까?

A. 인스턴스의 복사본을 만듭니다. 모든 인스턴스를 Application Load Balancer 뒤에 배치합니다.
B. Amazon S3에 대한 S3 VPC 엔드포인트를 생성합니다. 소프트웨어가 해당 엔드포인트를 참조하도록 업데이트합니다.
C. EC2 인스턴스를 중지합니다. 더 강력한 CPU와 더 많은 메모리를 가진 인스턴스 유형으로 수정합니다. 인스턴스를 다시 시작합니다.
D. 들어오는 요청을 Amazon Simple Queue Service(Amazon SQS)로 라우팅합니다. 대기열 크기를 기준으로 EC2 Auto Scaling 그룹을 구성합니다. 소프트웨어가 큐에서 읽도록 업데이트합니다.

```
A company runs analytics software on Amazon EC2 instances. The software accepts job requests from users to process data that has been uploaded to Amazon S3. Users report that some submitted data is not being processed Amazon CloudWatch reveals that the EC2 instances have a consistent CPU utilization at or near 100%. The company wants to improve system performance and scale the system based on user load.  
  
What should a solutions architect do to meet these requirements?

- A. Create a copy of the instance. Place all instances behind an Application Load Balancer.
- B. Create an S3 VPC endpoint for Amazon S3. Update the software to reference the endpoint.
- C. Stop the EC2 instances. Modify the instance type to one with a more powerful CPU and more memory. Restart the instances.
- D. Route incoming requests to Amazon Simple Queue Service (Amazon SQS). Configure an EC2 Auto Scaling group based on queue size. Update the software to read from the queue.
```

정답 : `D`

- 병목 원인 : CPU 100% 고착은 EC2가 입력 요청 속도를 따라가지 못한다는 신호
- SQS로 요청을 큐잉해 프런트엔드 수락과 백엔드 처리를 비동기 디커플링
- 대기열 길이(백로그/Backlog per instance) 를 지표로 EC2 Auto Scaling을 구성하면 부하 급증 시 워커 수를 늘리고, 잦아들면 감소시켜 탄력적 확장과 누락 없는 처리 보장
- SQS의 내구성･재시도･DLQ로 실패 항목을 격리해 데이터 미처리 문제 방지

오답 이유

- **A. ALB 뒤 수평 확장만**
    - ALB는 **동기 요청 분산**에 적합하나, 작업이 **장시간/배치성 처리**인 경우 백프레셔 제어가 어렵습니다. 큐 기반 버퍼링이 필요합니다.
    
- **B. S3 VPC 엔드포인트**
    - S3 접근 경로를 사설화/최적화할 수는 있지만, **CPU 포화**나 **부하 기반 확장** 문제의 본질을 해결하지 못합니다.
    
- **C. 수직 확장(인스턴스 타입 상향)**
    - 일시적 개선일 뿐 **스파이크 시 지속 포화**와 **탄력 확장** 요구를 해결하지 못합니다. 또한 비용 효율성도 떨어질 수 있습니다.


## #249
한 회사가 AWS 클라우드에서 호스팅되는 미디어 애플리케이션을 위해 공유 스토리지 솔루션을 구현하고 있습니다. 회사는 SMB 클라이언트를 사용하여 데이터에 접근할 수 있는 기능이 필요합니다. 솔루션은 완전 관리형이어야 합니다.

이 요구사항을 충족하는 AWS 솔루션은 무엇입니까?

A. AWS Storage Gateway 볼륨 게이트웨이를 생성합니다. 필요한 클라이언트 프로토콜을 사용하는 파일 공유를 생성합니다. 애플리케이션 서버를 파일 공유에 연결합니다.
B. AWS Storage Gateway 테이프 게이트웨이를 생성합니다. 테이프를 Amazon S3를 사용하도록 구성합니다. 애플리케이션 서버를 테이프 게이트웨이에 연결합니다.
C. Amazon EC2 Windows 인스턴스를 생성합니다. 인스턴스에 Windows 파일 공유 역할을 설치 및 구성합니다. 애플리케이션 서버를 파일 공유에 연결합니다.
D. Amazon FSx for Windows File Server 파일 시스템을 생성합니다. 파일 시스템을 원본 서버에 연결합니다. 애플리케이션 서버를 파일 시스템에 연결합니다.

```
A company is implementing a shared storage solution for a media application that is hosted in the AWS Cloud. The company needs the ability to use SMB clients to access data. The solution must be fully managed.  
  
Which AWS solution meets these requirements?

- A. Create an AWS Storage Gateway volume gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.
- B. Create an AWS Storage Gateway tape gateway. Configure tapes to use Amazon S3. Connect the application server to the tape gateway.
- C. Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.
- D. Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.
```

정답 : `D`

- 요구사항
	- SMB 프로토콜 지원 (Windows 기반 파일 공유)
	- 완전 관리형 서비스
- Amazon FSx for Windows File Server는 AWS에서 제공하는 완전 관리형 SMB/NFS 파일 시스템으로, Active Directory 통합, 자동 백업, 다중 AZ 배포, 확장성 등을 제공하며 Windows 환경 애플리케이션에 최적

오답 이유

- **A. Storage Gateway (Volume Gateway)**
    - iSCSI 블록 스토리지 제공. SMB 파일 공유 제공 아님.
    
- **B. Storage Gateway (Tape Gateway)**
    - 백업/아카이브 용도로 가상 테이프 제공. SMB 액세스와 무관.
    
- **C. EC2 Windows 인스턴스에 파일 서버 역할 설치**
    - 가능은 하지만 **자가 관리** 방식으로 패치, 확장성, 가용성 관리 부담이 큼. “완전 관리형” 요구를 충족하지 못함.


## #250
한 회사의 보안 팀은 VPC Flow Logs에서 네트워크 트래픽이 캡처되기를 요청했습니다. 로그는 처음 90일 동안은 자주 액세스되고, 그 이후에는 간헐적으로 액세스될 예정입니다.

솔루션스 아키텍트가 로그를 구성할 때 이 요구사항을 충족하려면 무엇을 해야 합니까?

A. Amazon CloudWatch를 대상으로 사용합니다. CloudWatch 로그 그룹에 만료 기간을 90일로 설정합니다.
B. Amazon Kinesis를 대상으로 사용합니다. Kinesis 스트림이 항상 로그를 90일 동안 보존하도록 구성합니다.
C. AWS CloudTrail을 대상으로 사용합니다. CloudTrail이 Amazon S3 버킷에 저장하도록 구성하고, S3 Intelligent-Tiering을 활성화합니다.
D. Amazon S3를 대상으로 사용합니다. S3 수명 주기 정책을 활성화하여 90일 후 로그를 S3 Standard-Infrequent Access(S3 Standard-IA)로 전환합니다.

```
A company’s security team requests that network traffic be captured in VPC Flow Logs. The logs will be frequently accessed for 90 days and then accessed intermittently.  
  
What should a solutions architect do to meet these requirements when configuring the logs?

- A. Use Amazon CloudWatch as the target. Set the CloudWatch log group with an expiration of 90 days
- B. Use Amazon Kinesis as the target. Configure the Kinesis stream to always retain the logs for 90 days.
- C. Use AWS CloudTrail as the target. Configure CloudTrail to save to an Amazon S3 bucket, and enable S3 Intelligent-Tiering.
- D. Use Amazon S3 as the target. Enable an S3 Lifecycle policy to transition the logs to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.
```

정답 : `D`

- 요구사항
	- 처음 90일 간은 빈번한 액세스 → S3 Standard가 적합
	- 이후 간헐적 액세스 → S3 Standard-IA로 이전해 비용 절감
- S3 라이프사이클 정책을 사용하면 자동으로 스토리지 클래스를 전환할 수 있어 운영 오버헤드가 거의 없음

오답 이유

- **A. CloudWatch Logs + 90일 만료**
    - 90일 후 로그가 **삭제**되므로 이후 간헐적 액세스 요구사항을 충족하지 못함.
    
- **B. Kinesis**
    - Kinesis Data Streams 보존 기간은 최대 365일까지 가능하지만, 일반적으로 **실시간 처리/스트리밍 분석용**이지 장기 로그 저장에는 부적합. 비용도 비효율적.
    
- **C. CloudTrail + S3 Intelligent-Tiering**
    - CloudTrail은 **API 호출 로그 서비스**이며, VPC Flow Logs는 CloudTrail이 아니라 **S3 또는 CloudWatch**에 직접 전송해야 함. 잘못된 서비스 선택.


## #251
새로운 VPC의 프라이빗 서브넷에 Amazon EC2 인스턴스가 있습니다. 이 서브넷은 아웃바운드 인터넷 액세스가 없지만, EC2 인스턴스는 외부 벤더로부터 월간 보안 업데이트를 다운로드할 수 있어야 합니다.

이 요구사항을 충족하려면 솔루션스 아키텍트는 무엇을 해야 합니까?

A. 인터넷 게이트웨이를 생성하여 VPC에 연결합니다. 프라이빗 서브넷의 라우트 테이블을 기본 경로로 인터넷 게이트웨이를 사용하도록 구성합니다.
B. NAT 게이트웨이를 생성하여 퍼블릭 서브넷에 배치합니다. 프라이빗 서브넷의 라우트 테이블을 기본 경로로 NAT 게이트웨이를 사용하도록 구성합니다.
C. NAT 인스턴스를 생성하여 EC2 인스턴스가 위치한 동일한 서브넷에 배치합니다. 프라이빗 서브넷의 라우트 테이블을 기본 경로로 NAT 인스턴스를 사용하도록 구성합니다.
D. 인터넷 게이트웨이를 생성하여 VPC에 연결합니다. NAT 인스턴스를 생성하여 EC2 인스턴스가 위치한 동일한 서브넷에 배치합니다. 프라이빗 서브넷의 라우트 테이블을 기본 경로로 인터넷 게이트웨이를 사용하도록 구성합니다.

```
An Amazon EC2 instance is located in a private subnet in a new VPC. This subnet does not have outbound internet access, but the EC2 instance needs the ability to download monthly security updates from an outside vendor.  
  
What should a solutions architect do to meet these requirements?

- A. Create an internet gateway, and attach it to the VPC. Configure the private subnet route table to use the internet gateway as the default route.
- B. Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.
- C. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the NAT instance as the default route.
- D. Create an internet gateway, and attach it to the VPC. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the internet gateway as the default route.
```

정답 : `B`

- 퍼블릭 서브넷에 NAT 게이트웨이를 두고, 프라이빗 서브넷의 기본 경로(0.0.0.0/0)를 NAT 게이트웨이로 지정
- 프라이빗 인스턴스가 아웃바운드로만 인터넷에 접근해 업데이트를 다운로드할 수 있으며, 인바운드로는 여전히 외부에서 직접 접근 못함

오답 이유

- **A. 인터넷 게이트웨이를 프라이빗 서브넷 기본 경로로 사용**
    - 그렇게 하면 해당 서브넷은 더 이상 **프라이빗 서브넷이 아니게** 되며, 인스턴스에 퍼블릭 IP가 없으면 통신도 성립하지 않습니다. 보안 요구와 불일치.
    
- **C. NAT 인스턴스를 프라이빗 서브넷에 배치**
    - NAT 인스턴스는 **퍼블릭 서브넷**에 배치되어야 하며, 자체 관리/스케일링/가용성 처리 필요. NAT **게이트웨이가 관리형/자동 확장**으로 더 적합.
    
- **D. IGW + NAT 인스턴스(프라이빗 서브넷) + IGW로 기본 경로**
    - 프라이빗 서브넷에서 IGW로 직접 라우팅은 잘못된 구성입니다. 또한 NAT 인스턴스 위치도 부적절.

## #252
솔루션스 아키텍트가 고객 사례 파일을 저장할 시스템을 설계해야 합니다. 이 파일들은 회사의 핵심 자산이며 매우 중요합니다. 파일 수는 시간이 지남에 따라 증가할 것입니다.

이 파일들은 Amazon EC2 인스턴스에서 실행되는 여러 애플리케이션 서버에서 동시에 접근할 수 있어야 합니다. 솔루션은 내장된 중복성(redundancy)을 가져야 합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. Amazon Elastic File System (Amazon EFS)
B. Amazon Elastic Block Store (Amazon EBS)
C. Amazon S3 Glacier Deep Archive
D. AWS Backup

```
A solutions architect needs to design a system to store client case files. The files are core company assets and are important. The number of files will grow over time.  
  
The files must be simultaneously accessible from multiple application servers that run on Amazon EC2 instances. The solution must have built-in redundancy.  
  
Which solution meets these requirements?

- A. Amazon Elastic File System (Amazon EFS)
- B. Amazon Elastic Block Store (Amazon EBS)
- C. Amazon S3 Glacier Deep Archive
- D. AWS Backup
```

정답 : `A`

- 여러 EC2 인스턴스에서 동시 접근 → EFS는 NFS 프로토콜 기반으로 여러 EC2에서 동시 마운트 가능
- 파일 증가에 따른 확장성 → EFS는 자동 확장형 스토리지. 파일 수/용량 증가에 맞춰 관리 불필요
- 중요 자산 보호 → EFS는 내장 고가용성 및 내구성 제공(AZ간 복제)

오답 이유

- **B. Amazon EBS**
    - EBS는 단일 EC2 인스턴스에만 연결 가능(멀티 어태치 옵션은 일부 사례에 한정, 일반적이지 않음). 여러 인스턴스에서 동시에 파일 접근 불가.
    
- **C. Amazon S3 Glacier Deep Archive**
    - 장기 아카이브/백업 스토리지. 즉시 액세스 불가(복원 시간이 필요). EC2 인스턴스에서 파일 시스템처럼 동시 접근 불가능.
    
- **D. AWS Backup**
    - 백업 관리 서비스로 **실시간 접근용 스토리지**가 아님. EC2 인스턴스에서 직접 사용할 수 없음.


## #253
솔루션스 아키텍트가 두 개의 IAM 정책(Policy1과 Policy2)을 생성했습니다. 두 정책은 IAM 그룹에 연결되어 있습니다.  

Policy 1:
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "iam:Get*",
        "iam:List*",
        "kms:List*",
        "ec2:*",
        "ds:*",
        "logs:Get*",
        "logs:Describe*"
      ],
      "Resource": "*"
    }
  ]
}

Policy 2:
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Deny",
      "Action": "ds:Delete*",
      "Resource": "*"
    }
  ]
}

한 클라우드 엔지니어가 IAM 사용자로서 이 IAM 그룹에 추가되었습니다. 이 클라우드 엔지니어가 수행할 수 있는 작업은 무엇입니까?

A. IAM 사용자 삭제  
B. 디렉토리 삭제  
C. Amazon EC2 인스턴스 삭제  
D. Amazon CloudWatch Logs에서 로그 삭제  

```
A solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are attached to an IAM group.  
  
Policy 1:
{
	"Version": "2012-10-17", "Statement": [
		{
			"Effect": "Allow",
			"Action": [
				"iam:Get*",
				"iam:List*",
				"kms:List*",
				"ec2:*",
				"ds:*",
				"logs:Get*",
				"logs:Describe*"
			],
			"Resource": "*"
		}
	]
}

Policy 2:
{
	"Version": "2012-10-17", "Statement": [
		{
			"Effect": "Deny",
			"Action": "ds:Delete*"
			"Resource": "*"
		}
	]
} 
  
A cloud engineer is added as an IAM user to the IAM group. Which action will the cloud engineer be able to perform?

- A. Deleting IAM users
- B. Deleting directories
- C. Deleting Amazon EC2 instances
- D. Deleting logs from Amazon CloudWatch Logs
```

정답 : `C`

- 정책 1에서 `ec2:*`이 허용되어 있으므로, EC2 관련 작업(생성, 수정, 삭제 포함) 가능
- 정책 2에서 `ds:Delete*` 명령에 대해 명시적 Deny 설정 → 디렉토리 삭제 허용X
- **IAM 관련 권한**은 `iam:Get*`와 `iam:List*`만 허용 → IAM 사용자 삭제(`iam:DeleteUser`)는 불가능.
- **CloudWatch Logs 관련 권한**은 `logs:Get*`와 `logs:Describe*`만 허용 → 로그 삭제(`logs:Delete*`)는 불가능.

오답 이유

- **A. IAM 사용자 삭제** → DeleteUser는 `iam:Delete*` 권한이 필요하지만 정책1에 포함되지 않음. 불가능.
- **B. 디렉토리 삭제** → `ds:Delete*`는 정책1에 포함되지만, 정책2에서 **명시적 Deny**. 따라서 불가능.
- **D. CloudWatch Logs 로그 삭제** → `logs:Get*` 및 `logs:Describe*`만 허용. 삭제(`logs:Delete*`)는 불가능.


## #254
한 회사가 최근에 3계층 애플리케이션을 VPC로 마이그레이션한 내용을 검토하고 있습니다. 보안 팀은 애플리케이션 계층 간 Amazon EC2 보안 그룹 인바운드 및 아웃바운드 규칙에 최소 권한 원칙(least privilege)이 적용되지 않고 있음을 발견했습니다.

이 문제를 수정하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?

A. 인스턴스 ID를 소스 또는 대상으로 사용하는 보안 그룹 규칙을 생성합니다.
B. 보안 그룹 ID를 소스 또는 대상으로 사용하는 보안 그룹 규칙을 생성합니다.
C. VPC CIDR 블록을 소스 또는 대상으로 사용하는 보안 그룹 규칙을 생성합니다.
D. 서브넷 CIDR 블록을 소스 또는 대상으로 사용하는 보안 그룹 규칙을 생성합니다.

```
A company is reviewing a recent migration of a three-tier application to a VPC. The security team discovers that the principle of least privilege is not being applied to Amazon EC2 security group ingress and egress rules between the application tiers.  
  
What should a solutions architect do to correct this issue?

- A. Create security group rules using the instance ID as the source or destination.
- B. Create security group rules using the security group ID as the source or destination.
- C. Create security group rules using the VPC CIDR blocks as the source or destination.
- D. Create security group rules using the subnet CIDR blocks as the source or destination.
```

정답 : `B`

- 보안 그룹을 서로 참조하여 규칙을 작성하면, 특정 계층(예: 웹 → 앱 → DB) 간 필요한 트래픽만 허용 가능
- 예: 웹 계층 보안 그룹 → 앱 계층 보안 그룹으로만 443/80 허용, 앱 계층 보안 그룹 → DB 계층 보안 그룹으로만 3306 허용

오답 이유

- **A. 인스턴스 ID 기반 규칙**: 보안 그룹은 인스턴스 ID를 소스/대상으로 지정할 수 없습니다. 잘못된 방법입니다.
    
- **C. VPC CIDR 블록 기반 규칙**: VPC 내 모든 리소스 간 트래픽을 허용하는 것이 되어 **너무 광범위**합니다. 최소 권한 원칙 위배.
    
- **D. 서브넷 CIDR 블록 기반 규칙**: 해당 서브넷에 속한 모든 인스턴스 간 트래픽을 허용하게 되어 **과도한 권한 부여**입니다.


## #255
한 회사에는 주문을 데이터베이스에 기록하고 결제 처리를 위해 서비스를 호출하는 이커머스 결제(체크아웃) 워크플로가 있습니다. 사용자들은 체크아웃 과정에서 타임아웃을 겪고 있습니다. 사용자가 체크아웃 폼을 다시 제출하면 동일한 거래에 대해 여러 개의 고유 주문이 생성됩니다.

중복 주문 생성을 방지하기 위해 솔루션스 아키텍트는 이 워크플로를 어떻게 리팩터링해야 합니까?

A. 웹 애플리케이션이 주문 메시지를 Amazon Kinesis Data Firehose로 보내도록 구성합니다. 결제 서비스가 Kinesis Data Firehose에서 메시지를 가져와 주문을 처리하도록 설정합니다.
B. AWS CloudTrail에서 기록된 애플리케이션 경로 요청을 기준으로 AWS Lambda 함수를 호출하는 규칙을 생성합니다. Lambda가 데이터베이스를 조회하고 결제 서비스를 호출하며 주문 정보를 전달하도록 합니다.
C. 주문을 데이터베이스에 저장합니다. 주문 번호를 포함하는 메시지를 Amazon Simple Notification Service(Amazon SNS)로 보냅니다. 결제 서비스가 Amazon SNS를 폴링하여 메시지를 가져오고 주문을 처리하도록 설정합니다.
D. 주문을 데이터베이스에 저장합니다. 주문 번호를 포함하는 메시지를 Amazon Simple Queue Service(Amazon SQS) FIFO 큐로 보냅니다. 결제 서비스가 메시지를 가져와 주문을 처리하도록 하고, 큐에서 메시지를 삭제합니다.

```
A company has an ecommerce checkout workflow that writes an order to a database and calls a service to process the payment. Users are experiencing timeouts during the checkout process. When users resubmit the checkout form, multiple unique orders are created for the same desired transaction.  
  
How should a solutions architect refactor this workflow to prevent the creation of multiple orders?

- A. Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the payment service to retrieve the message from Kinesis Data Firehose and process the order.
- B. Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path request. Use Lambda to query the database, call the payment service, and pass in the order information.
- C. Store the order in the database. Send a message that includes the order number to Amazon Simple Notification Service (Amazon SNS). Set the payment service to poll Amazon SNS, retrieve the message, and process the order.
- D. Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.
```

정답 : `D`

- SQS FIFO 큐를 사용하면 정확히 한 번 처리 및 순서 보장을 달성할 수 있어, 재시도나 중복 제출 시에도 중복 주문 생성을 방지
- 큐잉으로 웹 요청(쓰기)와 백엔드 결제 처리(실행)를 비동기 디커플링해 타임아웃 개선

오답 이유

- **A. Kinesis Data Firehose**: 대상(S3/Redshift/OpenSearch 등)으로 **배치 전송/적재**에 최적화된 서비스이며, 소비자가 임의로 **폴링/삭제**하는 패턴이 아닙니다. 결제 작업의 **정확히 한 번 처리** 보장과 작업 오케스트레이션에 부적합합니다.
    
- **B. CloudTrail 트리거 + Lambda**: CloudTrail은 **API 호출 감시/감사** 용도입니다. 애플리케이션 워크플로 구동에 적합하지 않고, **중복 억제/순서 보장** 문제도 해결하지 못합니다.
    
- **C. SNS**: SNS는 **푸시형 퍼브/섭**으로, 구독자에 대한 **중복/순서 보장**이 없습니다. 또한 “SNS를 폴링”은 일반적인 사용 패턴이 아닙니다. 중복 제출 시 중복 처리 가능성이 큽니다.


## #256
솔루션스 아키텍트가 Amazon S3 버킷을 스토리지로 사용하는 문서 검토 애플리케이션을 구현하고 있습니다. 이 솔루션은 문서의 실수로 인한 삭제를 방지하고, 문서의 모든 버전을 사용할 수 있어야 합니다. 사용자는 문서를 다운로드, 수정, 업로드할 수 있어야 합니다.

이 요구사항을 충족하기 위해 어떤 작업 조합을 수행해야 합니까? (두 가지를 선택하십시오.)

A. 읽기 전용 버킷 ACL을 활성화합니다.
B. 버킷에서 버전 관리를 활성화합니다.
C. 버킷에 IAM 정책을 연결합니다.
D. 버킷에서 MFA Delete를 활성화합니다.
E. AWS KMS를 사용하여 버킷을 암호화합니다.

```
A solutions architect is implementing a document review application using an Amazon S3 bucket for storage. The solution must prevent accidental deletion of the documents and ensure that all versions of the documents are available. Users must be able to download, modify, and upload documents.  
  
Which combination of actions should be taken to meet these requirements? (Choose two.)

- A. Enable a read-only bucket ACL.
- B. Enable versioning on the bucket.
- C. Attach an IAM policy to the bucket.
- D. Enable MFA Delete on the bucket.
- E. Encrypt the bucket using AWS KMS.
```

정답 : `B, D`

- B - 버전관리 활성화 → 모둔 문서 버전을 보관 가능. 수정/삭제 시 과거 버전도 유지되어 "실수로 삭제"를 방지
- D - MFA Delete 활성화 → 객체의 영구 삭제 및 버전 삭제를 위해 MFA 인증 필요

오답 이유

- **A. 읽기 전용 버킷 ACL**: 사용자들이 문서를 업로드/수정해야 하므로 읽기 전용 ACL은 요구사항을 방해함.
    
- **C. 버킷에 IAM 정책 연결**: 접근 권한 제어에는 필요할 수 있으나, 문제의 핵심 요구사항인 **삭제 방지 및 버전 유지**를 직접적으로 보장하지는 않음.
    
- **E. AWS KMS 암호화**: 보안 측면에서 유용하지만, 암호화는 **삭제 방지나 버전 관리와 무관**.


## #257
한 회사가 AWS 계정의 모든 애플리케이션에서 Amazon EC2 Auto Scaling 이벤트를 보고하는 솔루션을 구축하고 있습니다. 회사는 서버리스 솔루션을 사용하여 EC2 Auto Scaling 상태 데이터를 Amazon S3에 저장해야 합니다. 이후 S3의 데이터를 사용하여 대시보드에서 거의 실시간(near-real-time) 업데이트를 제공할 것입니다. 이 솔루션은 EC2 인스턴스 시작 속도에 영향을 주면 안 됩니다.

어떻게 데이터를 Amazon S3로 이동해야 이 요구사항을 충족할 수 있습니까?

A. Amazon CloudWatch 메트릭 스트림을 사용하여 EC2 Auto Scaling 상태 데이터를 Amazon Kinesis Data Firehose로 보냅니다. 데이터를 Amazon S3에 저장합니다.
B. Amazon EMR 클러스터를 시작하여 EC2 Auto Scaling 상태 데이터를 수집하고 Amazon Kinesis Data Firehose로 데이터를 보냅니다. 데이터를 Amazon S3에 저장합니다.
C. Amazon EventBridge 규칙을 생성하여 일정에 따라 AWS Lambda 함수를 호출합니다. Lambda 함수를 구성하여 EC2 Auto Scaling 상태 데이터를 직접 Amazon S3로 보냅니다.
D. EC2 인스턴스 시작 중 부트스트랩 스크립트를 사용하여 Amazon Kinesis Agent를 설치합니다. Kinesis Agent가 EC2 Auto Scaling 상태 데이터를 수집하여 Amazon Kinesis Data Firehose로 보내도록 구성합니다. 데이터를 Amazon S3에 저장합니다.

```
A company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications in an AWS account. The company needs to use a serverless solution to store the EC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 instance launches.  
  
How should the company move the data to Amazon S3 to meet these requirements?

- A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.
- B. Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.
- C. Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.
- D. Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent. Configure Kinesis Agent to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.
```

정답 : `A`

- CloudWatch Metric Streams → Kinesis Data Firehose → S3는 전부 서버리스
- 메트릭을 수 초 단위의 near-real-time으로 S3에 지속적으로 적재할 수 있어 대시보드 업데이트에 적합
- 에이전트 설치나 인스턴스 내부 처리 없이 외부 수집으로 EC2 인스턴스 시작 속도에 영향을 주지 않음

오답 이유

- **B. EMR 클러스터 사용**: EMR은 **서버리스가 아니며** 클러스터 운영/비용/관리 오버헤드가 큽니다. 목적(단순 스트리밍 적재)에 비해 과도합니다.
    
- **C. EventBridge + 정기 스케줄 Lambda**: 스케줄 기반 폴링은 **near-real-time 보장 어려움**. 또한 Auto Scaling 상태를 주기적으로 모으는 별도 로직 필요.
    
- **D. 인스턴스 부트스트랩 + Kinesis Agent**: 에이전트 설치가 **인스턴스 런치 시간을 증가**시켜 요구사항(런치 속도 영향 없음)에 반합니다. 또한 각 인스턴스에 구성 필요로 운영 복잡도 상승.


## #258
한 회사의 애플리케이션이 매시간 수백 개의 .csv 파일을 Amazon S3 버킷에 넣습니다. 파일 크기는 1GB입니다. 파일이 업로드될 때마다, 회사는 파일을 Apache Parquet 형식으로 변환하여 출력 파일을 S3 버킷에 배치해야 합니다.

다음 중 운영 오버헤드가 가장 낮은 방식으로 이러한 요구사항을 충족하는 솔루션은 무엇입니까?

A. .csv 파일을 다운로드하고 Parquet 형식으로 변환한 다음 출력 파일을 S3 버킷에 배치하는 AWS Lambda 함수를 생성합니다. 각 S3 PUT 이벤트마다 Lambda 함수를 호출합니다.
B. .csv 파일을 읽고 Parquet 형식으로 변환한 다음 출력 파일을 S3 버킷에 배치하는 Apache Spark 작업을 생성합니다. 각 S3 PUT 이벤트마다 Spark 작업을 호출하는 AWS Lambda 함수를 생성합니다.
C. 애플리케이션이 .csv 파일을 넣는 S3 버킷에 대해 AWS Glue 테이블과 AWS Glue 크롤러를 생성합니다. 주기적으로 AWS Lambda 함수가 Amazon Athena를 사용하여 AWS Glue 테이블을 쿼리하고, 쿼리 결과를 Parquet 형식으로 변환하여 출력 파일을 S3 버킷에 배치하도록 스케줄링합니다.
D. .csv 파일을 Parquet 형식으로 변환하여 출력 파일을 S3 버킷에 배치하는 AWS Glue ETL 작업을 생성합니다. 각 S3 PUT 이벤트마다 ETL 작업을 호출하는 AWS Lambda 함수를 생성합니다.

```
A company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet format and place the output file into an S3 bucket.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event.
- B. Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the Spark job.
- C. Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the .csv files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS Glue table, convert the query results into Parquet format, and place the output files into an S3 bucket.
- D. Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job.
```

정답 : `D`

- 파일이 1GB 크기이고 시간당 수백 개이므로 대량 변환에 적합한 서버리스 분산 ETL 필요
- AWS Glue ETL(Job, 서버리스 Spark) 는 대규모 파일 변환에 최적화
	- 및 자동확장/오케스트레이션/재시도 등 관리형 기능을 제공해 운영 오버헤드 최소
- S3 PUT 이벤트를 람다로 수신 → Glue Job 트리거 하면 업로드 시점에 즉시 변환 파이프라인이 동작

오답 이유

- **A. Lambda로 직접 변환**: 1GB 파일의 파싱/변환은 **실행 시간(최대 15분), 메모리, /tmp(최대 10GB)** 제약에 부딪힐 가능성이 큽니다. 수백 건 동시 처리에도 비효율적입니다.
    
- **B. 자체 Spark 작업**: 일반적으로 **EMR 클러스터 운영**이 필요하여 노드 관리/스케일링/패치 등 **운영 부담**이 큽니다. Glue가 같은 Spark 기반을 **완전관리형**으로 제공합니다.
    
- **C. Athena 쿼리로 변환**: Athena는 쿼리 결과를 S3에 저장할 수 있으나 **원본 파일 단위의 변환 파이프라인**이 아니며, 업로드마다 즉각 변환(이벤트 트리거) 요건에도 부적합합니다.


## #259
회사는 Amazon RDS DB 인스턴스에서 실행되는 모든 데이터베이스에 대해 새로운 데이터 보존 정책을 구현하고 있습니다. 회사는 최소 2년 동안 일일 백업을 보존해야 합니다. 백업은 일관성이 있어야 하며 복구 가능해야 합니다.

이 요구사항을 충족하기 위해 솔루션스 아키텍트는 어떤 솔루션을 권장해야 합니까?

A. AWS Backup에서 RDS 백업을 보존할 백업 볼트를 생성합니다. 일일 일정과 생성 후 2년의 만료 기간을 가진 새로운 백업 계획을 생성합니다. RDS DB 인스턴스를 백업 계획에 할당합니다.
B. RDS DB 인스턴스에 대해 일일 스냅샷을 위한 백업 윈도를 구성합니다. 각 RDS DB 인스턴스에 2년의 스냅샷 보존 정책을 할당합니다. Amazon Data Lifecycle Manager(Amazon DLM)를 사용하여 스냅샷 삭제를 스케줄링합니다.
C. 데이터베이스 트랜잭션 로그가 2년의 만료 기간으로 Amazon CloudWatch Logs에 자동으로 백업되도록 구성합니다.
D. AWS Database Migration Service(AWS DMS) 복제 작업을 구성합니다. 복제 인스턴스를 배포하고, 변경 데이터 캡처(CDC) 작업을 구성하여 데이터베이스 변경 사항을 대상지로 Amazon S3에 스트리밍합니다. 2년 후 스냅샷을 삭제하도록 S3 수명 주기 정책을 구성합니다.

```
A company is implementing new data retention policies for all databases that run on Amazon RDS DB instances. The company must retain daily backups for a minimum period of 2 years. The backups must be consistent and restorable.  
  
Which solution should a solutions architect recommend to meet these requirements?

- A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.
- B. Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.
- C. Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 2 years.
- D. Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication instance, and configure a change data capture (CDC) task to stream database changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 2 years.
```

정답 : `A`

- AWS Backup은 RDS 인스턴스에 대한 일관된 백업(스냅샷 기반)을 중앙집중식으로 스케줄링/보존/보호할 수 있음
- 수년 단위 보존 기간을 백업 계획에서 지정가능
- 생성된 백업은 복구 가능하며, 백업 볼트 보존 정책으로 2년 이상 일일 백업 보존 요건을 간단히 충족
- 별도의 스크립트나 클러스터 운영이 없고, 운영 오버헤드 최소화

오답 이유

- **B. RDS 자동 백업 보존 2년 + DLM**
    - RDS **자동 백업 보존 기간은 최대 35일**입니다. 2년 보존은 네이티브로 불가하며, **Amazon DLM은 EBS 스냅샷용**이라 RDS 스냅샷에 적용되지 않습니다.
    
- **C. 트랜잭션 로그를 CloudWatch Logs로**
    - CloudWatch Logs는 **로그 보관** 서비스이며 **복구 가능한 DB 백업**이 아닙니다. 복구 일관성과 스냅샷 기반 리스토어 요구를 충족하지 못합니다.
    
- **D. DMS CDC → S3**
    - DMS CDC는 변경 스트림을 S3에 적재할 뿐, **일관된 시점 백업/원클릭 RDS 복구**를 제공하지 않습니다. **스냅샷**이 아니므로 요구의 “일관성 있고 복구 가능” 조건에 부합하지 않습니다.


## #260
한 회사의 컴플라이언스 팀은 파일 공유를 AWS로 이동해야 합니다. 이 파일 공유는 Windows Server SMB 파일 공유에서 실행 중입니다. 온프레미스에서 직접 관리하는 Active Directory가 파일과 폴더에 대한 액세스를 제어하고 있습니다.

회사는 Amazon FSx for Windows File Server를 솔루션의 일부로 사용하려고 합니다. 회사는 마이그레이션 후에도 온프레미스 Active Directory 그룹이 FSx for Windows File Server SMB 컴플라이언스 공유, 폴더, 파일에 대한 액세스를 제한하도록 보장해야 합니다. 회사는 FSx for Windows File Server 파일 시스템을 생성했습니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. Active Directory Connector를 생성하여 Active Directory에 연결합니다. Active Directory 그룹을 IAM 그룹에 매핑하여 액세스를 제한합니다.  
B. Restrict 태그 키와 Compliance 태그 값을 가진 태그를 할당합니다. Active Directory 그룹을 IAM 그룹에 매핑하여 액세스를 제한합니다.  
C. FSx for Windows File Server에 직접 연결된 IAM 서비스 연결 역할(service-linked role)을 생성하여 액세스를 제한합니다.  
D. 파일 시스템을 Active Directory에 조인(join)하여 액세스를 제한합니다.  

```
A company’s compliance team needs to move its file shares to AWS. The shares run on a Windows Server SMB file share. A self-managed on-premises Active Directory controls access to the files and folders.  
  
The company wants to use Amazon FSx for Windows File Server as part of the solution. The company must ensure that the on-premises Active Directory groups restrict access to the FSx for Windows File Server SMB compliance shares, folders, and files after the move to AWS. The company has created an FSx for Windows File Server file system.  
  
Which solution will meet these requirements?

- A. Create an Active Directory Connector to connect to the Active Directory. Map the Active Directory groups to IAM groups to restrict access.
- B. Assign a tag with a Restrict tag key and a Compliance tag value. Map the Active Directory groups to IAM groups to restrict access.
- C. Create an IAM service-linked role that is linked directly to FSx for Windows File Server to restrict access.
- D. Join the file system to the Active Directory to restrict access.
```

정답 : `D`

- FSx for Windows File Server는 네이티브 Windows SMB 프로토콜 및 NTFS 권한을 지원
- 따라서 Active Directory에 직접 조인하면, 기존 온프레미스 AD의 사용자 및 그룹 권한이 그대로 적용되어 폴더/파일에 대한 접근 제어 가능
- IAM 그룹 매핑, 태그, 서비스 연결 역할은 SMB 수준의 세부 권한 관리와는 관련 없음

오답 이유

- **A. Active Directory Connector + IAM 그룹 매핑**
    - AD Connector는 AWS 서비스가 AD를 참조할 수 있도록 하지만, SMB 공유의 세부 ACL을 IAM 그룹으로 매핑하는 기능은 없습니다.
    
- **B. 태그 기반 제어**
    - 태그는 리소스 수준 권한 제어에 사용되며, SMB 공유나 NTFS 파일 접근 제어에는 적용되지 않습니다.
    
- **C. IAM 서비스 연결 역할**
    - 서비스 연결 역할은 AWS 리소스 관리 목적으로 사용되며, 사용자/그룹 파일 액세스 제어를 제공하지 않습니다.