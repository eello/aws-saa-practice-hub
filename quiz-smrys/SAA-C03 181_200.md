---
created: 2025-09-29 16:00:36
last_modified: 2025-09-30 12:26:26
---
## #181
한 회사는 Amazon EC2 인스턴스에서 실행되는 레거시 데이터 처리 애플리케이션을 보유하고 있습니다. 데이터는 순차적으로 처리되지만 결과의 순서는 중요하지 않습니다. 애플리케이션은 모놀리식 아키텍처를 사용합니다. 회사가 증가하는 수요를 충족하기 위해 애플리케이션을 확장하는 유일한 방법은 인스턴스의 크기를 늘리는 것입니다.

회사의 개발팀은 애플리케이션을 Amazon Elastic Container Service(Amazon ECS)에서 마이크로서비스 아키텍처로 재작성하기로 결정했습니다.

마이크로서비스 간 통신을 위해 솔루션스 아키텍트가 무엇을 권장해야 합니까?

A. Amazon Simple Queue Service(Amazon SQS) 큐를 생성합니다. 데이터 생산자에 코드를 추가하여 큐로 데이터를 보냅니다. 데이터 소비자에 코드를 추가하여 큐에서 데이터를 처리합니다.  

B. Amazon Simple Notification Service(Amazon SNS) 주제를 생성합니다. 데이터 생산자에 코드를 추가하여 주제에 알림을 게시합니다. 데이터 소비자에 코드를 추가하여 주제를 구독합니다.  

C. 메시지를 전달하기 위해 AWS Lambda 함수를 생성합니다. 데이터 생산자에 코드를 추가하여 데이터 객체와 함께 Lambda 함수를 호출합니다. 데이터 소비자에 코드를 추가하여 Lambda 함수에서 전달되는 데이터 객체를 수신합니다.  

D. Amazon DynamoDB 테이블을 생성합니다. DynamoDB Streams를 활성화합니다. 데이터 생산자에 코드를 추가하여 테이블에 데이터를 삽입합니다. 데이터 소비자에 코드를 추가하여 DynamoDB Streams API를 사용해 새 테이블 엔트리를 감지하고 데이터를 검색합니다.

```
A company has a legacy data processing application that runs on Amazon EC2 instances. Data is processed sequentially, but the order of results does not matter. The application uses a monolithic architecture. The only way that the company can scale the application to meet increased demand is to increase the size of the instances.  
  
The company’s developers have decided to rewrite the application to use a microservices architecture on Amazon Elastic Container Service (Amazon ECS).  
  
What should a solutions architect recommend for communication between the microservices?

- A. Create an Amazon Simple Queue Service (Amazon SQS) queue. Add code to the data producers, and send data to the queue. Add code to the data consumers to process data from the queue.
- B. Create an Amazon Simple Notification Service (Amazon SNS) topic. Add code to the data producers, and publish notifications to the topic. Add code to the data consumers to subscribe to the topic.
- C. Create an AWS Lambda function to pass messages. Add code to the data producers to call the Lambda function with a data object. Add code to the data consumers to receive a data object that is passed from the Lambda function.
- D. Create an Amazon DynamoDB table. Enable DynamoDB Streams. Add code to the data producers to insert data into the table. Add code to the data consumers to use the DynamoDB Streams API to detect new table entries and retrieve the data.
```

정답 : `A`

- Amazon SQS는 메시지 큐로서 생상자와 소비자를 완전히 분리(decoupling)하고, 메시지 내구성, 재시도(Visibility Timeout + DLQ), 자동 확장성 등을 제공하여 마이크로서비스 간 안정적인 비동기 처리에 적합
- 결과 순서 보장이 필요하지 않으므로 표준 SQS(또는 필요시 FIFO)를 선택할 수 있으며, 서비스 운영 오버헤드가 적고 설계가 단순

오답 이유

- **B. Amazon SNS (오답)**
    - SNS는 퍼블리시/서브스크라이브(pub/sub)용으로 _브로드캐스트_에 적합합니다. 다수 소비자에게 동시에 메시지를 전달해야 할 때 유리하지만, 개별 소비자의 재시도/내구성(특히 실패 시 재처리 보장)은 SQS만큼 간단하게 보장되지 않습니다. 또한 메시지 처리가 느린 소비자가 전체 플로우에 영향을 줄 수 있어 비동기 처리 제어에 부적합할 수 있습니다.
- **C. AWS Lambda 브로커 (오답)**
    - Lambda를 메시지 중개자로 사용하면 호출/비용과 연결 관리가 복잡해지고, 대량 메시지(높은 처리량·버스트)에서 Lambda 동시성/한계에 걸릴 수 있습니다. 또한 내구성(메시지 영속성)과 재시도 로직을 직접 구현해야 하므로 운영·관리 부담이 커집니다.
- **D. DynamoDB + Streams (오답)**
    - DynamoDB 스트림은 이벤트 기반 처리에 사용 가능하지만, 메시지 큐로서의 목적(명확한 ack/visibility/재시도/DLQ 등)을 제공하지 않으며, 데이터 모델(테이블) 유지·비용이 추가되고 소비자 설계가 복잡해집니다. 단순 메시지 전달/비동기 작업 분배에는 과도한 솔루션입니다.


## #182
한 회사는 온프레미스의 MySQL 데이터베이스를 AWS로 마이그레이션하려고 합니다. 회사는 최근에 비즈니스에 심각한 영향을 미친 데이터베이스 장애를 겪었습니다. 다시는 이런 일이 발생하지 않도록 회사는 데이터 손실을 최소화하고 모든 트랜잭션이 최소 두 노드에 저장되도록 하는 신뢰할 수 있는 데이터베이스 솔루션을 AWS에서 원합니다.

어떤 솔루션이 이러한 요구사항을 충족합니까?

A. 세 개의 가용 영역(Availability Zones)에 있는 세 노드로 동기 복제를 구성한 Amazon RDS DB 인스턴스를 생성합니다.  

B. 데이터가 동기적으로 복제되도록 Multi-AZ 기능을 활성화한 Amazon RDS MySQL DB 인스턴스를 생성합니다.  

C. Amazon RDS MySQL DB 인스턴스를 생성한 다음 별도의 AWS 리전에 리드 리플리카(read replica)를 생성하여 데이터를 동기적으로 복제합니다.  

D. MySQL 엔진이 설치된 Amazon EC2 인스턴스를 생성하고 AWS Lambda 함수를 트리거하여 데이터를 동기적으로 Amazon RDS MySQL DB 인스턴스로 복제합니다.

```
A company wants to migrate its MySQL database from on premises to AWS. The company recently experienced a database outage that significantly impacted the business. To ensure this does not happen again, the company wants a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes.  
  
Which solution meets these requirements?

- A. Create an Amazon RDS DB instance with synchronous replication to three nodes in three Availability Zones.
- B. Create an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data.
- C. Create an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data.
- D. Create an Amazon EC2 instance with a MySQL engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance.
```

정답 : `B`

- Amazon RDS MySQL의 Multi-AZ 배포는 쓰기 가능한 기본(Primary) 인스턴스와 하나의 동기 복제된 대기(Standby) 인스턴스를 다른 가용 영역에 두어 운영 장애 시 자동 페일오버를 제공
- 멀티 AZ 구성에는 트랜잭션이 기본 인스턴스에 커밋될 때 스탠바이에도 동기적으로 적용되므로 데이터 손실 위험이 최소화
- RDS Multi-AZ는 관리형 서비스로 자동화된 백업, 소프트웨어 패치, 모니터링, 자동 페일오버를 제공하므로 운영 부담을 줄이면서 고가용성 보장

오답 이유

- **A. “세 개 AZ에 세 노드로 동기 복제된 RDS DB 인스턴스”**
    - 옵션 A는 모호합니다. 표준 **RDS MySQL**은 기본적으로 두 노드(Multi-AZ: primary + standby)로 동기 복제됩니다. RDS(표준 MySQL 엔진)에서 “세 노드로 동기 복제”를 제공한다는 것은 일반적인 구성과 일치하지 않습니다. (Aurora는 스토리지를 여러 AZ에 분산하고 복제본을 여러 노드에 유지하지만, 표현이 애매합니다.) 따라서 문제 문맥에서 신뢰할 만한 표준 솔루션으로 보기 어렵습니다。
- **C. RDS + 리드 리플리카(다른 리전) — 동기 복제라고 되어 있음**
    - **리전 간 리플리카는 일반적으로 비동기 복제**입니다(특히 RDS for MySQL의 리전 간 리플리카). 비동기 복제는 네트워크 지연과 장애 시 데이터 손실 위험이 있으므로 “모든 트랜잭션을 최소 두 노드에 동기적으로 저장”하려는 요구에는 부합하지 않습니다.
- **D. EC2 MySQL + Lambda로 RDS 동기 복제**
    - 이 접근은 복잡하고 신뢰성·운영상의 문제가 큽니다. 직접 동기 복제를 구현하는 것은 오류가 발생하기 쉽고 관리 오버헤드가 높으며 자동 페일오버 같은 관리형 기능을 제공하지 않습니다. 또한 구현 자체가 본래 요구(관리 최소화, 신뢰성 보장)와 상충합니다.


## #183
한 회사가 신규 동적 주문 웹사이트를 구축하고 있습니다. 회사는 서버 유지관리 및 패치를 최소화하기를 원합니다. 웹사이트는 고가용성이어야 하며 사용자 수요 변화에 맞춰 읽기 및 쓰기 용량을 가능한 한 신속하게 확장할 수 있어야 합니다.

어떤 솔루션이 이러한 요구사항을 충족합니까?

A. 정적 콘텐츠를 Amazon S3에 호스팅합니다. 동적 콘텐츠는 Amazon API Gateway와 AWS Lambda를 사용하여 호스팅합니다. 데이터베이스는 온디맨드 용량(ondemand capacity)의 Amazon DynamoDB를 사용합니다. 웹사이트 콘텐츠 제공을 위해 Amazon CloudFront를 구성합니다.  

B. 정적 콘텐츠를 Amazon S3에 호스팅합니다. 동적 콘텐츠는 Amazon API Gateway와 AWS Lambda를 사용하여 호스팅합니다. 데이터베이스는 Aurora Auto Scaling이 적용된 Amazon Aurora를 사용합니다. 웹사이트 콘텐츠 제공을 위해 Amazon CloudFront를 구성합니다.  

C. 모든 웹사이트 콘텐츠를 Amazon EC2 인스턴스에 호스팅합니다. EC2 인스턴스를 스케일링하기 위해 Auto Scaling 그룹을 생성합니다. 트래픽 분산을 위해 Application Load Balancer를 사용합니다. 데이터베이스는 프로비저닝된 쓰기 용량(provisioned write capacity)의 Amazon DynamoDB를 사용합니다.  

D. 모든 웹사이트 콘텐츠를 Amazon EC2 인스턴스에 호스팅합니다. EC2 인스턴스를 스케일링하기 위해 Auto Scaling 그룹을 생성합니다. 트래픽 분산을 위해 Application Load Balancer를 사용합니다. 데이터베이스는 Aurora Auto Scaling이 적용된 Amazon Aurora를 사용합니다.

```
A company is building a new dynamic ordering website. The company wants to minimize server maintenance and patching. The website must be highly available and must scale read and write capacity as quickly as possible to meet changes in user demand.  
  
Which solution will meet these requirements?

- A. Host static content in Amazon S3. Host dynamic content by using Amazon API Gateway and AWS Lambda. Use Amazon DynamoDB with on-demand capacity for the database. Configure Amazon CloudFront to deliver the website content.
- B. Host static content in Amazon S3. Host dynamic content by using Amazon API Gateway and AWS Lambda. Use Amazon Aurora with Aurora Auto Scaling for the database. Configure Amazon CloudFront to deliver the website content.
- C. Host all the website content on Amazon EC2 instances. Create an Auto Scaling group to scale the EC2 instances. Use an Application Load Balancer to distribute traffic. Use Amazon DynamoDB with provisioned write capacity for the database.
- D. Host all the website content on Amazon EC2 instances. Create an Auto Scaling group to scale the EC2 instances. Use an Application Load Balancer to distribute traffic. Use Amazon Aurora with Aurora Auto Scaling for the database.
```


정답 : `A`

- 서버 유지관리 및 패치를 최소화해야 하므로 서버리스 서비스(API Gateway, Lambda, S3, CloudFront, DynamoDB)를 사용하면 운영 부담이 거의 없음
- 읽기, 쓰기 용량을 가장 신속하게 자동 확장하려면 DynamoDB 온디맨드가 가장 적합. 온디맨드 모드는 사전 용량 프로비저닝 없이 트래픽 변화에 즉시 대응하며 짧은 시간에 급격히 증가하는 읽기/쓰기에도 자동으로 스케일
- S3 + CloudFront는 정적 콘텐츠를 전 세계에 빠르게 제공하고, API Gateway + Lambda는 동적 요청을 관리하는 완전관리형(서버리스) 패턴으로, 고가용성/무관리 운영 측면에서 최적

오답 이유

- **B. (Aurora + Lambda/API Gateway + S3/CloudFront)**
    - Aurora는 관리형 RDB로 읽기 확장(리더 복제)은 가능하고 Aurora Auto Scaling(리더 추가)은 제공하지만, **쓰기 스케일(클러스터의 쓰기 처리량)은 인스턴스 및 스토리지 설계의 제약**을 받습니다. 또한 RDS(및 Aurora)는 자동 패치가 있더라도 관리형 DB 인스턴스 운영(백업/모니터링/튜닝 등) 필요성이 서버리스보다 큽니다. 요구한 “서버 유지관리 및 패치 최소화”와 “읽기·쓰기 용량을 가능한 한 빨리 확장”을 동시에 만족시키는 면에서 DynamoDB 온디맨드만큼 즉시성과 무관리성을 제공하지 않습니다.
- **C. (EC2 호스팅 + ALB + DynamoDB 프로비저닝)**
    - EC2 기반은 서버(운영체제) 유지·관리 및 패치가 필요하여 운영 부담이 큽니다. 또한 DynamoDB를 프로비저닝 모드로 사용하면 급증하는 트래픽에 대해 사전 용량 조정이 필요하여 자동 즉시 확장성 측면에서 불리합니다(프로비저닝·오토스케일링 설정이 가능하나 온디맨드보다 운영 개입 필요).
- **D. (EC2 호스팅 + Aurora Auto Scaling)**
    - 마찬가지로 EC2 기반 호스팅은 서버 유지관리와 패치 부담 존재. Aurora는 관리형 DB지만 앞서 언급한 쓰기 확장성과 운영 부담 측면에서 DynamoDB 온디맨드 대비 요구사항에 덜 적합합니다.


## #184
한 회사는 소프트웨어 엔지니어링을 위해 사용되는 AWS 계정을 보유하고 있습니다. 해당 AWS 계정은 AWS Direct Connect 연결 쌍을 통해 회사의 온프레미스 데이터 센터에 액세스할 수 있습니다. 모든 비-VPC 트래픽은 가상 프라이빗 게이트웨이(virtual private gateway)로 라우팅됩니다.

개발 팀이 최근 콘솔을 통해 AWS Lambda 함수를 생성했습니다. 개발 팀은 해당 함수가 회사 데이터 센터의 프라이빗 서브넷에서 실행 중인 데이터베이스에 접근할 수 있도록 해야 합니다.

어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. Lambda 함수를 적절한 보안 그룹과 함께 VPC에서 실행되도록 구성합니다.  
B. AWS에서 데이터 센터로의 VPN 연결을 설정합니다. Lambda 함수의 트래픽을 VPN을 통해 라우팅합니다.  
C. Lambda 함수가 Direct Connect를 통해 온프레미스 데이터 센터에 액세스할 수 있도록 VPC의 라우트 테이블을 업데이트합니다.  
D. 탄력적 IP 주소(Elastic IP address)를 생성합니다. 탄력적 네트워크 인터페이스 없이 Lambda 함수가 탄력적 IP 주소를 통해 트래픽을 전송하도록 구성합니다.

```
A company has an AWS account used for software engineering. The AWS account has access to the company’s on-premises data center through a pair of AWS Direct Connect connections. All non-VPC traffic routes to the virtual private gateway.  
  
A development team recently created an AWS Lambda function through the console. The development team needs to allow the function to access a database that runs in a private subnet in the company’s data center.  
  
Which solution will meet these requirements?

- A. Configure the Lambda function to run in the VPC with the appropriate security group.
- B. Set up a VPN connection from AWS to the data center. Route the traffic from the Lambda function through the VPN.
- C. Update the route tables in the VPC to allow the Lambda function to access the on-premises data center through Direct Connect.
- D. Create an Elastic IP address. Configure the Lambda function to send traffic through the Elastic IP address without an elastic network interface.
```

정답 : `A`

- 람다 함수가 온프레미스에 있는 프라이빗 데이터베이스에 접근하려면 람다를 VPC 내부에서 실행하도록 구성해야함
- 람다에 VPC 서브넷(프라이빗 서브넷 권장)과 적절한 보안 그룹을 지정하면 AWS가 함수 실행을 위해 ENI(탄력적 네트워크 인터페이스)를 생성하고 그 ENI를 통해 VPC 라우팅을 사용
- 이미 구성된 Direct Connect + 가상 프라이빗 게이트웨이(VGW) 경로가 있고 서브넷의 라우트 테이블에 온프레미스 대상으로 향하는 경로가 있다면, 람다의 트래픽은 VPC 라우팅을 통해 VGW -> Direct Connect로 전달되어 온프레미스 데이터베이스에 도달

오답 이유

- **B. VPN 연결을 설정**
    - 이미 Direct Connect가 존재하므로 반드시 필요한 조치는 아닙니다. VPN을 추가하면 이중 연결이 될 수는 있으나(재해복구용) 문제에서 요구한 “Lambda가 온프레 DB에 접근”을 위해 반드시 새 VPN을 설정할 필요는 없습니다. 또한 VPN을 설정하더라도 Lambda가 VPC와 연동되지 않으면 VPC 라우팅을 통해 온프레로 나갈 수 없습니다.
    
- **C. 라우트 테이블만 업데이트**
    - VPC의 라우트 테이블에 온프레로 가는 경로를 추가하는 것은 필요 조건이지만 **Lambda가 VPC에 연결되어 있지 않다면** 해당 라우팅을 사용하지 못합니다. 즉, 라우트 테이블 변경만으로는 충분하지 않습니다. Lambda를 VPC에 연결하여 ENI가 생성되어야 라우팅이 적용됩니다.
    
- **D. Elastic IP를 Lambda에 사용**
    - Lambda 함수는 콘솔에서 VPC에 연결하지 않는 한 고정 퍼블릭 EIP를 직접 갖지 않습니다. “탄력적 네트워크 인터페이스 없이 EIP를 통해 트래픽을 보낸다”는 구성은 불가능하거나 비표준적입니다. 또한 EIP를 통해 인터넷 경유로 온프레에 접근하면 트래픽이 인터넷을 통과할 수 있어 보안/요구사항에 맞지 않습니다.


## #185
한 회사는 Amazon ECS를 사용하여 애플리케이션을 실행하고 있습니다.  
이 애플리케이션은 원본 이미지의 크기를 조정한 버전을 생성한 다음, Amazon S3 API 호출을 통해 조정된 이미지를 Amazon S3에 저장합니다.  

솔루션스 아키텍트는 애플리케이션이 Amazon S3에 접근할 권한을 갖도록 어떻게 보장할 수 있습니까?  

A. AWS IAM에서 S3 역할을 업데이트하여 Amazon ECS에서의 읽기/쓰기 접근을 허용한 다음 컨테이너를 다시 실행합니다.  
B. S3 권한을 가진 IAM 역할을 생성한 다음, 해당 역할을 작업 정의(task definition)의 taskRoleArn으로 지정합니다.  
C. Amazon ECS에서 Amazon S3로의 접근을 허용하는 보안 그룹을 생성하고, ECS 클러스터에서 사용하는 런치 구성을 업데이트합니다.  
D. S3 권한을 가진 IAM 사용자를 생성한 다음, 이 계정으로 로그인하여 ECS 클러스터의 Amazon EC2 인스턴스를 다시 실행합니다.

```
A company runs an application using Amazon ECS. The application creates resized versions of an original image and then makes Amazon S3 API calls to store the resized images in Amazon S3.  
  
How can a solutions architect ensure that the application has permission to access Amazon S3?

- A. Update the S3 role in AWS IAM to allow read/write access from Amazon ECS, and then relaunch the container.
- B. Create an IAM role with S3 permissions, and then specify that role as the taskRoleArn in the task definition.
- C. Create a security group that allows access from Amazon ECS to Amazon S3, and update the launch configuration used by the ECS cluster.
- D. Create an IAM user with S3 permissions, and then relaunch the Amazon EC2 instances for the ECS cluster while logged in as this account.
```

정답 : `B`

- ECS에서 실행되는 컨테이너(Task)가 AWS 리소스(S3 등)에 접근할 수 있도록 하려면, IAM 역할을 생성하고 taskRoleArn에 지정
- 이렇게 하면 컨테이너 내에서 실행되는 애플리케이션이 AWS SDK/API를 사용할 때 자동으로 임시 자격 증명(STS)을 받아서 S3 API 호출 가능

오답 이유

- **A. IAM 역할 업데이트 후 컨테이너 재실행**
    - ECS 자체는 “ECS 서비스 역할”을 사용하지만, 컨테이너(Task)가 직접 S3를 호출하려면 **taskRoleArn**을 지정해야 합니다. 단순히 IAM 역할만 업데이트한다고 컨테이너 애플리케이션이 권한을 가지지 않습니다.
    
- **C. 보안 그룹 생성**
    - S3는 보안 그룹으로 제어할 수 있는 서비스가 아닙니다(보안 그룹은 VPC 내부 인스턴스 간 트래픽 제어용). S3 접근은 IAM 권한으로 제어해야 합니다. 따라서 잘못된 접근 방식입니다.
    
- **D. IAM 사용자 생성 후 EC2 인스턴스에서 로그인**
    - ECS 컨테이너가 IAM 사용자 자격 증명을 직접 쓰도록 하는 것은 비권장 방식이며 보안적으로 위험합니다. 또한 ECS 클러스터에 로그인 계정이 영향을 주는 구조가 아닙니다.


## #186
한 회사는 Windows 기반 애플리케이션을 AWS로 마이그레이션해야 합니다. 이 애플리케이션은 여러 가용 영역(AZ)에 배포된 여러 Amazon EC2 Windows 인스턴스에 연결되는 공유 Windows 파일 시스템을 사용해야 합니다.

이 요구사항을 충족하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?

A. AWS Storage Gateway를 볼륨 게이트웨이 모드로 구성합니다. 각 Windows 인스턴스에 볼륨을 마운트합니다.  
B. Amazon FSx for Windows File Server를 구성합니다. 각 Windows 인스턴스에 Amazon FSx 파일 시스템을 마운트합니다.  
C. Amazon Elastic File System(Amazon EFS)으로 파일 시스템을 구성합니다. 각 Windows 인스턴스에 EFS 파일 시스템을 마운트합니다.  
D. 필요한 크기의 Amazon Elastic Block Store(Amazon EBS) 볼륨을 구성합니다. 각 EC2 인스턴스에 볼륨을 연결합니다. 볼륨 내에 파일 시스템을 만들어 각 Windows 인스턴스에 마운트합니다.

```
A company has a Windows-based application that must be migrated to AWS. The application requires the use of a shared Windows file system attached to multiple Amazon EC2 Windows instances that are deployed across multiple Availability Zone:  
  
What should a solutions architect do to meet this requirement?

- A. Configure AWS Storage Gateway in volume gateway mode. Mount the volume to each Windows instance.
- B. Configure Amazon FSx for Windows File Server. Mount the Amazon FSx file system to each Windows instance.
- C. Configure a file system by using Amazon Elastic File System (Amazon EFS). Mount the EFS file system to each Windows instance.
- D. Configure an Amazon Elastic Block Store (Amazon EBS) volume with the required size. Attach each EC2 instance to the volume. Mount the file system within the volume to each Windows instance.
```

정답 : `B`

- Amazon FSx for Windows File Server는 네이티브 Windows 파일 서버(SMB 프로토콜)를 완전관리형으로 제공하며, Active Directory 통합, 파일 권한(NTFS ACL) 지원, 고가용성(Windows 파일 서버의 멀티-AZ 배포 옵션)을 제공
- 따라서 여러 Windows 인스턴스가 동일한 SMB 파일 공유를 마운트하여 사용해야 하는 시나리오에 적합

오답 이유

- **A. AWS Storage Gateway (볼륨 게이트웨이 모드)**
    - 볼륨 게이트웨이는 iSCSI 블록 볼륨을 온프레/VM에 제공하고 스냅샷을 S3/EBS에 저장하는 용도입니다. 여러 EC2 인스턴스가 동일 블록 장치를 동시에 마운트해 공유 파일시스템처럼 사용하는 것은 데이터 손상 위험이 있어 적합하지 않습니다. 또한 Windows 네이티브 SMB 공유를 제공하지 않습니다.
    
- **C. Amazon EFS**
    - EFS는 NFS 기반의 파일 시스템으로 Linux/Unix 워크로드에 적합합니다. Windows에서는 기본적으로 NFS 클라이언트를 통해 접근하는 방법이 복잡하고(Windows에서 NFS는 비권장/제한적), NTFS ACL과 같은 Windows 파일 시스템 기능을 제공하지 않으므로 Windows 애플리케이션용 공유 파일시스템 요구를 충족하지 못합니다.
    
- **D. EBS 볼륨을 여러 인스턴스에 연결**
    - EBS 볼륨(일반적)은 단일 EC2 인스턴스에만 읽기/쓰기 방식으로 연결 가능합니다. 여러 인스턴스가 동시에 동일 EBS를 마운트하면 데이터 손상 발생하므로 불가(다중-attach io2/volume의 경우도 제한적이고 Windows 파일 서버 공유 시나리오에 적합하지 않음). 또한 AZ 간 공유가 불가합니다.


## #187
한 회사는 로드 밸런싱된 프런트 엔드, 컨테이너 기반 애플리케이션, 관계형 데이터베이스로 구성될 전자상거래 애플리케이션을 개발하고 있습니다. 솔루션스 아키텍트는 가능한 한 적은 수동 개입으로 작동하는 고가용성 솔루션을 만들어야 합니다.

어떤 솔루션이 이러한 요구사항을 충족합니까? (두 가지 선택)

A. 다중 가용 영역(Multi-AZ) 모드에서 Amazon RDS DB 인스턴스를 생성합니다.  
B. Amazon RDS DB 인스턴스를 생성하고 다른 가용 영역에 하나 이상의 리플리카를 생성합니다.  
C. 동적 애플리케이션 부하를 처리하기 위해 Amazon EC2 인스턴스 기반 Docker 클러스터를 생성합니다.  
D. 동적 애플리케이션 부하를 처리하기 위해 Amazon Elastic Container Service(Amazon ECS) 클러스터를 Fargate 런치 타입으로 생성합니다.  
E. 동적 애플리케이션 부하를 처리하기 위해 Amazon Elastic Container Service(Amazon ECS) 클러스터를 Amazon EC2 런치 타입으로 생성합니다.

```
A company is developing an ecommerce application that will consist of a load-balanced front end, a container-based application, and a relational database. A solutions architect needs to create a highly available solution that operates with as little manual intervention as possible.  
  
Which solutions meet these requirements? (Choose two.)

- A. Create an Amazon RDS DB instance in Multi-AZ mode.
- B. Create an Amazon RDS DB instance and one or more replicas in another Availability Zone.
- C. Create an Amazon EC2 instance-based Docker cluster to handle the dynamic application load.
- D. Create an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type to handle the dynamic application load.
- E. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type to handle the dynamic application load.
```

정답 : `A, D`

- A - Amazon RDS Multi-AZ
	- RDS의 멀티-AZ 배포는 관리형으로 동기적 스탠바이 복제를 제공하며 기본 인스턴스에 장애가 발생하면 자동 페일오버 수행
	- 백업/패치/모니터링 등 운영 작업도 RDS가 관리하므로 수동 개입이 최소화
	- 관계형 데이터베이스의 고가용성을 가장 적은 운영 부담으로 제공하는 표준적인 방법
- D - Amazon ECS on AWS Fargate
	- Fargate 런치 타입을 사용하면 컨테이너를 실행하기 위한 EC2 인스턴스의 프로비저닝/패치/용량 관리를 사용자가 직접 할 필요가 없음
	- ECS 서비스 및 Fargate는 필요 시 컨테이너 인스턴스를 자동으로 재시작/스케일링하고, ALB와 통합해 고가용성 구성을 쉽게 만들 수 있음
	- 따라서 수동 개입을 크게 줄이면서 컨테이너 기반 애플리케이션을 안정적으로 운영 가능

오답 이유

- **B — RDS + 리플리카(다른 AZ)**
    - 읽기 확장을 위해 리플리카는 유용하지만, RDS 읽기 리플리카는 **일반적으로 비동기 복제**이며 자동 페일오버(마스터 장애 시 자동 승격)를 보장하지 않습니다. 프로덕션 쓰기 장애 시 자동 무중단 전환이 필요하면 Multi-AZ가 더 적합합니다. 리플리카는 읽기 분산 또는 리전 간 복제 용도로는 좋지만 “최소 수동 개입으로 고가용성” 요구에는 Multi-AZ가 더 적절합니다.
    
- **C — EC2 인스턴스 기반 Docker 클러스터**
    - EC2 기반으로 컨테이너를 운영하면 인스턴스의 패치, 용량 계획, AMI/OS 관리 등 **운영 오버헤드가 증가**합니다. 문제 요구는 “가능한 한 적은 수동 개입”이므로, EC2 관리형 솔루션은 부적합합니다.
    
- **E — ECS with EC2 launch type**
    - ECS(EC2 런치 타입)는 컨테이너 오케스트레이션을 제공하지만 **호스트 인스턴스(EC2)의 패치·스케일·용량관리**를 사용자가 수행해야 합니다. 운영 오버헤드를 최소화하려면 Fargate(서버리스 컨테이너)가 더 적합합니다.



## #188
한 회사는 Amazon S3를 데이터 레이크로 사용하고 있습니다. 새 파트너가 SFTP를 사용하여 데이터 파일을 업로드해야 합니다. 솔루션스 아키텍트는 운영 오버헤드를 최소화하면서 고가용성 SFTP 솔루션을 구현해야 합니다.

어떤 솔루션이 이러한 요구사항을 충족합니까?

A. AWS Transfer Family를 사용하여 공용 액세스 가능한 엔드포인트로 SFTP 사용이 가능한 서버를 구성합니다. 대상은 S3 데이터 레이크로 선택합니다.  
B. Amazon S3 File Gateway를 SFTP 서버로 사용합니다. S3 File Gateway 엔드포인트 URL을 새 파트너에게 노출합니다. S3 File Gateway 엔드포인트를 새 파트너와 공유합니다.  
C. VPC의 프라이빗 서브넷에 Amazon EC2 인스턴스를 시작합니다. 새 파트너에게 VPN을 통해 EC2 인스턴스로 파일을 업로드하도록 지시합니다. EC2 인스턴스에서 cron 작업 스크립트를 실행하여 파일을 S3 데이터 레이크로 업로드합니다.  
D. VPC의 프라이빗 서브넷에 Amazon EC2 인스턴스를 시작합니다. EC2 인스턴스 앞에 Network Load Balancer(NLB)를 배치합니다. NLB에 대한 SFTP 수신 포트를 생성합니다. NLB 호스트 이름을 새 파트너와 공유합니다. EC2 인스턴스에서 cron 작업 스크립트를 실행하여 파일을 S3 데이터 레이크로 업로드합니다.

```
A company uses Amazon S3 as its data lake. The company has a new partner that must use SFTP to upload data files. A solutions architect needs to implement a highly available SFTP solution that minimizes operational overhead.  
  
Which solution will meet these requirements?

- A. Use AWS Transfer Family to configure an SFTP-enabled server with a publicly accessible endpoint. Choose the S3 data lake as the destination.
- B. Use Amazon S3 File Gateway as an SFTP server. Expose the S3 File Gateway endpoint URL to the new partner. Share the S3 File Gateway endpoint with the new partner.
- C. Launch an Amazon EC2 instance in a private subnet in a VPInstruct the new partner to upload files to the EC2 instance by using a VPN. Run a cron job script, on the EC2 instance to upload files to the S3 data lake.
- D. Launch Amazon EC2 instances in a private subnet in a VPC. Place a Network Load Balancer (NLB) in front of the EC2 instances. Create an SFTP listener port for the NLB. Share the NLB hostname with the new partner. Run a cron job script on the EC2 instances to upload files to the S3 data lake.
```

정답 : `A`

- AWS Transfer Family (SFTP)는 S3를 대상으로 하는 완전관리형 SFTP 서비스로, SFTP/FTPS/FTP를 지원하며 엔드포인트는 AWS가 관리하므로 고가용성이 기본 제공
- 운영자가 서버 패치/가용성/스케일링을 직접 관리할 필요가 없어 운영 오버헤드 최소화
- Transfer Family는 S3 버킷을 직접 대상(데이터 레이크)로 설정할 수 있고, 사용자 인증(서비스 관리형, IAM, LDAP/Active Directory 연동 등), 로깅(Amazon CloudWatch, S3 액세스 로그) 및 VPC엔드포인트(프라이빗 연결) 옵션을 제공해 보안/감사 요건 충족이 쉬움

오답 이유

- **B. S3 File Gateway를 SFTP 서버로 사용**
    - S3 File Gateway는 온프레/애플리케이션에서 S3에 액세스하도록 NFS/SMB 파일 인터페이스(파일 게이트웨이) 또는 볼륨 게이트웨이 등을 제공하지만, **SFTP 프로토콜을 네이티브로 제공하지 않습니다**. 따라서 파트너가 SFTP로 업로드하려면 별도의 SFTP 서버가 필요하며 본 요구사항(운영 최소화·관리형 고가용성)을 충족하지 못합니다.
    
- **C. EC2 + VPN + cron 스크립트**
    - 동작은 가능하나 **고가용성·확장성·운영 부담**(인스턴스 패치, 모니터링, VPN 관리, 스크립트 오류 처리 등)이 큽니다. 또한 파트너가 VPN으로 연결해야 하는 운영·관리 제약이 있고 관리 부담이 증가합니다.
    
- **D. EC2 + NLB + cron 스크립트**
    - NLB는 TCP 레벨로 트래픽을 전달할 수 있지만, **SFTP 서버의 고가용성·스케일링·패치와 데이터 전송 신뢰성(재시도, 무결성 등)**을 직접 운영해야 합니다. 또한 cron 기반 반복 업로드/정리 로직과 장애 복구를 직접 관리해야 하므로 운영 오버헤드가 큽니다.


## #189
한 회사는 계약 문서를 저장해야 합니다. 계약은 5년 동안 유효합니다. 5년 기간 동안 회사는 문서를 덮어쓰거나 삭제하지 못하도록 보장해야 합니다. 회사는 문서를 저장 시 암호화해야 하며 암호화 키를 매년 자동으로 교체(로테이션)해야 합니다.

이 요구사항을 가장 적은 운영 오버헤드로 충족하기 위해 솔루션스 아키텍트가 취해야 할 단계의 조합은 무엇입니까? (두 개 선택)

A. Amazon S3에 문서를 저장합니다. S3 Object Lock을 거버넌스 모드(governance mode)로 사용합니다.  
B. Amazon S3에 문서를 저장합니다. S3 Object Lock을 컴플라이언스 모드(compliance mode)로 사용합니다.  
C. Amazon S3 관리형 암호화 키(SSE-S3)를 사용한 서버 측 암호화를 사용합니다. 키 회전을 구성합니다.  
D. AWS Key Management Service(AWS KMS) 고객 관리형 키(customer managed keys)를 사용한 서버 측 암호화를 사용합니다. 키 회전을 구성합니다.  
E. AWS Key Management Service(AWS KMS) 고객 제공(가져온) 키(customer provided / imported keys)를 사용한 서버 측 암호화를 사용합니다. 키 회전을 구성합니다.

```
A company needs to store contract documents. A contract lasts for 5 years. During the 5-year period, the company must ensure that the documents cannot be overwritten or deleted. The company needs to encrypt the documents at rest and rotate the encryption keys automatically every year.  
  
Which combination of steps should a solutions architect take to meet these requirements with the LEAST operational overhead? (Choose two.)

- A. Store the documents in Amazon S3. Use S3 Object Lock in governance mode.
- B. Store the documents in Amazon S3. Use S3 Object Lock in compliance mode.
- C. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure key rotation.
- D. Use server-side encryption with AWS Key Management Service (AWS KMS) customer managed keys. Configure key rotation.
- E. Use server-side encryption with AWS Key Management Service (AWS KMS) customer provided (imported) keys. Configure key rotation.
```

정답 : `B, D`

- 문서를 5년동안 절대 삭제/덥어쓰지 못하게 -> S3 Object Lock의 컴플라이언스 모드 사용
	- 컴플라이언스 모드는 보관 기간 동안 어떤 사용자(루트 포함)도 객체를 삭제하거나 버전 취소(누락)할 수 없도록 하며, 규정 준수 요구를 충족하는 가장 강력한 보호 제공
	- 거버넌스 모드는 권한을 가진 사용자가 삭제를 우회할 수 있으므로 요구사항에 부합하지 않음
- 암호화 키를 매년 자동회전 -> AWS KMS 고객 관리형 키(CMK) 사용
	- KMS CMK는 키 회전 옵션(enableKeyRotation)을 활성화하면 AWS가 연간 자동으로 키를 롤(over)하도록 지원
	- SSE-KMS(즉, S3의 서버사이드 암호화에 KMS CMK 사용)는 S3 객체를 암호화하면서 KMS의 자동 키 회전을 활용할 수 있어 최소 운영 오버헤드 요구조건 충족

오답 이유

- **A. S3 Object Lock — 거버넌스 모드**
    - 거버넌스 모드는 보통 삭제를 방지하지만, 특정 IAM 권한을 가진 사용자(예: s3:BypassGovernanceRetention 권한)가 보관 기간을 우회할 수 있습니다. 문제 요구는 “5년 동안 문서를 덮어쓰거나 삭제하지 못하도록 보장”하는 것으로, 완전한 불변(immutable) 보장을 원하므로 컴플라이언스 모드가 더 적합합니다.
    
- **C. SSE-S3(아마존 S3 관리형 키) + 키 회전 구성**
    - SSE-S3는 AWS가 키를 관리(백그라운드에서 키를 교체)하지만 고객이 연 단위 자동 회전 정책을 직접 구성·제어하는 메커니즘을 제공하지 않습니다. 또한 문제에서 ‘키를 매년 자동으로 회전’이라는 요구는 고객이 키 회전 정책을 직접 활성화·관리할 수 있는 KMS CMK가 더 적합합니다.
    
- **E. KMS 고객 제공(임포트) 키(Imported keys)**
    - 가져온 키(CMK 외부에서 업로드한 키)는 자동 회전 기능을 AWS KMS에서 제공하지 않습니다. 키 로테이션을 자동으로 수행하려면 KMS가 자체적으로 관리하는 고객 관리형 키(CMK)를 사용해야 합니다. 수동으로 키를 교체·업로드하면 운영 오버헤드가 커집니다.


## #190
한 회사는 Java와 PHP 기반의 웹 애플리케이션을 보유하고 있습니다. 회사는 이 애플리케이션을 온프레미스에서 AWS로 이전하려고 합니다.  
회사는 새로운 사이트 기능을 자주 테스트할 수 있는 능력이 필요합니다. 또한 고가용성이 보장되며 관리형이고 운영 오버헤드가 최소인 솔루션이 필요합니다.  

어떤 솔루션이 이러한 요구사항을 충족합니까?

A. Amazon S3 버킷을 생성합니다. S3 버킷에서 정적 웹 호스팅을 활성화합니다. 정적 콘텐츠를 S3 버킷에 업로드합니다. 모든 동적 콘텐츠를 처리하기 위해 AWS Lambda를 사용합니다.  

B. 웹 애플리케이션을 AWS Elastic Beanstalk 환경에 배포합니다. 기능 테스트를 위해 여러 Elastic Beanstalk 환경 간에 URL 스와핑(URL swapping)을 사용합니다.  

C. Java와 PHP로 구성된 Amazon EC2 인스턴스에 웹 애플리케이션을 배포합니다. Auto Scaling 그룹과 Application Load Balancer를 사용하여 웹사이트의 가용성을 관리합니다.  

D. 웹 애플리케이션을 컨테이너화합니다. 웹 애플리케이션을 Amazon EC2 인스턴스에 배포합니다. AWS Load Balancer Controller를 사용하여 새로운 사이트 기능을 포함하는 컨테이너 간의 트래픽을 동적으로 라우팅합니다.  

```
A company has a web application that is based on Java and PHP. The company plans to move the application from on premises to AWS. The company needs the ability to test new site features frequently. The company also needs a highly available and managed solution that requires minimum operational overhead.  
  
Which solution will meet these requirements?

- A. Create an Amazon S3 bucket. Enable static web hosting on the S3 bucket. Upload the static content to the S3 bucket. Use AWS Lambda to process all dynamic content.
- B. Deploy the web application to an AWS Elastic Beanstalk environment. Use URL swapping to switch between multiple Elastic Beanstalk environments for feature testing.
- C. Deploy the web application to Amazon EC2 instances that are configured with Java and PHP. Use Auto Scaling groups and an Application Load Balancer to manage the website’s availability.
- D. Containerize the web application. Deploy the web application to Amazon EC2 instances. Use the AWS Load Balancer Controller to dynamically route traffic between containers that contain the new site features for testing.
```

정답 : `B`

- Elastic Beanstalk는 Java, PHP와 같은 런타임을 기본 지원하는 완전 관리형 PaaS 서비스
- 인프라 관리(오토 스케일링, 로드 밸런싱, 패치 등)를 자동으로 처리하여 운영 오버헤드가 최소화
- URL 스와핑을 통해 Blue/Green 배포 방식처럼 새로운 기능을 손쉽게 테스트하고 트래픽을 전환할 수 있어, 문제의 "새로운 사이트 기능을 자주 테스트" 요구에 적합

오답 이유

- **A. S3 + Lambda**
    - Java/PHP 기반의 동적 애플리케이션을 S3 정적 웹 호스팅으로는 직접 실행할 수 없습니다. Lambda로 모든 동적 처리를 하는 것도 복잡하고 관리 오버헤드가 커짐.
    
- **C. EC2 + Auto Scaling + ALB**
    - 고가용성은 제공되지만, 인스턴스 구성/운영/패치 관리가 필요해 **운영 오버헤드가 커짐**. 또한 새 기능을 자주 테스트하려면 배포 파이프라인을 직접 관리해야 함.
    
- **D. EC2 + 컨테이너 + Load Balancer Controller**
    - 컨테이너화는 가능하지만, ECS나 EKS 같은 관리형 컨테이너 서비스가 아닌 EC2에 직접 배포하면 인프라 관리 부담이 큼. 문제에서 요구하는 “최소 운영 오버헤드”와 맞지 않음.


## #191
한 회사는 고객 정보를 Amazon RDS for MySQL에 저장하는 주문 애플리케이션을 운영하고 있습니다.  
영업시간 동안 직원들은 보고 목적의 일회성 쿼리를 실행합니다.  
보고 쿼리가 오래 실행되어 주문 처리 중에 **타임아웃**이 발생하고 있습니다.  
회사는 직원이 쿼리를 수행하지 못하게 하지 않으면서 **타임아웃을 제거**하고자 합니다.  

이 요구사항을 충족하기 위해 솔루션 아키텍트가 취해야 할 조치는 무엇입니까?

A. 읽기 전용 복제본(Read Replica)을 생성합니다. 보고 쿼리를 읽기 전용 복제본에서 실행합니다.  
B. 읽기 전용 복제본(Read Replica)을 생성합니다. 주문 애플리케이션을 기본 DB 인스턴스와 읽기 전용 복제본에 분산합니다.  
C. 주문 애플리케이션을 온디맨드 용량을 사용하는 Amazon DynamoDB로 마이그레이션합니다.  
D. 보고 쿼리를 피크가 아닌 시간으로 예약합니다.  

```
A company has an ordering application that stores customer information in Amazon RDS for MySQL. During regular business hours, employees run one-time queries for reporting purposes. Timeouts are occurring during order processing because the reporting queries are taking a long time to run. The company needs to eliminate the timeouts without preventing employees from performing queries.  
  
What should a solutions architect do to meet these requirements?

- A. Create a read replica. Move reporting queries to the read replica.
- B. Create a read replica. Distribute the ordering application to the primary DB instance and the read replica.
- C. Migrate the ordering application to Amazon DynamoDB with on-demand capacity.
- D. Schedule the reporting queries for non-peak hours.
```

정답 : `A`

- 문제 요점 : 보고용 쿼리가 RDS MySQL 기본 인스턴스의 성능을 방해하여 주문 처리 타임 아웃 발생
- 읽기 전용 복제본(Read Replica)은 쓰기 지연 없이 읽기 전용 쿼리를 별도의 DB 인스턴스에서 실행할 수 있음
- 직원들은 복제본에서 보고 쿼리를 실행할 수 있고, 주문 처리 트랜잭션은 기본 인스턴스에서 영향을 받지 않음

오답 이유

- **B. 읽기 전용 복제본 + 애플리케이션 분산**
    - 주문 애플리케이션까지 복제본에서 처리하도록 분산하면 **쓰기 트랜잭션은 복제본에서 처리되지 않음** → 주문 처리에는 도움이 되지 않음.
    
- **C. DynamoDB로 마이그레이션**
    - 기존 MySQL 기반 애플리케이션을 DynamoDB로 마이그레이션하는 것은 **대규모 변경**이 필요하며, 운영 오버헤드가 큼. 문제 요구사항과 맞지 않음.
    
- **D. 보고 쿼리를 비피크 시간에 예약**
    - 임시 해결책일 뿐 **실시간 보고 쿼리를 방지**하지 않으면 타임아웃이 계속 발생 가능.



## #192
한 병원이 방대한 역사적 기록 문서의 디지털 사본을 만들고자 합니다.  
병원은 매일 수백 건의 새로운 문서를 계속 추가할 예정입니다.  
병원의 데이터 팀은 문서를 스캔하고 AWS 클라우드에 업로드합니다.  

솔루션 아키텍트는 다음 요구사항을 충족하는 솔루션을 구현해야 합니다:

- 문서를 분석하고, 의료 정보를 추출하며,  
- 애플리케이션이 데이터에 대해 SQL 쿼리를 실행할 수 있도록 문서를 저장

이 솔루션은 **확장성과 운영 효율성을 극대화**해야 합니다.

이 요구사항을 충족하기 위해 솔루션 아키텍트가 수행해야 할 조치의 조합은 무엇입니까? (두 가지 선택)

A. 문서 정보를 MySQL 데이터베이스가 실행되는 Amazon EC2 인스턴스에 작성합니다.  

B. 문서 정보를 Amazon S3 버킷에 작성합니다. Amazon Athena를 사용하여 데이터를 쿼리합니다.  

C. 스캔된 파일을 처리하고 의료 정보를 추출하는 맞춤 애플리케이션을 실행하는 Amazon EC2 인스턴스의 Auto Scaling 그룹을 생성합니다.  

D. 새 문서가 업로드될 때 실행되는 AWS Lambda 함수를 생성합니다. Amazon Rekognition을 사용하여 문서를 원시 텍스트로 변환하고, Amazon Transcribe Medical을 사용하여 텍스트에서 관련 의료 정보를 감지하고 추출합니다.  

E. 새 문서가 업로드될 때 실행되는 AWS Lambda 함수를 생성합니다. Amazon Textract를 사용하여 문서를 원시 텍스트로 변환하고, Amazon Comprehend Medical을 사용하여 텍스트에서 관련 의료 정보를 감지하고 추출합니다.  

```
A hospital wants to create digital copies for its large collection of historical written records. The hospital will continue to add hundreds of new documents each day. The hospital’s data team will scan the documents and will upload the documents to the AWS Cloud.  
  
A solutions architect must implement a solution to analyze the documents, extract the medical information, and store the documents so that an application can run SQL queries on the data. The solution must maximize scalability and operational efficiency.  
  
Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)

- A. Write the document information to an Amazon EC2 instance that runs a MySQL database.
- B. Write the document information to an Amazon S3 bucket. Use Amazon Athena to query the data.
- C. Create an Auto Scaling group of Amazon EC2 instances to run a custom application that processes the scanned files and extracts the medical information.
- D. Create an AWS Lambda function that runs when new documents are uploaded. Use Amazon Rekognition to convert the documents to raw text. Use Amazon Transcribe Medical to detect and extract relevant medical information from the text.
- E. Create an AWS Lambda function that runs when new documents are uploaded. Use Amazon Textract to convert the documents to raw text. Use Amazon Comprehend Medical to detect and extract relevant medical information from the text.
```

정답 : `B, E`

- B. S3 + Athena
	- S3는 무제한 스토리지 확장성 제공
	- Athena는 S3에 저장된 데이터를 대상으로 SQL 쿼리를 실행할 수 있어 운영 효율성 극대화
- E. Lambda + Textract + Comprehend Medical
	- Lambda로 서버리스 이벤트 기반 처리 구현 -> 새 문서가 업로드될 때 자동 실행
	- Textract: 문서를 원시 텍스트로 변환
	- Comprehend Medical: 외료 관련 정보 추출
	- Ec2 기반 수동 처리보다 높은 확장성과 운영 효율성 제공

오답 이유

- **A. EC2 + MySQL**
    - 문서 데이터 양이 매우 많아지면 EC2/관계형 DB로 처리하는 것은 **확장성 부족**. 운영 오버헤드 큼.
    
- **C. EC2 Auto Scaling 그룹**
    - 맞춤 애플리케이션으로 스캔 및 분석 가능하지만 **관리/운영 오버헤드**가 크고, 서버리스 대비 확장성이 떨어짐.
    
- **D. Rekognition + Transcribe Medical**
    - Rekognition은 이미지/비디오 객체 인식 서비스이고, 텍스트 추출에는 적합하지 않음.
    - 문서 내 의료 정보 추출에는 **Textract + Comprehend Medical**이 올바른 조합임.


## #193
한 회사가 Amazon EC2 인스턴스에서 배치 애플리케이션을 실행하고 있습니다.  
애플리케이션은 여러 Amazon RDS 데이터베이스가 있는 백엔드로 구성됩니다.  
애플리케이션으로 인해 데이터베이스에서 읽기 요청이 많이 발생하고 있습니다.  
솔루션 아키텍트는 데이터베이스 읽기 요청 수를 줄이면서 **높은 가용성**을 보장해야 합니다.

이 요구사항을 충족하려면 솔루션 아키텍트는 무엇을 해야 합니까?

A. Amazon RDS 읽기 전용 복제본(read replicas)을 추가합니다.  
B. Amazon ElastiCache for Redis를 사용합니다.  
C. Amazon Route 53 DNS 캐싱을 사용합니다.  
D. Amazon ElastiCache for Memcached를 사용합니다.  

```
A company is running a batch application on Amazon EC2 instances. The application consists of a backend with multiple Amazon RDS databases. The application is causing a high number of reads on the databases. A solutions architect must reduce the number of database reads while ensuring high availability.  
  
What should the solutions architect do to meet this requirement?

- A. Add Amazon RDS read replicas.
- B. Use Amazon ElastiCache for Redis.
- C. Use Amazon Route 53 DNS caching
- D. Use Amazon ElastiCache for Memcached.
```

정답 : `A(61%)`

- 읽기 전용 복제본을 추가하면 읽기 요청을 여러 DB 인스턴스에 분산 가능 -> 일기 부하 감소
- Multi-AZ 구성 시, 장애가 발생해도 데이터베이스 가용성을 보장 -> 높은 가용성 확보
- 기존 애플리케이션을 거의 변경하지 않고 배포 가능 -> 운영 오버헤드 최소화

오답 이유

## **4. 오답 이유**

- **B. ElastiCache for Redis**
	- 정답 선택률 39%
    - 캐시를 추가하면 읽기 요청 부하를 줄일 수 있으나, 애플리케이션 코드를 캐시를 활용하도록 수정해야 함 → 운영 오버헤드 증가
    - 문제 조건에서 “최소한의 운영 오버헤드”와 “높은 가용성”을 동시에 고려하면 RDS 읽기 복제본이 더 적합
    
- **C. Route 53 DNS 캐싱**
    - DNS 캐시는 IP 주소만 캐싱하므로 DB 읽기 요청 부하 감소와 무관
    
- **D. ElastiCache for Memcached**
    - Memcached는 읽기 캐시로 사용할 수 있으나 고가용성 기능이 제한적이며, 캐시 사용을 위해 애플리케이션 코드 변경이 필요



## #194
한 회사가 AWS에서 중요한 애플리케이션을 실행해야 합니다.  
회사는 애플리케이션의 데이터베이스로 Amazon EC2를 사용해야 합니다.  
데이터베이스는 **고가용성**이어야 하며, 장애나 방해 이벤트가 발생하면 **자동으로 장애 조치(failover)**가 되어야 합니다.

이 요구사항을 충족하려면 어떤 솔루션을 선택해야 합니까?

A. 두 개의 EC2 인스턴스를 시작합니다. 각 인스턴스는 동일한 AWS 리전 내 서로 다른 가용 영역에 배치합니다. 두 EC2 인스턴스에 데이터베이스를 설치합니다. EC2 인스턴스를 클러스터로 구성하고 데이터베이스 복제를 설정합니다.  

B. 한 가용 영역에 EC2 인스턴스를 시작합니다. EC2 인스턴스에 데이터베이스를 설치합니다. Amazon Machine Image(AMI)를 사용하여 데이터를 백업합니다. 장애가 발생하면 AWS CloudFormation을 사용해 EC2 인스턴스를 자동으로 프로비저닝합니다.  

C. 두 개의 EC2 인스턴스를 시작합니다. 각 인스턴스는 서로 다른 AWS 리전에 배치합니다. 두 EC2 인스턴스에 데이터베이스를 설치하고 데이터베이스 복제를 설정합니다. 데이터베이스를 두 번째 리전으로 장애 조치(failover)합니다.  

D. 한 가용 영역에 EC2 인스턴스를 시작합니다. EC2 인스턴스에 데이터베이스를 설치합니다. Amazon Machine Image(AMI)를 사용하여 데이터를 백업합니다. EC2 자동 복구(automatic recovery)를 사용하여 장애가 발생하면 인스턴스를 복구합니다.  

```
A company needs to run a critical application on AWS. The company needs to use Amazon EC2 for the application’s database. The database must be highly available and must fail over automatically if a disruptive event occurs.  
  
Which solution will meet these requirements?

- A. Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Set up database replication.
- B. Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use AWS CloudFormation to automate provisioning of the EC2 instance if a disruptive event occurs.
- C. Launch two EC2 instances, each in a different AWS Region. Install the database on both EC2 instances. Set up database replication. Fail over the database to a second Region.
- D. Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use EC2 automatic recovery to recover the instance if a disruptive event occurs.
```


정답 : `A`

- 동일 리전 내 서로 다른 가용 영역(AZ)에 데이터베이스를 클러스터로 구성하고 복제하면 AZ 장애 시 자동으로 다른 인스턴스로 장애 조치 가능 -> 고가용성 + 자동 장애 조치 충족
- 데이터베이스 복제를 통해 데이터 손실 최소화
- EC2 기반으로 직접 HA 구성 가능 -> 요구사항 충족


## #195
한 회사의 주문 시스템은 클라이언트로부터 요청을 Amazon EC2 인스턴스로 보냅니다.  
EC2 인스턴스는 주문을 처리한 후 Amazon RDS의 데이터베이스에 주문을 저장합니다.  
사용자들은 시스템 장애가 발생하면 주문을 다시 처리해야 한다고 보고합니다.  
회사는 시스템 장애 발생 시 **주문을 자동으로 처리할 수 있는 내결함(resilient) 솔루션**을 원합니다.

이 요구사항을 충족하려면 어떤 솔루션을 선택해야 합니까?

A. EC2 인스턴스를 Auto Scaling 그룹으로 이동합니다. Amazon EventBridge(CloudWatch Events) 규칙을 생성하여 Amazon Elastic Container Service(Amazon ECS) 작업을 대상으로 지정합니다.  

B. EC2 인스턴스를 Auto Scaling 그룹 뒤에 Application Load Balancer(ALB)와 함께 이동합니다. 주문 시스템이 ALB 엔드포인트로 메시지를 보내도록 업데이트합니다.  

C. EC2 인스턴스를 Auto Scaling 그룹으로 이동합니다. 주문 시스템이 Amazon Simple Queue Service(Amazon SQS) 큐로 메시지를 보내도록 구성합니다. EC2 인스턴스가 큐에서 메시지를 소비하도록 구성합니다.  

D. Amazon Simple Notification Service(Amazon SNS) 주제를 생성합니다. AWS Lambda 함수를 생성하고, 함수를 SNS 주제에 구독시킵니다. 주문 시스템이 SNS 주제로 메시지를 보내도록 구성합니다. EC2 인스턴스가 AWS Systems Manager Run Command를 사용하여 메시지를 처리하도록 명령을 보냅니다.

```
A company’s order system sends requests from clients to Amazon EC2 instances. The EC2 instances process the orders and then store the orders in a database on Amazon RDS. Users report that they must reprocess orders when the system fails. The company wants a resilient solution that can process orders automatically if a system outage occurs.  
  
What should a solutions architect do to meet these requirements?

- A. Move the EC2 instances into an Auto Scaling group. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to target an Amazon Elastic Container Service (Amazon ECS) task.
- B. Move the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB). Update the order system to send messages to the ALB endpoint.
- C. Move the EC2 instances into an Auto Scaling group. Configure the order system to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the EC2 instances to consume messages from the queue.
- D. Create an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function, and subscribe the function to the SNS topic. Configure the order system to send messages to the SNS topic. Send a command to the EC2 instances to process the messages by using AWS Systems Manager Run Command.
```

정답 : `C`

- SQS는 메시지를 안전하게 보관하고, 수신자가 처리할 때까지 삭제되지 않음 -> 시스템 장애 발생 시에도 주문 손실 없음
- EC2 인스턴스가 오토 스케일링 그룹으로 구성되면, 가용성을 높이고 장애 시 자동 복구 가능

오답 이유

- **A. EventBridge → ECS**
    - EventBridge는 스케줄 또는 이벤트 기반 트리거용
    - 기존 주문 시스템이 EC2에서 바로 처리하는 구조와 잘 맞지 않고, 주문 재처리 내결함성 보장 어려움
    
- **B. ALB 뒤 Auto Scaling EC2**
    - ALB는 **실시간 요청 라우팅용**
    - 주문 메시지가 손실되면 재처리 불가 → 내결함성 요구사항 미충족
    
- **D. SNS + Lambda + Run Command**
    - SNS는 **발행-구독(pub/sub) 모델**
    - EC2에서 Run Command로 처리하면, 장애 발생 시 메시지가 처리되지 않을 가능성이 있음
    - **자동 재처리 보장 어려움**


## #196
한 회사가 대규모 Amazon EC2 인스턴스 군에서 애플리케이션을 실행하고 있습니다.  
애플리케이션은 Amazon DynamoDB 테이블에 항목을 읽고 쓰기를 수행합니다.  
DynamoDB 테이블의 크기는 지속적으로 증가하지만, 애플리케이션은 **최근 30일 데이터만 필요**합니다.  
회사는 비용과 개발 노력을 최소화하는 솔루션이 필요합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. AWS CloudFormation 템플릿을 사용하여 전체 솔루션을 배포합니다. 30일마다 CloudFormation 스택을 다시 배포하고 원래 스택을 삭제합니다.  

B. AWS Marketplace의 모니터링 애플리케이션을 실행하는 EC2 인스턴스를 사용합니다. 모니터링 애플리케이션을 구성하여 Amazon DynamoDB Streams를 사용해 테이블에 새 항목이 생성될 때 타임스탬프를 저장합니다. EC2 인스턴스에서 실행되는 스크립트를 사용하여 30일보다 오래된 항목을 삭제합니다.  

C. Amazon DynamoDB Streams를 구성하여 테이블에 새 항목이 생성될 때 AWS Lambda 함수를 호출하도록 설정합니다. Lambda 함수를 구성하여 테이블에서 30일보다 오래된 항목을 삭제합니다.  

D. 애플리케이션을 확장하여 테이블에 새 항목이 생성될 때 현재 타임스탬프 + 30일 값을 가지는 속성을 추가합니다. DynamoDB에서 이 속성을 TTL(Time to Live) 속성으로 사용하도록 구성합니다.

```
A company runs an application on a large fleet of Amazon EC2 instances. The application reads and writes entries into an Amazon DynamoDB table. The size of the DynamoDB table continuously grows, but the application needs only data from the last 30 days. The company needs a solution that minimizes cost and development effort.  
  
Which solution meets these requirements?

- A. Use an AWS CloudFormation template to deploy the complete solution. Redeploy the CloudFormation stack every 30 days, and delete the original stack.
- B. Use an EC2 instance that runs a monitoring application from AWS Marketplace. Configure the monitoring application to use Amazon DynamoDB Streams to store the timestamp when a new item is created in the table. Use a script that runs on the EC2 instance to delete items that have a timestamp that is older than 30 days.
- C. Configure Amazon DynamoDB Streams to invoke an AWS Lambda function when a new item is created in the table. Configure the Lambda function to delete items in the table that are older than 30 days.
- D. Extend the application to add an attribute that has a value of the current timestamp plus 30 days to each new item that is created in the table. Configure DynamoDB to use the attribute as the TTL attribute.
```

정답 : `D`

- DynamoDB는 TTL 속성이 설정된 항목을 자동으로 만료 처리
- 항목 삭제가 자동으로 백그라운드에서 수행 -> 운영 및 개발 부담 최소
- 비용 효율적, 서버리스 방식으로 관리 필요 없음

오답 이유

- **A. CloudFormation 스택 30일마다 재배포**
    - 스택 삭제/재배포는 전체 테이블를 삭제해야 하므로 데이터 손실 가능
    - 관리 복잡도 높음, 비용 효율적 아님
    
- **B. EC2 + DynamoDB Streams + 스크립트**
    - EC2 인스턴스 항상 실행 → 추가 비용 발생
    - 스크립트 유지관리 필요 → 개발 부담 ↑
    
- **C. DynamoDB Streams + Lambda**
    - Lambda로 오래된 항목 삭제 가능하지만,
    - 테이블 규모가 커지면 Lambda 호출 수 증가 → 비용 상승
    - 개발/운영 부담 있음


## #197
한 회사가 Microsoft .NET 애플리케이션을 온프레미스 Windows Server에서 실행하고 있습니다.  
애플리케이션은 Oracle Database Standard Edition 서버를 사용하여 데이터를 저장합니다.  
회사는 AWS로 마이그레이션을 계획하고 있으며, 개발 변경을 최소화하면서 애플리케이션을 이동하기를 원합니다.  
AWS 애플리케이션 환경은 **고가용성(Highly Available)**이어야 합니다.

이 요구사항을 충족하는 조합은 무엇입니까? (두 가지 선택)

A. AWS Lambda 함수(.NET Core 실행)로 서버리스로 애플리케이션을 리팩터링합니다.  
B. AWS Elastic Beanstalk에서 .NET 플랫폼으로 애플리케이션을 재호스트(Rehost)하고 Multi-AZ 배포를 구성합니다.  
C. 애플리케이션을 Amazon EC2에서 Amazon Linux AMI로 재플랫폼(Replatform)합니다.  
D. AWS Database Migration Service(AWS DMS)를 사용하여 Oracle 데이터베이스를 Amazon DynamoDB(Multi-AZ 배포)로 마이그레이션합니다.  
E. AWS Database Migration Service(AWS DMS)를 사용하여 Oracle 데이터베이스를 Amazon RDS에서 Oracle(Multi-AZ 배포)로 마이그레이션합니다.

```
A company has a Microsoft .NET application that runs on an on-premises Windows Server. The application stores data by using an Oracle Database Standard Edition server. The company is planning a migration to AWS and wants to minimize development changes while moving the application. The AWS application environment should be highly available.  
  
Which combination of actions should the company take to meet these requirements? (Choose two.)

- A. Refactor the application as serverless with AWS Lambda functions running .NET Core.
- B. Rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment.
- C. Replatform the application to run on Amazon EC2 with the Amazon Linux Amazon Machine Image (AMI).
- D. Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Amazon DynamoDB in a Multi-AZ deployment.
- E. Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle on Amazon RDS in a Multi-AZ deployment.
```

정답 : `B, E`

- B - Elastic Beanstalk .NET Multi-AZ
	- 기존 .NET 애플리케이션을 거의 수정하지 않고 그대로 AWS로 호출 가능
	- 멀티-AZ 배포로 고가용성 제공
	- 관리형 환경 -> 운영 부담 최소화
- E - AWS DMS로 Oracle -> RDS Oracle Multi-AZ
	- 기존 오라클 데이터베이스 구조 유지 가능 -> 개발 변경 최소
	- Amazon RDS Multi-AZ 배포 -> 데이터베이스 고가용성 제공
	- AWS DMS를 이용하면 다운타임 최소화 마이그레이션 가능

오답 이유

- **A. 서버리스 Lambda로 리팩터링**
    - 개발 변경이 매우 크고, 기존 .NET 애플리케이션 구조 변경 필요
    - “개발 변경 최소화” 요구사항 위반
    
- **C. Amazon Linux EC2로 재플랫폼**
    - 기존 Windows 기반 .NET 애플리케이션과 호환되지 않음
    - OS 변경 → 개발/운영 부담 ↑
    
- **D. Oracle → DynamoDB 마이그레이션**
    - 기존 SQL 기반 애플리케이션과 호환되지 않음
    - 개발 변경이 크고, 데이터 모델 변경 필요 → 요구사항 위반


## #198
한 회사가 컨테이너화된 애플리케이션을 온프레미스 데이터센터의 Kubernetes 클러스터에서 실행하고 있습니다.  
회사는 데이터 저장소로 MongoDB를 사용하고 있습니다.  
회사는 일부 환경을 AWS로 마이그레이션하려고 하지만, 현재 시점에서는 코드 변경이나 배포 방식 변경이 불가능합니다.  
회사는 운영 부담을 최소화하는 솔루션이 필요합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. Amazon Elastic Container Service(Amazon ECS)와 Amazon EC2 워커 노드를 컴퓨트에 사용하고, MongoDB를 EC2에서 실행  
B. Amazon Elastic Container Service(Amazon ECS)와 AWS Fargate를 컴퓨트에 사용하고, Amazon DynamoDB를 데이터 저장소로 사용  
C. Amazon Elastic Kubernetes Service(Amazon EKS)와 Amazon EC2 워커 노드를 컴퓨트에 사용하고, Amazon DynamoDB를 데이터 저장소로 사용  
D. Amazon Elastic Kubernetes Service(Amazon EKS)와 AWS Fargate를 컴퓨트에 사용하고, Amazon DocumentDB(MongoDB 호환)를 데이터 저장소로 사용

```
A company runs a containerized application on a Kubernetes cluster in an on-premises data center. The company is using a MongoDB database for data storage. The company wants to migrate some of these environments to AWS, but no code changes or deployment method changes are possible at this time. The company needs a solution that minimizes operational overhead.  
  
Which solution meets these requirements?

- A. Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes for compute and MongoDB on EC2 for data storage.
- B. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute and Amazon DynamoDB for data storage
- C. Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes for compute and Amazon DynamoDB for data storage.
- D. Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and Amazon DocumentDB (with MongoDB compatibility) for data storage.
```

정답 : `D`

- EKS + Fargate + DocumentDB(MongoDB 호환)
	- EKS -> 기존 쿠버네티스 기반 배포 그대로 운영 가능 -> 코드/배포 방식 변경 불필요
	- Fargate -> 서버 관리, 워커 노드 관리 필요 없음 -> 운영 부담 최소화
	- DocumentDB -> MongoDB 호환 데이터베이스 제공 -> 기존 애플리케이션 코드를 변경하지 않고도 사용 가능

오답 이유

- **A. ECS + EC2 + MongoDB on EC2**
    - MongoDB를 직접 EC2에서 운영 → 관리 부담 크고 고가용성 설정 필요
    - 코드 변경은 없지만 운영 부담이 높음
    
- **B. ECS + Fargate + DynamoDB**
    - DynamoDB 사용 시 기존 MongoDB 코드 변경 필요 → “코드 변경 불가” 요구사항 위반
    
- **C. EKS + EC2 + DynamoDB**
    - DynamoDB 사용 시 기존 MongoDB 코드 변경 필요 → 요구사항 위반
    - EC2 운영 부담 존재 → 최소 운영 부담 요구사항 위반


## #199
한 텔레마케팅 회사가 AWS에서 고객 콜센터 기능을 설계하고 있습니다.  
회사는 여러 화자를 인식하고, 전사(transcript) 파일을 생성할 수 있는 솔루션이 필요합니다.  
또한 비즈니스 패턴을 분석하기 위해 전사 파일을 쿼리할 수 있어야 합니다.  
전사 파일은 감사 목적으로 7년간 저장되어야 합니다.  

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. 여러 화자 인식을 위해 Amazon Rekognition 사용. 전사 파일은 Amazon S3에 저장. 전사 파일 분석을 위해 머신러닝 모델 사용  
B. 여러 화자 인식을 위해 Amazon Transcribe 사용. 전사 파일 분석을 위해 Amazon Athena 사용  
C. 여러 화자 인식을 위해 Amazon Translate 사용. 전사 파일은 Amazon Redshift에 저장. 전사 파일 분석을 위해 SQL 쿼리 사용  
D. 여러 화자 인식을 위해 Amazon Rekognition 사용. 전사 파일은 Amazon S3에 저장. 전사 파일 분석을 위해 Amazon Textract 사용

```
A telemarketing company is designing its customer call center functionality on AWS. The company needs a solution that provides multiple speaker recognition and generates transcript files. The company wants to query the transcript files to analyze the business patterns. The transcript files must be stored for 7 years for auditing purposes.  
  
Which solution will meet these requirements?

- A. Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use machine learning models for transcript file analysis.
- B. Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena for transcript file analysis.
- C. Use Amazon Translate for multiple speaker recognition. Store the transcript files in Amazon Redshift. Use SQL queries for transcript file analysis.
- D. Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use Amazon Textract for transcript file analysis.
```

정답 : `B`

- Amazon Transcribe
	- 음성에서 텍스트 전사 가능
	- 여러 화자(Speaker) 인식 가능 -> 멀티스피커 전사 지원
	- 전사 파일을 S3에 저장 가능
- Amazon Athena : S3에 저장된 전사 파일을 쿼리하여 분석 가능
- 7년 보관 -> S3 객체 수명 주기 또는 표준 스토리지로 장기 보관 가능


## #200
한 회사가 AWS에서 애플리케이션을 호스팅하고 있습니다.  
회사는 Amazon Cognito를 사용하여 사용자를 관리합니다.  
사용자가 애플리케이션에 로그인하면 애플리케이션은 Amazon API Gateway에서 호스팅되는 REST API를 사용하여 Amazon DynamoDB에서 필요한 데이터를 가져옵니다.  
회사는 **개발 노력 감소**를 위해 REST API에 대한 접근을 제어할 수 있는 AWS 관리형 솔루션을 원합니다.  

운영 부담이 가장 적은 솔루션은 무엇입니까?

A. 요청한 사용자가 누구인지 확인하기 위해 API Gateway에서 AWS Lambda 함수를 authorizer로 구성  
B. 각 사용자마다 API 키를 생성하고 할당. 요청 시 키를 보내고 AWS Lambda 함수로 키 검증  
C. 매 요청마다 사용자의 이메일 주소를 헤더에 전송. AWS Lambda 함수로 이메일 사용자가 접근 권한이 있는지 검증  
D. Amazon Cognito 사용자 풀 authorizer를 API Gateway에서 구성하여 Amazon Cognito가 각 요청을 검증하도록 허용

```
A company hosts its application on AWS. The company uses Amazon Cognito to manage users. When users log in to the application, the application fetches required data from Amazon DynamoDB by using a REST API that is hosted in Amazon API Gateway. The company wants an AWS managed solution that will control access to the REST API to reduce development efforts.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Configure an AWS Lambda function to be an authorizer in API Gateway to validate which user made the request.
- B. For each user, create and assign an API key that must be sent with each request. Validate the key by using an AWS Lambda function.
- C. Send the user’s email address in the header with every request. Invoke an AWS Lambda function to validate that the user with that email address has proper access.
- D. Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to validate each request.
```

정답 : `D`

- API Gateway + Amazon Cognito user poll authorizer는 AWS 관리형 접근 제어 솔루션
- 개발자가 직접 람다를 구현하지 않아도 Cognito 토큰 기반 인증 및 권한 부여가 자동 처리
- 최소한의 운영 부담으로, 로그인한 사용자가 REST API에 접근할 수 있도록 쉽게 구성 가능
- 추가적인 사용자 관리 로직이나 API 키 발급, Lambda authorizer 코드 작성 필요 없음


오답 이유

- **A. Lambda authorizer 사용**
    - 맞춤형 접근 제어 가능하지만 **개발 및 유지보수 부담이 큼** → 운영 부담 증가
    
- **B. API 키 + Lambda 검증**
    - API 키 관리와 검증 로직 필요 → 개발 및 운영 부담 높음
    
- **C. 이메일 헤더 + Lambda 검증**
    - 이메일 전송, Lambda 검증 로직 필요 → 보안, 개발, 유지보수 부담 증가