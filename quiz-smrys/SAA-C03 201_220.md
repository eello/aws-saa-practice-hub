---
created: 2025-10-01 10:26:13
last_modified: 2025-10-01 12:00:30
---
## #201
한 회사가 모바일 앱 사용자를 대상으로 하는 마케팅 커뮤니케이션 서비스를 개발하고 있습니다. 회사는 사용자에게 단문 메시지 서비스(SMS)를 사용하여 확인 메시지를 보내야 합니다. 사용자는 SMS 메시지에 회신할 수 있어야 합니다. 회사는 분석을 위해 응답을 1년 동안 저장해야 합니다.

이 요구사항을 충족하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?

A. SMS 메시지를 보내기 위해 Amazon Connect 연락 흐름(contact flow)을 생성합니다. 응답을 처리하기 위해 AWS Lambda를 사용합니다.
B. Amazon Pinpoint 여정을 구축합니다. 분석 및 아카이빙을 위해 이벤트를 Amazon Kinesis 데이터 스트림으로 전송하도록 Amazon Pinpoint를 구성합니다.
C. SMS 메시지를 분배하기 위해 Amazon Simple Queue Service(Amazon SQS)를 사용합니다. 응답을 처리하기 위해 AWS Lambda를 사용합니다.
D. Amazon Simple Notification Service(Amazon SNS) FIFO 주제를 생성합니다. 분석 및 아카이빙을 위해 Amazon Kinesis 데이터 스트림을 SNS 주제에 구독시킵니다.

```
A company is developing a marketing communications service that targets mobile app users. The company needs to send confirmation messages with Short Message Service (SMS) to its users. The users must be able to reply to the SMS messages. The company must store the responses for a year for analysis.  
  
What should a solutions architect do to meet these requirements?

- A. Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to process the responses.
- B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.
- C. Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS Lambda to process the responses.
- D. Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving.
```

정답 : `B`

- Amazon Pinpoint는 SMS로 메시지를 보낼 수 있고, 사용자의 회신(reply)도 처리할 수 있는 관리형 서비스
- Pinpoint는 분석 및 마케팅 캠페인 관리에 특화되어 있어, 이벤트 데이터를 Amazon Kinesis Data Streams 또는 Amazon Kinesis Data Firehose로 전송하여 장기 보관 및 분석 가능

오답 이유

- **A. Amazon Connect**
    - Amazon Connect는 콜센터/컨택센터 솔루션으로 전화/채팅 기반입니다. SMS 대량 발송이나 회신 처리 기능은 본질적으로 제공하지 않습니다. 따라서 SMS 기반 마케팅 커뮤니케이션 요구사항에 적합하지 않습니다.
    
- **C. Amazon SQS**
    - SQS는 메시지 큐 서비스로 SMS 발송 기능을 제공하지 않습니다. SMS를 직접 보낼 수 없으므로 요구사항을 충족할 수 없습니다.
    
- **D. Amazon SNS FIFO**
    - SNS는 SMS 발송을 지원하지만 **사용자 회신(reply) 처리 기능**은 제공하지 않습니다. 또한 SNS FIFO 주제를 이용한 이벤트 저장은 가능하지만, 마케팅 캠페인 분석이나 장기 아카이빙에는 적합하지 않습니다.
    - Pinpoint처럼 마케팅 데이터 분석 기능과 회신 수집을 지원하지 않기 때문에 부적합합니다.


## #202
한 회사가 데이터를 Amazon S3 버킷으로 이동하려고 합니다. 데이터는 S3 버킷에 저장될 때 암호화되어야 합니다. 추가로, 암호화 키는 매년 자동으로 교체(로테이션)되어야 합니다.

이 요구사항을 가장 적은 운영 오버헤드로 충족하는 솔루션은 무엇입니까?

A. 데이터를 S3 버킷으로 이동합니다. Amazon S3 관리형 암호화 키(SSE-S3)를 사용한 서버 측 암호화를 사용합니다. SSE-S3 암호화 키의 기본 제공 키 로테이션 동작을 사용합니다.
B. AWS Key Management Service(AWS KMS) 고객 관리형 키를 생성합니다. 자동 키 로테이션을 활성화합니다. S3 버킷의 기본 암호화 동작이 고객 관리형 KMS 키를 사용하도록 설정합니다. 데이터를 S3 버킷으로 이동합니다.
C. AWS Key Management Service(AWS KMS) 고객 관리형 키를 생성합니다. S3 버킷의 기본 암호화 동작이 고객 관리형 KMS 키를 사용하도록 설정합니다. 데이터를 S3 버킷으로 이동합니다. 매년 KMS 키를 수동으로 로테이션합니다.
D. 데이터를 S3 버킷으로 이동하기 전에 고객 키 자료로 데이터를 암호화합니다. 키 자료가 없는 AWS Key Management Service(AWS KMS) 키를 생성합니다. 해당 KMS 키에 고객 키 자료를 가져옵니다(Import). 자동 키 로테이션을 활성화합니다.

```
A company is planning to move its data to an Amazon S3 bucket. The data must be encrypted when it is stored in the S3 bucket. Additionally, the encryption key must be automatically rotated every year.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.
- B. Create an AWS Key Management Service (AWS KMS) customer managed key. Enable automatic key rotation. Set the S3 bucket’s default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket.
- C. Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3 bucket’s default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket. Manually rotate the KMS key every year.
- D. Encrypt the data with customer key material before moving the data to the S3 bucket. Create an AWS Key Management Service (AWS KMS) key without key material. Import the customer key material into the KMS key. Enable automatic key rotation.
```

정답 : `B`

- 요구사항: 1. S3에 저장 시 암호화, 2. 매년 자동 키 로테이션
- AWS KMS 고객 관리형 키(CMK)는 자동 키 로테이션(연 1회) 기능을 제공하며, S3 버킷의 기본 암호화를 SSE-KMS(해당 CMK) 로 설정하면 신규 업로드 객체가 자동으로 해당 키로 암호화

오답 이유

- **A. SSE-S3**
    - S3 관리형 키는 키 관리를 S3가 알아서 처리하지만, **고객이 명시적으로 “매년” 로테이션을 보장/구성할 수 없습니다.** 시험 맥락에서는 “연 1회 자동 로테이션”의 명확한 보증이 필요한 경우 **KMS 고객 관리형 키 + 자동 로테이션**이 정답입니다.
    
- **C. KMS CMK + 수동 로테이션**
    - 기능적으로 요구사항을 만족할 수 있으나, **매년 수동 로테이션**은 운영 오버헤드가 큽니다. 문제는 “LEAST operational overhead(최소 운영 오버헤드)”를 묻고 있으므로 부적합합니다.
    
- **D. 가져온 키 자료(Imported key material)**
    - KMS에 **가져온 키 자료**를 사용하는 키는 **자동 키 로테이션이 지원되지 않습니다.** 가져온 키는 로테이션을 직접 설계/실행해야 하므로 요구사항(자동 로테이션)과 모순됩니다. 또한 사전 암호화 및 키 자료 수명 관리까지 필요해 오버헤드가 큽니다.
    - KMS에 가져온 키 자료 = 온프레미스 키를 KMS로 가져와 사용


## #203
한 금융 회사의 고객들은 문자 메시지를 보내 재무 상담사와의 약속을 요청합니다. Amazon EC2 인스턴스에서 실행되는 웹 애플리케이션이 약속 요청을 수락합니다. 문자 메시지는 웹 애플리케이션을 통해 Amazon Simple Queue Service(Amazon SQS) 큐에 게시됩니다. 그런 다음 EC2 인스턴스에서 실행되는 또 다른 애플리케이션이 고객에게 미팅 초대장과 미팅 확인 이메일 메시지를 보냅니다. 일정이 성공적으로 예약된 후, 이 애플리케이션은 미팅 정보를 Amazon DynamoDB 데이터베이스에 저장합니다.

회사가 확장함에 따라, 고객들은 미팅 초대장이 도착하는 데 더 오래 걸린다고 보고합니다.

이 문제를 해결하기 위해 솔루션스 아키텍트는 무엇을 권장해야 합니까?

A. DynamoDB 데이터베이스 앞에 DynamoDB Accelerator(DAX) 클러스터를 추가합니다.
B. 약속 요청을 수락하는 웹 애플리케이션 앞에 Amazon API Gateway API를 추가합니다.
C. Amazon CloudFront 배포를 추가합니다. 오리진을 약속 요청을 수락하는 웹 애플리케이션으로 설정합니다.
D. 미팅 초대장을 보내는 애플리케이션에 대해 Auto Scaling 그룹을 추가합니다. Auto Scaling 그룹이 SQS 큐의 깊이에 따라 확장되도록 구성합니다.

```
The customers of a finance company request appointments with financial advisors by sending text messages. A web application that runs on Amazon EC2 instances accepts the appointment requests. The text messages are published to an Amazon Simple Queue Service (Amazon SQS) queue through the web application. Another application that runs on EC2 instances then sends meeting invitations and meeting confirmation email messages to the customers. After successful scheduling, this application stores the meeting information in an Amazon DynamoDB database.  
  
As the company expands, customers report that their meeting invitations are taking longer to arrive.  
  
What should a solutions architect recommend to resolve this issue?

- A. Add a DynamoDB Accelerator (DAX) cluster in front of the DynamoDB database.
- B. Add an Amazon API Gateway API in front of the web application that accepts the appointment requests.
- C. Add an Amazon CloudFront distribution. Set the origin as the web application that accepts the appointment requests.
- D. Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SQS queue.
```

정답 : `D`

- 지연은 "초대장 발송" 단계에서 발생 -> 이 단계의 작업은 SQS 큐에 적재된 메시지를 처리하는 EC2 기반 워커 애플리케이션이 수행
- 회사가 확장되면서 수요 급증 -> 큐 적체 증가 -> 워커 처리 지연
- SQS 큐의 깊이(대기 메시지 수, 지연 시간 등)를 지표로 오토 스케일링을 구성해 워커 인스턴스를 자동으로 수평 확장

오답 이유

- **A. DAX 추가**
    - DAX는 **DynamoDB 읽기 지연**을 줄이는 인메모리 캐시입니다. 문제의 지연은 “초대장/이메일 발송”까지 도달하기 전 **큐 처리 병목**이 원인입니다. 데이터 저장(스케줄 성공 후) 단계의 읽기/쓰기 최적화가 핵심이 아니므로 효과가 미미합니다.
    
- **B. API Gateway 앞단 추가**
    - API Gateway는 **프런트엔드 엔드포인트 관리**와 통합/보안/스로틀링에 유용하지만, 현재 병목은 “요청 수락”이 아니라 **백엔드의 큐 소비자 처리량**입니다. 앞단을 바꿔도 초대장 발송 지연은 해소되지 않습니다.
    
- **C. CloudFront 배포 추가**
    - CloudFront는 **정적/캐시 가능한 콘텐츠**의 전송 가속에 유리합니다. 약속 요청은 동적 트래픽이며, 병목은 캐시로 해결할 수 없는 **백엔드 비동기 처리**입니다. 따라서 지연 감소에 직접적 도움이 되지 않습니다.


## #204
한 온라인 소매 회사는 5천만 명이 넘는 활성 고객을 보유하고 있으며 하루에 25,000개 이상의 주문을 받습니다. 회사는 고객의 구매 데이터를 수집하여 Amazon S3에 저장합니다. 추가 고객 데이터는 Amazon RDS에 저장됩니다.

회사는 다양한 팀이 분석을 수행할 수 있도록 모든 데이터에 접근할 수 있게 하기를 원합니다. 솔루션은 데이터에 대한 세분화된 권한 관리를 제공해야 하며, 운영 오버헤드를 최소화해야 합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. 구매 데이터를 Amazon RDS에 직접 쓰도록 마이그레이션합니다. 액세스를 제한하기 위해 RDS 액세스 제어를 사용합니다.
B. AWS Lambda 함수를 예약하여 Amazon RDS에서 Amazon S3로 주기적으로 데이터를 복사합니다. AWS Glue 크롤러를 생성합니다. Amazon Athena를 사용하여 데이터를 쿼리합니다. 액세스를 제한하기 위해 S3 정책을 사용합니다.
C. AWS Lake Formation을 사용하여 데이터 레이크를 생성합니다. Amazon RDS에 대한 AWS Glue JDBC 연결을 생성합니다. Lake Formation에 S3 버킷을 등록합니다. 액세스를 제한하기 위해 Lake Formation 액세스 제어를 사용합니다.
D. Amazon Redshift 클러스터를 생성합니다. AWS Lambda 함수를 예약하여 Amazon S3 및 Amazon RDS에서 Amazon Redshift로 주기적으로 데이터를 복사합니다. 액세스를 제한하기 위해 Amazon Redshift 액세스 제어를 사용합니다.

```
An online retail company has more than 50 million active customers and receives more than 25,000 orders each day. The company collects purchase data for customers and stores this data in Amazon S3. Additional customer data is stored in Amazon RDS.  
  
The company wants to make all the data available to various teams so that the teams can perform analytics. The solution must provide the ability to manage fine-grained permissions for the data and must minimize operational overhead.  
  
Which solution will meet these requirements?

- A. Migrate the purchase data to write directly to Amazon RDS. Use RDS access controls to limit access.
- B. Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3. Create an AWS Glue crawler. Use Amazon Athena to query the data. Use S3 policies to limit access.
- C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.
- D. Create an Amazon Redshift cluster. Schedule an AWS Lambda function to periodically copy data from Amazon S3 and Amazon RDS to Amazon Redshift. Use Amazon Redshift access controls to limit access.
```

정답 : `C`

- 요구사항
	- S3와 RDS 전반의 데이터 분석 접근
	- 세분화된 권한 관리(테이블/열/행 수준 등)
	- 운영 오버헤드 최소화
- AWS Lake Formation은 S3 기반 데이터 레이크에 대해 중앙집중적 권한 관리(컬럼 수준 권한, LF-Tags 기반 권한, 크로스어카운트 위임 등)를 제공
- Glue Data Catalog를 통해 스키마를 관리하여 Athena/EMR/Redshift Spectrum/Glue 등과 통합되어 일관된 접근 제어를 적용 가능
- Glue JDBC 연결을 통해 RDS 메타데이터를 카탈로그화하거나 ETL 경로로 S3에 적재하여 동일한 거버넌스 모델 하에 제공할 수 있어, 분석 팀별 미세 권한 제어를 일관되게 적용하면서 운영 부담을 낮춤

오답 이유

- **A. RDS로 일원화**
    - RDS는 OLTP에 적합하며 대규모 분석 워크로드/다양한 팀 공유에 비효율적입니다. 세분화된 데이터 거버넌스를 여러 엔진/툴 전반에서 일관되게 제공하기 어렵고, 기존 S3 기반 구매 데이터 이동 자체가 큰 마이그레이션/운영 부담입니다.
    
- **B. RDS→S3 주기 복사 + Athena + S3 정책**
    - 작동은 가능하나, **세분화된 권한 관리**를 S3 버킷/프리픽스 정책만으로 구현하기는 어렵고(특히 **컬럼 수준** 등), 팀/데이터셋 증가 시 정책 복잡도가 급상승합니다. 또한 **복사 잡 관리** 등 운영 오버헤드가 큽니다. Lake Formation의 중앙 거버넌스 기능이 부재합니다.
    
- **D. Redshift로 집계(복사)하고 Redshift 권한 사용**
    - 전용 DW 운영(클러스터/워크로드 관리/용량 계획)이 필요하여 **운영 오버헤드가 큼**. 또한 S3·RDS 원천으로부터 **주기적 복사/동기화 파이프라인** 관리가 필요하며, 모든 팀이 Redshift를 통해서만 접근해야 하는 제약도 생깁니다. S3 데이터 레이크 전반의 **중앙 거버넌스** 요구에는 Lake Formation이 더 적합합니다.

## #205
한 회사가 온프레미스 데이터 센터에서 마케팅 웹사이트를 호스팅합니다. 웹사이트는 정적 문서로 구성되어 있으며 단일 서버에서 실행됩니다. 관리자는 웹사이트 콘텐츠를 드물게 업데이트하며 SFTP 클라이언트를 사용해 새 문서를 업로드합니다.

회사는 웹사이트를 AWS에서 호스팅하고 Amazon CloudFront를 사용하기로 결정했습니다. 회사의 솔루션스 아키텍트는 CloudFront 배포를 생성했습니다. 솔루션스 아키텍트는 CloudFront 오리진으로 사용할 웹사이트 호스팅을 위해 가장 비용 효율적이고 탄력적인 아키텍처를 설계해야 합니다.

이 요구사항을 충족할 수 있는 솔루션은 무엇입니까?

A. Amazon Lightsail을 사용하여 가상 서버를 생성합니다. Lightsail 인스턴스에 웹 서버를 구성합니다. SFTP 클라이언트를 사용하여 웹사이트 콘텐츠를 업로드합니다.
B. Amazon EC2 인스턴스를 위한 AWS Auto Scaling 그룹을 생성합니다. Application Load Balancer를 사용합니다. SFTP 클라이언트를 사용하여 웹사이트 콘텐츠를 업로드합니다.
C. 비공개 Amazon S3 버킷을 생성합니다. S3 버킷 정책을 사용하여 CloudFront 오리진 액세스 아이덴티티(OAI)에서의 액세스를 허용합니다. AWS CLI를 사용하여 웹사이트 콘텐츠를 업로드합니다.
D. 퍼블릭 Amazon S3 버킷을 생성합니다. SFTP용 AWS Transfer를 구성합니다. S3 버킷을 웹사이트 호스팅으로 구성합니다. SFTP 클라이언트를 사용하여 웹사이트 콘텐츠를 업로드합니다.

```
A company hosts a marketing website in an on-premises data center. The website consists of static documents and runs on a single server. An administrator updates the website content infrequently and uses an SFTP client to upload new documents.  
  
The company decides to host its website on AWS and to use Amazon CloudFront. The company’s solutions architect creates a CloudFront distribution. The solutions architect must design the most cost-effective and resilient architecture for website hosting to serve as the CloudFront origin.  
  
Which solution will meet these requirements?

- A. Create a virtual server by using Amazon Lightsail. Configure the web server in the Lightsail instance. Upload website content by using an SFTP client.
- B. Create an AWS Auto Scaling group for Amazon EC2 instances. Use an Application Load Balancer. Upload website content by using an SFTP client.
- C. Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access identity (OAI). Upload website content by using the AWS CLI.
- D. Create a public Amazon S3 bucket. Configure AWS Transfer for SFTP. Configure the S3 bucket for website hosting. Upload website content by using the SFTP client.
```

정답 : `C`

- 정적 콘텐츠를 드물게 업데이트 & CloudFront 사용을 전제
- 이 상황에서 가장 비용 효율적이고 탄력적인 오리진은 비공개 S3 버킷 + CloudFront OAI(OAC) 패턴
- 서버를 운영할 필요가 없어 운영 오버헤드와 비용이 최소화, S3의 내구성과 가용성으로 탄력성도 높음
- CloudFront에서만 S3에 접근하도록 구성하면 퍼블릭 노출 없이 보안 강화

오답 이유

- **A. Lightsail 가상 서버**
    - 단일 서버 운영(패치/스케일/장애 조치) 부담이 생기며, S3보다 탄력성과 내구성이 낮습니다. 정적 사이트에 서버를 유지하는 것은 비용·운영 측면에서 비효율적입니다.
    
- **B. EC2 Auto Scaling + ALB**
    - 정적 문서 제공에 과도한 아키텍처입니다. 인스턴스/ALB 비용과 관리 복잡도가 증가하며, S3+CloudFront 대비 비용 효율이 떨어집니다.
    
- **D. 퍼블릭 S3 + AWS Transfer for SFTP + S3 웹사이트 호스팅**
    - Transfer Family를 추가하면 불필요한 비용이 늘고, 버킷을 퍼블릭으로 열고 S3 웹사이트 엔드포인트를 구성할 필요가 없습니다(CloudFront가 오리진으로 직접 S3를 사용). 보안·비용 측면에서 C보다 열등합니다.


## #206
한 회사가 Amazon Machine Image(AMI)를 관리하려고 합니다. 회사는 현재 AMI가 생성된 동일한 AWS 리전에 AMI를 복사합니다. 회사는 AWS API 호출을 캡처하고 회사 계정 내에서 Amazon EC2 CreateImage API 작업이 호출될 때마다 알림을 보내는 애플리케이션을 설계해야 합니다.

이 요구사항을 가장 적은 운영 오버헤드로 충족하는 솔루션은 무엇입니까?

A. CreateImage API 호출이 감지되면 AWS CloudTrail 로그를 쿼리하고 알림을 보내는 AWS Lambda 함수를 생성합니다.
B. 업데이트된 로그가 Amazon S3로 전송될 때 발생하는 Amazon Simple Notification Service(Amazon SNS) 알림과 함께 AWS CloudTrail을 구성합니다. API 호출이 감지될 때 CreateImage를 쿼리하기 위해 Amazon Athena에서 새 테이블을 생성하고 쿼리합니다.
C. CreateImage API 호출에 대한 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다. CreateImage API 호출이 감지되면 알림을 보내기 위해 대상을 Amazon Simple Notification Service(Amazon SNS) 주제로 구성합니다.
D. AWS CloudTrail 로그의 대상으로 Amazon Simple Queue Service(Amazon SQS) FIFO 큐를 구성합니다. CreateImage API 호출이 감지되면 Amazon Simple Notification Service(Amazon SNS) 주제로 알림을 보내는 AWS Lambda 함수를 생성합니다.

```
A company wants to manage Amazon Machine Images (AMIs). The company currently copies AMIs to the same AWS Region where the AMIs were created. The company needs to design an application that captures AWS API calls and sends alerts whenever the Amazon EC2 CreateImage API operation is called within the company’s account.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a CreateImage API call is detected.
- B. Configure AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS) notification that occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to query on CreateImage when an API call is detected.
- C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the CreateImage API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a CreateImage API call is detected.
- D. Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS CloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon Simple Notification Service (Amazon SNS) topic when a CreateImage API call is detected.
```

정답 : `C`

- CloudTrail은 관리 이벤트를 EventBridge로 실시간 전달할 수 있고 EventBridge 규칙에서 CreateImage(EC2의 AMI 생성) API 호출 이벤트 패턴을 매칭해 바로 SNS로 알림 전송 가능
- 풀링/배치 쿼리/저장소 관리가 필요 없고, 완전관리형 서비스 간 이벤트 라우팅 만으로 동작하므로 운영 오버헤드 최소화

오답 이유

- **A. CloudTrail 로그를 Lambda로 주기 쿼리**
    - S3/CloudWatch Logs에 적재된 로그를 **주기적으로 스캔**해야 하며, 지연과 비용, 운영 복잡도가 증가합니다. 실시간성도 C보다 떨어집니다.
    
- **B. CloudTrail→S3+SNS 알림 후 Athena 쿼리**
    - S3 적재, Glue/스키마 관리, Athena 쿼리 트리거링 등 **구성 요소가 많아 운영 오버헤드가 큼**. 단순 알림 목적에 과도합니다.
    
- **D. CloudTrail 로그의 대상으로 SQS FIFO 구성**
    - CloudTrail은 로그 배송 대상으로 **S3/CloudWatch Logs**를 지원하며, **직접 SQS로 로그를 전달하지 않습니다**. 이벤트 기반 실시간 알림은 EventBridge를 통해 처리하는 것이 표준 패턴입니다. 또한 FIFO까지 요구할 이유가 없습니다.


## #207
한 회사가 비동기 API를 소유하고 있으며, 이 API는 사용자 요청을 수집하고 요청 유형에 따라 적절한 마이크로서비스로 요청을 디스패치합니다. 회사는 Amazon API Gateway를 사용해 API 프런트엔드를 배포하고 있으며, AWS Lambda 함수가 Amazon DynamoDB를 호출해 사용자 요청을 저장한 다음 처리 마이크로서비스로 디스패치합니다.

회사는 예산이 허용하는 만큼의 DynamoDB 처리량을 프로비저닝했지만, 여전히 가용성 문제를 겪고 있으며 사용자 요청을 유실하고 있습니다.

기존 사용자에게 영향을 주지 않고 이 문제를 해결하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?

A. 서버 측 스로틀 제한으로 API Gateway에 스로틀링을 추가합니다.
B. DynamoDB Accelerator(DAX)와 Lambda를 사용하여 DynamoDB에 대한 쓰기를 버퍼링합니다.
C. 사용자 요청이 있는 테이블에 대해 DynamoDB에 보조 인덱스를 생성합니다.
D. Amazon Simple Queue Service(Amazon SQS) 큐와 Lambda를 사용하여 DynamoDB에 대한 쓰기를 버퍼링합니다.

```
A company owns an asynchronous API that is used to ingest user requests and, based on the request type, dispatch requests to the appropriate microservice for processing. The company is using Amazon API Gateway to deploy the API front end, and an AWS Lambda function that invokes Amazon DynamoDB to store user requests before dispatching them to the processing microservices.  
  
The company provisioned as much DynamoDB throughput as its budget allows, but the company is still experiencing availability issues and is losing user requests.  
  
What should a solutions architect do to address this issue without impacting existing users?

- A. Add throttling on the API Gateway with server-side throttling limits.
- B. Use DynamoDB Accelerator (DAX) and Lambda to buffer writes to DynamoDB.
- C. Create a secondary index in DynamoDB for the table with the user requests.
- D. Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB.
```

정답 : `D`

- SQS를 수신(ingest) 경로와 DynamoDB 사이의 버퍼로 두면, 트래픽 스파이크로 인한 DynamoDB 쓰기 제한(프로비저닝/온디맨드 상관없이)에도 요청을 내구성 있게 적재하고 자동 재시도와 배치 쓰기로 백엔드 압력을 완화
- 람다(소비자)는 SQS를 이벤트 소스로 사용해 배치 크기/가시성 타임아웃/최대 동시성을 조정하여 테이블 쓰기 처리량에 맞춰 탄력적으로 처리율 조절
- 필요시 DLQ로 실패 메시지를 보존해 추가 손실을 방지

오답 이유

- **A. API Gateway 스로틀링 추가**
    - 스로틀링은 백엔드 보호에는 유용하지만 **요청 자체를 제한/거절**하여 **유실**을 유발할 수 있습니다. 문제의 핵심은 **버퍼링과 내구성**이며, 스로틀링만으로는 유실 방지 불가.
    
- **B. DAX로 버퍼링**
    - DAX는 **읽기 지연을 줄이는 인메모리 캐시**로, **쓰기 버퍼링이나 쓰기 처리량 문제를 해결하지 않습니다**. DynamoDB에 대한 **쓰기** 병목 및 유실 방지와 무관.
    
- **C. 보조 인덱스(GSI) 생성**
    - GSI는 **추가 쓰기 용량을 소비**하여 오히려 쓰기 병목을 악화시킬 수 있습니다. 인덱스는 조회 패턴 최적화용이지, **유실 방지/버퍼링** 솔루션이 아닙니다.


## #208
한 회사는 Amazon EC2 인스턴스에서 Amazon S3 버킷으로 데이터를 이동해야 합니다. 회사는 어떠한 API 호출과 데이터도 공용 인터넷 경로를 통해 라우팅되지 않도록 해야 합니다. 오직 EC2 인스턴스만 S3 버킷에 데이터를 업로드할 수 있어야 합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. EC2 인스턴스가 위치한 서브넷에 Amazon S3용 인터페이스 VPC 엔드포인트를 생성합니다. 액세스를 EC2 인스턴스의 IAM 역할만 허용하도록 S3 버킷에 리소스 정책을 연결합니다.
B. EC2 인스턴스가 위치한 가용 영역에 Amazon S3용 게이트웨이 VPC 엔드포인트를 생성합니다. 엔드포인트에 적절한 보안 그룹을 연결합니다. 액세스를 EC2 인스턴스의 IAM 역할만 허용하도록 S3 버킷에 리소스 정책을 연결합니다.
C. EC2 인스턴스 내부에서 nslookup 도구를 실행하여 S3 버킷의 서비스 API 엔드포인트의 프라이빗 IP 주소를 가져옵니다. VPC 라우팅 테이블에 경로를 생성하여 EC2 인스턴스가 S3 버킷에 액세스할 수 있도록 합니다. 액세스를 EC2 인스턴스의 IAM 역할만 허용하도록 S3 버킷에 리소스 정책을 연결합니다.
D. AWS에서 제공하는 공개 ip-ranges.json 파일을 사용하여 S3 버킷의 서비스 API 엔드포인트의 프라이빗 IP 주소를 가져옵니다. VPC 라우팅 테이블에 경로를 생성하여 EC2 인스턴스가 S3 버킷에 액세스할 수 있도록 합니다. 액세스를 EC2 인스턴스의 IAM 역할만 허용하도록 S3 버킷에 리소스 정책을 연결합니다.

```
A company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The company must ensure that no API calls and no data are routed through public internet routes. Only the EC2 instance can have access to upload data to the S3 bucket.  
  
Which solution will meet these requirements?

- A. Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.
- B. Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance is located. Attach appropriate security groups to the endpoint. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.
- C. Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3 bucket’s service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.
- D. Use the AWS provided, publicly available ip-ranges.json file to obtain the private IP address of the S3 bucket’s service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.
```

정답 : `A`

- 인터페이스 VPC 엔드포인트(PrivateLink for S3)를 사용하면 S3 API 트래픽이 VPC 내부의 프라이빗 ENI를 통해 이루어져 공용 인터넷을 경유하지 않음
- S3 버킷 정책에서 Principal을 EC2 인스턴스의 IAM 역할로 제한하고, 필요하면 aws:SourceVpce 조건으로 특정 인터페이스 엔드포인트에서만 접근하도록 제한해 "오직 EC2 인스턴스만 업로드"를 강제 가능

오답 이유

- **B. 게이트웨이 VPC 엔드포인트 + 보안 그룹 연결**
    - **게이트웨이 VPC 엔드포인트(S3, DynamoDB)** 는 **보안 그룹을 연결하지 않습니다**(라우팅 테이블에 엔드포인트 대상으로 경로를 추가). 또한 “가용 영역 단위”로 만들지 않습니다(라우팅 테이블/서브넷 범위). 기술적으로 틀린 설명이 포함되어 부적절합니다.
    - (참고) 게이트웨이 엔드포인트 자체는 요건을 충족할 수 있지만, 보기의 서술이 잘못되어 정답으로 보기 어렵습니다.
    
- **C. nslookup으로 프라이빗 IP 확인 후 라우팅**
    - S3는 **가상 호스팅/퍼블릭 엔드포인트** 형태이며, 특정 “프라이빗 IP”를 조회해 VPC 라우트에 수동 등록하는 방식은 지원되지 않습니다. S3에 사설로 접근하려면 **VPC 엔드포인트**를 사용해야 합니다.
    
- **D. ip-ranges.json으로 프라이빗 IP 얻어 라우팅**
    - ip-ranges.json은 **퍼블릭** IP 범위를 제공합니다. S3에 사설 경로를 만들 목적으로 라우팅 테이블에 직접 추가하는 방식은 동작하지 않습니다. 역시 **VPC 엔드포인트**가 정식 방법입니다.


## #209
한 솔루션스 아키텍트가 AWS 클라우드에 배포될 새로운 애플리케이션의 아키텍처를 설계하고 있습니다. 이 애플리케이션은 Amazon EC2 온디맨드 인스턴스에서 실행되며, 여러 가용 영역에 걸쳐 자동으로 확장됩니다. EC2 인스턴스는 하루 종일 빈번하게 증가하고 감소할 것입니다. Application Load Balancer(ALB)가 부하 분산을 처리합니다. 아키텍처는 분산 세션 데이터 관리(distributed session data management)를 지원해야 합니다. 회사는 필요하다면 코드 변경도 수용할 수 있습니다.

아키텍처가 분산 세션 데이터 관리를 지원하도록 하려면 솔루션스 아키텍트는 무엇을 해야 합니까?

A. Amazon ElastiCache를 사용하여 세션 데이터를 관리하고 저장합니다.
B. ALB의 세션 어피니티(스티키 세션)를 사용하여 세션 데이터를 관리합니다.
C. AWS Systems Manager의 Session Manager를 사용하여 세션을 관리합니다.
D. AWS Security Token Service(AWS STS)의 GetSessionToken API 작업을 사용하여 세션을 관리합니다.

```
A solutions architect is designing the architecture of a new application being deployed to the AWS Cloud. The application will run on Amazon EC2 On-Demand Instances and will automatically scale across multiple Availability Zones. The EC2 instances will scale up and down frequently throughout the day. An Application Load Balancer (ALB) will handle the load distribution. The architecture needs to support distributed session data management. The company is willing to make changes to code if needed.  
  
What should the solutions architect do to ensure that the architecture supports distributed session data management?

- A. Use Amazon ElastiCache to manage and store session data.
- B. Use session affinity (sticky sessions) of the ALB to manage session data.
- C. Use Session Manager from AWS Systems Manager to manage the session.
- D. Use the GetSessionToken API operation in AWS Security Token Service (AWS STS) to manage the session.
```

정답 : `A`

- EC2 인스턴스가 수시로 스케일 인/아웃 되는 환경에서는 세션 데이터를 인스턴스 로컬 메모리에 저장하면 인스턴스 종료 시 데이터 유실
- 분산 세션 관리를 위해 세션 데이터를 중앙에서 관리 가능한 저장소에 두는 것이 필요
- Amazon ElastiCache (Redis/Memcached) 는 빠른 인메모리 캐시 저자이소로서 세션 데이터를 저정하고 여러 EC2 인스턴스에서 공유 가능
- 코드에서 세션 저장 위치를 ElastiCache로 변경하면, 어떤 인스턴스가 요청을 처리하더라도 동일한 세션 데이터를 참조

오답 이유

- **B. ALB 스티키 세션(Session Affinity)**
    - 특정 사용자를 항상 같은 인스턴스로 라우팅하지만, **인스턴스 종료/축소 시 세션 데이터 유실** 문제가 발생합니다. 인스턴스 수명이 짧고 빈번히 변경되는 환경에서는 적절하지 않습니다.
    
- **C. Systems Manager Session Manager**
    - 이는 **운영자가 EC2 인스턴스에 접속**하기 위한 도구로, 애플리케이션 사용자 세션 관리와는 무관합니다.
    
- **D. AWS STS GetSessionToken**
    - STS는 **IAM 인증 토큰 발급**을 위한 서비스로, 애플리케이션의 웹 세션 관리에는 사용되지 않습니다


## #210
한 회사는 빠르게 성장 중인 음식 배달 서비스를 제공합니다. 성장 때문에 회사의 주문 처리 시스템은 피크 트래픽 시간 동안 확장 문제를 겪고 있습니다. 현재 아키텍처는 다음을 포함합니다:

• 애플리케이션에서 주문을 수집하기 위해 Amazon EC2 Auto Scaling 그룹에서 실행되는 Amazon EC2 인스턴스 그룹
• 주문을 처리(이행)하기 위해 Amazon EC2 Auto Scaling 그룹에서 실행되는 또 다른 EC2 인스턴스 그룹

주문 수집 프로세스는 빠르게 진행되지만, 주문 이행 프로세스는 더 오래 걸릴 수 있습니다. 스케일링 이벤트로 인해 데이터가 손실되어서는 안 됩니다.

솔루션스 아키텍트는 피크 트래픽 시간 동안 주문 수집 프로세스와 주문 이행 프로세스가 모두 적절히 확장되도록 해야 합니다. 솔루션은 회사의 AWS 리소스 활용을 최적화해야 합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. Amazon CloudWatch 지표를 사용하여 Auto Scaling 그룹의 각 인스턴스 CPU를 모니터링합니다. 각 Auto Scaling 그룹의 최소 용량을 피크 워크로드 값에 맞게 구성합니다.
B. Amazon CloudWatch 지표를 사용하여 Auto Scaling 그룹의 각 인스턴스 CPU를 모니터링합니다. Amazon Simple Notification Service(Amazon SNS) 주제를 호출하는 CloudWatch 경보를 구성하여 온디맨드로 추가 Auto Scaling 그룹을 생성합니다.
C. 두 개의 Amazon Simple Queue Service(Amazon SQS) 큐를 프로비저닝합니다: 하나는 주문 수집용, 다른 하나는 주문 이행용. EC2 인스턴스가 각자의 큐를 폴링하도록 구성합니다. 큐가 보내는 알림을 기반으로 Auto Scaling 그룹을 스케일링합니다.
D. 두 개의 Amazon Simple Queue Service(Amazon SQS) 큐를 프로비저닝합니다: 하나는 주문 수집용, 다른 하나는 주문 이행용. EC2 인스턴스가 각자의 큐를 폴링하도록 구성합니다. 인스턴스당 백로그 계산을 기반으로 한 지표를 생성합니다. 이 지표를 기반으로 Auto Scaling 그룹을 스케일링합니다.

```
A company offers a food delivery service that is growing rapidly. Because of the growth, the company’s order processing system is experiencing scaling problems during peak traffic hours. The current architecture includes the following:  
  
• A group of Amazon EC2 instances that run in an Amazon EC2 Auto Scaling group to collect orders from the application  
• Another group of EC2 instances that run in an Amazon EC2 Auto Scaling group to fulfill orders  
  
The order collection process occurs quickly, but the order fulfillment process can take longer. Data must not be lost because of a scaling event.  
  
A solutions architect must ensure that the order collection process and the order fulfillment process can both scale properly during peak traffic hours. The solution must optimize utilization of the company’s AWS resources.  
  
Which solution meets these requirements?

- A. Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure each Auto Scaling group’s minimum capacity according to peak workload values.
- B. Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure a CloudWatch alarm to invoke an Amazon Simple Notification Service (Amazon SNS) topic that creates additional Auto Scaling groups on demand.
- C. Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Scale the Auto Scaling groups based on notifications that the queues send.
- D. Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric.
```

정답 : `D`

- 단계가 분리된 비동기 처리(수집 -> 이행)에서는 SQS로 디커플링하여 내구성 있게 버퍼링
- 처리 시간이 더 긴 이행 단계는 백로그(대기 메시지) 규모에 맞춰 탄력적으로 스케일
- 인스턴스당 백로그(Backlog per instance) 지표(예: ApproximateNumberOfMessagesVisible / 현재 소비자 인스턴스 수)를 만들어 타깃 추적 스케일리 적용
- SQS는 최소 1회 전달, 가시성 타임아웃, DLQ로 데이터 손실 방지 보장

오답 이유
- **A. CPU 기반 + 최소 용량 피크 고정**
    - CPU는 큐 백로그와 직접 상관이 없고, 피크에 맞춘 **과잉 프로비저닝**으로 비용이 증가합니다. 워크로드 변화에 따른 탄력적 확장이 어렵습니다.
    
- **B. SNS로 추가 Auto Scaling 그룹 생성**
    - Auto Scaling 그룹을 **여러 개 동적으로 추가**하는 방식은 불필요하게 복잡하고 운영 오버헤드가 큽니다. 또한 CPU 기반 모니터링은 큐 적체를 직접 반영하지 못합니다.
    
- **C. 큐 알림 기반 스케일링**
    - SQS는 **본질적으로 알림을 “푸시”하지 않으며**(별도 설정 없이), 표준적인 스케일 트리거는 **CloudWatch 큐 지표**입니다. “알림”만으로는 정교한 스케일 기준(백로그/인스턴스)을 구현하기 어렵습니다.


## #211
한 회사가 여러 프로덕션 애플리케이션을 호스팅하고 있습니다. 그 중 하나의 애플리케이션은 여러 AWS 리전에 걸쳐 Amazon EC2, AWS Lambda, Amazon RDS, Amazon Simple Notification Service(Amazon SNS), Amazon Simple Queue Service(Amazon SQS) 리소스로 구성되어 있습니다. 모든 회사 리소스는 "application"이라는 태그 이름과 각 애플리케이션에 해당하는 값을 사용하여 태깅되어 있습니다. 솔루션스 아키텍트는 태그된 모든 컴포넌트를 식별하기 위한 가장 빠른 솔루션을 제공해야 합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. AWS CloudTrail을 사용하여 application 태그가 있는 리소스 목록을 생성합니다.
B. AWS CLI를 사용하여 모든 리전의 각 서비스에 쿼리하여 태그된 컴포넌트를 보고합니다.
C. Amazon CloudWatch Logs Insights에서 쿼리를 실행하여 application 태그가 있는 컴포넌트를 보고합니다.
D. AWS Resource Groups Tag Editor로 쿼리를 실행하여 application 태그가 있는 리소스를 전역적으로 보고합니다.

```
A company hosts multiple production applications. One of the applications consists of resources from Amazon EC2, AWS Lambda, Amazon RDS, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Queue Service (Amazon SQS) across multiple AWS Regions. All company resources are tagged with a tag name of “application” and a value that corresponds to each application. A solutions architect must provide the quickest solution for identifying all of the tagged components.  
  
Which solution meets these requirements?

- A. Use AWS CloudTrail to generate a list of resources with the application tag.
- B. Use the AWS CLI to query each service across all Regions to report the tagged components.
- C. Run a query in Amazon CloudWatch Logs Insights to report on the components with the application tag.
- D. Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with the application tag.
```

정답 : `D`

- AWS Resource Groups Tag Editor는 태그 기반으로 여러 리전과 서비스 전반의 리소스를 검색할 수 있는 도구
- "application"이라는 키로 태깅된 리소스를 빠르게 전역 조회할 수 있으며, EC2, Lambda, RDS, SNS, SQS 등 주요 서비스가 지원
- 추가적인 로그 분석이나 수동 반복 쿼리 없이도 콘솔/CLI에서 바로 태그 기반 검색이 가능

오답 이유

- **A. CloudTrail**
    - CloudTrail은 **API 호출 기록**을 남기는 서비스이지, 태그된 리소스 자체 목록을 생성하지 않습니다. 태그 쿼리를 위한 솔루션이 아님.
    
- **B. AWS CLI**
    - 각 서비스/리전을 반복적으로 호출해 태그 정보를 수집할 수는 있지만, **시간과 운영 오버헤드가 크다**는 문제가 있습니다. “가장 빠른 방법” 조건에 맞지 않음.
    
- **C. CloudWatch Logs Insights**
    - 이는 **로그 분석 도구**로, 태그 기반 리소스 검색을 지원하지 않습니다. 태그된 리소스 보고 목적과 맞지 않습니다.


## #212
한 회사는 매일 한 번씩 데이터베이스를 Amazon S3로 내보내어 다른 팀이 액세스할 수 있도록 해야 합니다. 내보낸 객체 크기는 2GB에서 5GB 사이입니다. 데이터에 대한 S3 액세스 패턴은 가변적이며 빠르게 변합니다. 데이터는 즉시 사용 가능해야 하며 최대 3개월 동안 액세스 가능해야 합니다. 회사는 검색 시간을 늘리지 않으면서 가장 비용 효율적인 솔루션이 필요합니다.

이 요구사항을 충족하기 위해 회사는 어떤 S3 스토리지 클래스를 사용해야 합니까?

A. S3 Intelligent-Tiering  
B. S3 Glacier Instant Retrieval  
C. S3 Standard  
D. S3 Standard-Infrequent Access (S3 Standard-IA)  

```
A company needs to export its database once a day to Amazon S3 for other teams to access. The exported object size varies between 2 GB and 5 GB. The S3 access pattern for the data is variable and changes rapidly. The data must be immediately available and must remain accessible for up to 3 months. The company needs the most cost-effective solution that will not increase retrieval time.  
  
Which S3 storage class should the company use to meet these requirements?

- A. S3 Intelligent-Tiering
- B. S3 Glacier Instant Retrieval
- C. S3 Standard
- D. S3 Standard-Infrequent Access (S3 Standard-IA)
```

정답 : `A`

- 데이터 엑세스 패턴이 가변적이고 빠르게 변함 -> 어떤 날은 자주 읽고, 어떤 날은 거의 읽지 않을 수도 있음
- Intellignet-Tiering은 자동으로 자주 액세스되는 객체는 Frequent Access 티어에, 드물게 액세스 되는 객체는 Infrequent Access 티어로 이동
- 추가적인 운영 오버헤드 없이 비용 최적화가 가능하며 모든 티어에서 즉시 접근 가능
- Glacier Instant Retrieval은 저비용이지만 장기간 보관 및 드문 액세스에 적합하며, 빠르게 변하는 액세스 패턴에는 적합하지 않음
- Standard는 무조건 빠르지만 비용 최적화 측면에서 불리하고, Standard-IA는 최소 스토리지 기간(30일)과 접근 비용이 있으므로 빠르게 변하는 액세스 패턴에 적합하지 않음

오답 이유

- **B. S3 Glacier Instant Retrieval**
    - 즉시 액세스 가능하긴 하지만 **주 용도는 장기 보관 + 드문 액세스 데이터**. 매일 내보낸 후 팀들이 자주/불규칙적으로 접근할 수 있는 데이터에는 적합하지 않음.
    
- **C. S3 Standard**
    - 항상 즉시 접근 가능하지만, **비용 최적화** 요구사항을 만족하지 못함. Intelligent-Tiering이 더 저렴하게 동일 성능 제공.
    
- **D. S3 Standard-IA**
    - 드문 액세스에는 좋지만, **액세스 패턴이 자주 바뀌는 경우**(즉, 빈번히 접근하는 날이 있으면) retrieval 비용이 증가해 비용 최적화 실패.


## #213
한 회사가 새로운 모바일 앱을 개발하고 있습니다. 회사는 교차 사이트 스크립팅(XSS)이나 SQL 인젝션과 같은 일반적인 애플리케이션 수준 공격으로부터 Application Load Balancer(ALB)를 보호하기 위해 적절한 트래픽 필터링을 구현해야 합니다. 회사에는 인프라와 운영 인력이 최소 수준입니다. 회사는 AWS 환경에서 서버를 관리, 업데이트, 보안 처리하는 책임을 줄여야 합니다.

이 요구사항을 충족하기 위해 솔루션스 아키텍트는 무엇을 권장해야 합니까?

A. AWS WAF 규칙을 구성하고 이를 ALB에 연결합니다.
B. 공개 호스팅이 활성화된 Amazon S3를 사용하여 애플리케이션을 배포합니다.
C. AWS Shield Advanced를 배포하고 ALB를 보호 대상에 추가합니다.
D. 서드파티 방화벽이 실행되는 Amazon EC2 인스턴스로 트래픽을 전달하는 새로운 ALB를 생성하고, 해당 방화벽이 트래픽을 현재 ALB로 전달하도록 합니다.

```
A company is developing a new mobile app. The company must implement proper traffic filtering to protect its Application Load Balancer (ALB) against common application-level attacks, such as cross-site scripting or SQL injection. The company has minimal infrastructure and operational staff. The company needs to reduce its share of the responsibility in managing, updating, and securing servers for its AWS environment.  
  
What should a solutions architect recommend to meet these requirements?

- A. Configure AWS WAF rules and associate them with the ALB.
- B. Deploy the application using Amazon S3 with public hosting enabled.
- C. Deploy AWS Shield Advanced and add the ALB as a protected resource.
- D. Create a new ALB that directs traffic to an Amazon EC2 instance running a third-party firewall, which then passes the traffic to the current ALB.
```

정답 : `A`

- AWS WAF는 ALB와 네이티브 통합되며, SQLi/XSS 등 애플리케이션 계층(L7) 공격을 차단하는 관리형 규칙(Managed Rule Groups) 을 제공해 운영 부담을 최소화
- 서버를 직접 운영/패치할 필요가 없는 완전관리형 서비스로 회사의 "서버 관리 책임을 줄이는" 요구에 충족
- 규칙 업데이트(서명/패턴)는 AWS가 관리하므로 지속적인 보안 업데이트를 손쉽게 적용 가능

오답 이유

- **B. S3 정적 호스팅**
    - 정적 웹 호스팅용이며, 모바일 앱 백엔드(동적 API)를 대체하지 못합니다. 또한 **애플리케이션 레벨 방어** 기능(규칙 기반 필터링)을 제공하지 않습니다.
    
- **C. AWS Shield Advanced**
    - 주로 **DDoS 보호**(L3/L4 및 일부 L7 가용성 보호)에 특화. SQLi/XSS 같은 **애플리케이션 취약점 악용** 차단은 **WAF의 역할**입니다. 요구사항과 다릅니다.
    
- **D. 서드파티 방화벽 EC2**
    - 별도 EC2/ALB/라우팅 구성으로 **운영·패치·확장 책임이 증가**합니다. “운영 부담 최소화, 서버 관리 책임 축소” 요구에 반합니다.


## #214
한 회사의 리포팅 시스템은 매일 수백 개의 .csv 파일을 Amazon S3 버킷에 전달합니다. 회사는 이 파일들을 Apache Parquet 형식으로 변환해야 하며, 변환된 데이터 버킷에 파일을 저장해야 합니다.

가장 적은 개발 노력으로 이러한 요구사항을 충족할 수 있는 솔루션은 무엇입니까?

A. Apache Spark가 설치된 Amazon EMR 클러스터를 생성합니다. 데이터를 변환하기 위한 Spark 애플리케이션을 작성합니다. 변환된 데이터 버킷에 파일을 쓰기 위해 EMR 파일 시스템(EMRFS)을 사용합니다.
B. 데이터를 검색하기 위한 AWS Glue 크롤러를 생성합니다. 데이터를 변환하기 위한 AWS Glue ETL(추출, 변환, 로드) 작업을 생성합니다. 출력 단계에서 변환된 데이터 버킷을 지정합니다.
C. 데이터를 변환하고 변환된 데이터 버킷으로 데이터를 출력하기 위한 Bash 구문으로 작업 정의를 생성하기 위해 AWS Batch를 사용합니다. 작업 정의를 사용하여 작업을 제출합니다. 작업 유형으로 배열 작업을 지정합니다.
D. 데이터를 변환하고 변환된 데이터 버킷으로 데이터를 출력하기 위한 AWS Lambda 함수를 생성합니다. S3 버킷에 대한 이벤트 알림을 구성합니다. 이벤트 알림의 대상으로 Lambda 함수를 지정합니다.

```
A company’s reporting system delivers hundreds of .csv files to an Amazon S3 bucket each day. The company must convert these files to Apache Parquet format and must store the files in a transformed data bucket.  
  
Which solution will meet these requirements with the LEAST development effort?

- A. Create an Amazon EMR cluster with Apache Spark installed. Write a Spark application to transform the data. Use EMR File System (EMRFS) to write files to the transformed data bucket.
- B. Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and load (ETL) job to transform the data. Specify the transformed data bucket in the output step.
- C. Use AWS Batch to create a job definition with Bash syntax to transform the data and output the data to the transformed data bucket. Use the job definition to submit a job. Specify an array job as the job type.
- D. Create an AWS Lambda function to transform the data and output the data to the transformed data bucket. Configure an event notification for the S3 bucket. Specify the Lambda function as the destination for the event notification.
```

정답 : `B`

- AWS Glue는 서버리스 ETL 서비스로 Glue Studio/워크플로 마법사를 통해 CSV -> Parquet 변환 파이프라인을 거의 코드 없이 구성 가능 => 최소 개발 노력
- Glue Crawler가 S3의 CSV를 스캔해 스키마를 자동으로 추론하여 Data Catalog에 등록 => 스키마 자동 인식
- Glue ETL(Job/Built-in transform)에서 출력 포맷을 Parquet로 지정하고 압축 코덱(snappy 등)도 선택 가능 => 내장 변환 & 출력 포맷 선택
- 서버/클러스터 관리가 필요 없고, 스케줄/트리거로 일단위 실행을 쉽게 자동화 => 운영 부담 낮음

오답 이유

- **A. EMR + Spark 애플리케이션**
    - 가능은 하지만 **클러스터 관리(프로비저닝/스케일/패치)** 와 **코드(스파크 잡) 작성**이 필요해 개발·운영 부담이 큼. “가장 적은 개발 노력” 조건에 부적합.
    
- **C. AWS Batch + Bash 스크립트**
    - 컨테이너 이미지 준비, 스크립트 작성, 실패 재시도/로깅 등 **파이프라인 구성 요소를 직접 관리**해야 함. CSV→Parquet 변환을 Bash로 처리하려면 별도 툴 설치가 필요해 구현 복잡도↑.
    
- **D. S3 이벤트 → Lambda 변환**
    - Parquet 변환에는 **파서/라이브러리 번들링**이 필요하고, **메모리/실행 시간(최대 15분)/패키지 크기** 제약으로 대용량 파일(2–5GB) 처리에 부적합. 멀티파트/스트리밍 처리 구현도 복잡하여 개발 노력이 큼.


## #215
한 회사는 데이터 센터의 네트워크 연결 스토리지(NAS)에 700TB의 백업 데이터를 보관하고 있습니다. 이 백업 데이터는 드물게 발생하는 규제 요청을 위해 액세스 가능해야 하며 7년 동안 보존되어야 합니다. 회사는 이 백업 데이터를 데이터 센터에서 AWS로 마이그레이션하기로 결정했습니다. 마이그레이션은 1개월 이내에 완료되어야 합니다. 회사는 데이터 전송을 위해 공용 인터넷 연결에서 500Mbps의 전용 대역폭을 사용할 수 있습니다.

가장 낮은 비용으로 데이터를 마이그레이션하고 저장하려면 솔루션스 아키텍트는 무엇을 해야 합니까?

A. 데이터를 전송하기 위해 AWS Snowball 디바이스를 주문합니다. 수명 주기 정책을 사용하여 파일을 Amazon S3 Glacier Deep Archive로 전환합니다.
B. 데이터 센터와 Amazon VPC 사이에 VPN 연결을 배포합니다. 온프레미스에서 Amazon S3 Glacier로 데이터를 복사하기 위해 AWS CLI를 사용합니다.
C. 500Mbps AWS Direct Connect 연결을 프로비저닝하고 데이터를 Amazon S3로 전송합니다. 수명 주기 정책을 사용하여 파일을 Amazon S3 Glacier Deep Archive로 전환합니다.
D. AWS DataSync를 사용하여 데이터를 전송하고 온프레미스에 DataSync 에이전트를 배포합니다. DataSync 작업을 사용하여 온프레미스 NAS 스토리지에서 Amazon S3 Glacier로 파일을 복사합니다.

```
A company has 700 TB of backup data stored in network attached storage (NAS) in its data center. This backup data need to be accessible for infrequent regulatory requests and must be retained 7 years. The company has decided to migrate this backup data from its data center to AWS. The migration must be complete within 1 month. The company has 500 Mbps of dedicated bandwidth on its public internet connection available for data transfer.  
  
What should a solutions architect do to migrate and store the data at the LOWEST cost?

- A. Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.
- B. Deploy a VPN connection between the data center and Amazon VPC. Use the AWS CLI to copy the data from on premises to Amazon S3 Glacier.
- C. Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.
- D. Use AWS DataSync to transfer the data and deploy a DataSync agent on premises. Use the DataSync task to copy files from the on-premises NAS storage to Amazon S3 Glacier.
```

정답 : `A`

- 전송 기한 1개월 vs 500Mbps 회선: 500 Mbps(약 62.5MB/s)로 30일 풀 가동시 전송 가능한 용량은 대략 ~160TB 수준으로 700TB를 1개월 내 전송하기에는 현저히 부족
- 대용량/단기간 마이그레이션에는 AWS Snowball(오프라인 전송)이 표준 해법
	- 여러 대의 Snowball Edge 디바이스를 병렬 사용해 700TB를 물리적으로 반출입해 한달 내 완료
- 저장 비용 최소화 요구에는 S3에 적재 후 라이프사이클 정책으로 S3 Glacier Deep Archive 전환이 최적(장기 보관, 드문 액세스, 최저 스토리지 비용, 필요 시 복구 가능)

오답 이유

- **B. VPN 통해 인터넷 전송 + S3 Glacier로 직접 복사**
    - 500 Mbps로는 1개월 내 **700 TB 전송 불가**. 또한 “S3 Glacier로 직접 업로드”라는 개념은 S3의 스토리지 클래스 지정일 뿐이며, 네트워크 병목 문제를 해결하지 못합니다.
    
- **C. 신규 500 Mbps Direct Connect 구축**
    - DX 신규 회선은 **개통 리드타임이 길어** 1개월 내 마이그레이션 완료가 현실적으로 어렵습니다. 또한 500 Mbps 대역폭 자체도 **용량/기간 제약**을 해소하지 못합니다(여전히 700 TB에 부족).
    
- **D. AWS DataSync로 온라인 전송**
    - DataSync는 관리형 전송 서비스지만 **기반 대역폭 한계(500 Mbps)** 는 동일합니다. 1개월 내 700 TB 전송 요건을 충족하지 못합니다. (또한 일반적으로 DataSync는 S3 버킷으로 전송 후 필요 시 수명 주기로 Glacier 계열로 전환하는 패턴이 보편적입니다.)

## #216
한 회사는 Amazon S3 버킷에 수백만 개의 객체가 있는 서버리스 웹사이트를 운영하고 있습니다. 회사는 Amazon CloudFront 배포의 오리진으로 S3 버킷을 사용합니다. 회사는 객체가 업로드되기 전에 S3 버킷에서 암호화를 설정하지 않았습니다. 솔루션스 아키텍트는 모든 기존 객체와 앞으로 S3 버킷에 추가되는 모든 객체에 대해 암호화를 활성화해야 합니다.

이 요구사항을 가장 적은 노력으로 충족하는 솔루션은 무엇입니까?

A. 새로운 S3 버킷을 생성합니다. 새 S3 버킷에 대해 기본 암호화 설정을 켭니다. 모든 기존 객체를 임시 로컬 스토리지로 다운로드합니다. 객체를 새 S3 버킷에 업로드합니다.
B. S3 버킷의 기본 암호화 설정을 켭니다. 암호화되지 않은 객체를 나열하는 .csv 파일을 생성하기 위해 S3 인벤토리 기능을 사용합니다. 이러한 객체를 암호화하기 위해 copy 명령을 사용하는 S3 배치 작업을 실행합니다.
C. AWS Key Management Service(AWS KMS)를 사용하여 새 암호화 키를 생성합니다. S3 버킷의 설정을 변경하여 AWS KMS 관리형 암호화 키(SSE-KMS)를 사용하는 서버 측 암호화를 사용하도록 합니다. S3 버킷에 버저닝을 켭니다.
D. AWS 관리 콘솔에서 Amazon S3로 이동합니다. S3 버킷의 객체를 탐색합니다. 암호화 필드로 정렬합니다. 암호화되지 않은 각 객체를 선택합니다. S3 버킷의 모든 암호화되지 않은 객체에 기본 암호화 설정을 적용하기 위해 Modify 버튼을 사용합니다.

```
A company has a serverless website with millions of objects in an Amazon S3 bucket. The company uses the S3 bucket as the origin for an Amazon CloudFront distribution. The company did not set encryption on the S3 bucket before the objects were loaded. A solutions architect needs to enable encryption for all existing objects and for all objects that are added to the S3 bucket in the future.  
  
Which solution will meet these requirements with the LEAST amount of effort?

- A. Create a new S3 bucket. Turn on the default encryption settings for the new S3 bucket. Download all existing objects to temporary local storage. Upload the objects to the new S3 bucket.
- B. Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create a .csv file that lists the unencrypted objects. Run an S3 Batch Operations job that uses the copy command to encrypt those objects.
- C. Create a new encryption key by using AWS Key Management Service (AWS KMS). Change the settings on the S3 bucket to use server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Turn on versioning for the S3 bucket.
- D. Navigate to Amazon S3 in the AWS Management Console. Browse the S3 bucket’s objects. Sort by the encryption field. Select each unencrypted object. Use the Modify button to apply default encryption settings to every unencrypted object in the S3 bucket.
```

정답 : `B`

- 미래 객체 : 버킷의 기존 암호화를 켜면 이후 업로드되는 모든 객체가 자동으로 서버측 암호화(SSE-S3 또는 SSE-KMS)
- 기존 객체 : 이미 업로드된 객체는 기본 암호화를 켜도 소급 적용되지 않음
	- 따라서 S3 인벤토리로 암호화되지 않은 객체 목록을 생성
	- S3 Batch Operations의 Copy(자기 자신으로 복사) 작업을 실행해 메타데이터/암호화 재지정을 통해 암호화를 적용

오답 이유

- **A. 새 버킷 생성 후 다운로드/재업로드**
    - 대량(수백만 객체)의 **다운로드/재업로드는 비효율적**이며 비용·시간·운영 오버헤드가 큼. CloudFront 오리진 변경 등 부수 작업도 증가.
    
- **C. SSE-KMS로 버킷 설정 변경 + 버저닝**
    - **기존 객체는 여전히 암호화되지 않은 상태**로 남습니다. 버저닝은 목적과 무관. 기존 객체에 암호화를 적용할 별도 처리(복사)가 없으므로 미완.
    
- **D. 콘솔에서 수동 수정**
    - 수백만 객체에 대해 **수작업은 현실적으로 불가능**. 또한 “Modify”만으로 기존 객체의 암호화를 일괄 소급 적용할 수 없습니다(복사/재쓰기 필요).

## #217
한 회사가 Application Load Balancer 뒤에서 Amazon EC2 인스턴스들로 글로벌 웹 애플리케이션을 운영하고 있습니다. 애플리케이션은 Amazon Aurora에 데이터를 저장합니다. 회사는 재해 복구 솔루션을 만들어야 하며, 최대 30분의 다운타임과 잠재적인 데이터 손실을 허용할 수 있습니다. 프라이머리 인프라가 정상일 때는 해당 부하를 처리할 필요가 없습니다.

이 요구사항을 충족하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?

A. 필요한 인프라 요소를 갖춘 상태로 애플리케이션을 배포합니다. Amazon Route 53을 사용하여 액티브-패시브 페일오버를 구성합니다. 두 번째 AWS 리전에 Aurora 리플리카를 생성합니다.
B. 두 번째 AWS 리전에 축소된 배포로 애플리케이션을 호스팅합니다. Amazon Route 53을 사용하여 액티브-액티브 페일오버를 구성합니다. 두 번째 리전에 Aurora 리플리카를 생성합니다.
C. 프라이머리 인프라를 두 번째 AWS 리전에 복제합니다. Amazon Route 53을 사용하여 액티브-액티브 페일오버를 구성합니다. 최신 스냅샷에서 복원된 Aurora 데이터베이스를 생성합니다.
D. AWS Backup으로 데이터를 백업합니다. 백업을 사용하여 두 번째 AWS 리전에 필요한 인프라를 생성합니다. Amazon Route 53을 사용하여 액티브-패시브 페일오버를 구성합니다. 두 번째 리전에 Aurora 두 번째 프라이머리 인스턴스를 생성합니다.

```
A company runs a global web application on Amazon EC2 instances behind an Application Load Balancer. The application stores data in Amazon Aurora. The company needs to create a disaster recovery solution and can tolerate up to 30 minutes of downtime and potential data loss. The solution does not need to handle the load when the primary infrastructure is healthy.  
  
What should a solutions architect do to meet these requirements?

- A. Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in a second AWS Region.
- B. Host a scaled-down deployment of the application in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora Replica in the second Region.
- C. Replicate the primary infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora database that is restored from the latest snapshot.
- D. Back up data with AWS Backup. Use the backup to create the required infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-passive failover. Create an Aurora second primary instance in the second Region.
```

정답 : `A`

- 요구사항 : 1. 최대 30분 RTO/RPO 허용, 2. 평소에는 보조 리전이 부하를 처리할 필요 없음
- 액티브-패시브 + 교차 리전 Aurora 리플리카(프로모션 가능)는 수분~수십 분 내 애플리케이션과 DB를 승격하여 RTO≈30분 이내 달성이 현실적
- 리플리카의 비동기 복제 특성상 일정 수준의 데이터 손실(RPO)도 허용 범위에 부합
- EC2, ALB, 보조 리전에 필요한 구성요소를 미리 준비하고 Route 53 헬스체크/페일오버로 전환하면 운영 간소성과 전환 속도를 동시 확보 가능

오답 이유

- **B (액티브-액티브 + 축소 배포)**: 액티브-액티브는 **불필요한 비용/복잡도**. 문제에서 “프라이머리가 건강할 때 보조가 부하를 처리할 필요가 없다”고 했으므로 **액티브-패시브**가 맞습니다.
    
- **C (완전 복제 + 액티브-액티브 + 스냅샷 복원)**: 스냅샷 복원은 **RTO가 길어질 가능성**이 높고, 액티브-액티브 요구도 아님.
    
- **D (AWS Backup 복구 + 패시브)**: 백업에서 **인프라와 DB를 새로 띄우는 콜드 스타트**는 보통 **30분 RTO 충족이 어려움**. 또한 “Aurora 두 번째 프라이머리 인스턴스”라는 개념은 부정확.

## #218
한 회사가 퍼블릭 서브넷의 Amazon EC2 인스턴스에서 웹 서버를 실행하고 있으며, Elastic IP 주소가 할당되어 있습니다. EC2 인스턴스에는 기본 보안 그룹이 할당되어 있습니다. 기본 네트워크 ACL은 모든 트래픽을 차단하도록 수정되었습니다. 솔루션스 아키텍트는 웹 서버를 포트 443에서 전 세계 어디서든 접근할 수 있도록 만들어야 합니다.

다음 단계 중 어떤 조합이 이 작업을 달성할 수 있습니까? (2개를 선택하십시오.)

A. 소스 0.0.0.0/0에서 TCP 포트 443을 허용하는 규칙이 있는 보안 그룹을 생성합니다.  
B. 대상 0.0.0.0/0으로 TCP 포트 443을 허용하는 규칙이 있는 보안 그룹을 생성합니다.  
C. 소스 0.0.0.0/0에서 TCP 포트 443을 허용하도록 네트워크 ACL을 업데이트합니다.  
D. 소스 0.0.0.0/0 및 대상 0.0.0.0/0에 대해 인바운드/아웃바운드 TCP 포트 443을 허용하도록 네트워크 ACL을 업데이트합니다.  
E. 소스 0.0.0.0/0에서 인바운드 TCP 포트 443을 허용하고, 대상 0.0.0.0/0에 대해 아웃바운드 TCP 포트 32768–65535를 허용하도록 네트워크 ACL을 업데이트합니다.  

```
A company has a web server running on an Amazon EC2 instance in a public subnet with an Elastic IP address. The default security group is assigned to the EC2 instance. The default network ACL has been modified to block all traffic. A solutions architect needs to make the web server accessible from everywhere on port 443.  
  
Which combination of steps will accomplish this task? (Choose two.)

- A. Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.
- B. Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0.
- C. Update the network ACL to allow TCP port 443 from source 0.0.0.0/0.
- D. Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to destination 0.0.0.0/0.
- E. Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0.
```

정답 : `A, E`

- 보안 그룹: 웹 서버는 인바운드 HTTPS(443) 트래픽을 허용해야 하므로 0.0.0.0/0 소스에서 TCP 443을 허용하는 규칙 필요
- 네트워크 ACL: NACL은 stateless이므로 인바운드와 아웃바운드 규칙을 각각 명시
	- 인바운드: 포트 443 허용 (소스 0.0.0.0/0)
	- 아웃바운드: 응답 트래픽은 클라이언트의 임시 포트(ephemeral ports: 클라이언트 측에서 응답을 받을 때 사용하는 임시 포트, 32768-65535 범위)를 사용하므로 해당 범위를 허용

오답 이유

- **B. SG에 “대상 0.0.0.0/0” 규칙**
    - 보안 그룹 규칙은 **대상(destination)** 이 아닌 **소스(source)** 기준으로 정의합니다. 잘못된 옵션.
    
- **C. NACL 인바운드만 허용**
    - NACL은 **stateless**라서 아웃바운드 응답이 없으면 연결 성립 불가. 불완전한 설정.
    
- **D. NACL에서 인/아웃 모두 443 허용**
    - 아웃바운드 응답 트래픽은 클라이언트의 임시 포트를 사용하므로 단순히 443 허용으로는 **응답 불가**. 올바른 방법은 ephemeral 포트 범위 허용(E).


## #219
한 회사의 애플리케이션에 성능 문제가 발생하고 있습니다. 이 애플리케이션은 상태를 유지(stateful)하며 Amazon EC2 인스턴스에서 메모리 내(in-memory) 작업을 완료해야 합니다. 회사는 AWS CloudFormation을 사용하여 인프라를 배포했고 M5 EC2 인스턴스 패밀리를 사용했습니다. 트래픽이 증가함에 따라 애플리케이션 성능이 저하되었습니다. 사용자는 애플리케이션에 액세스하려고 할 때 지연이 발생한다고 보고하고 있습니다.

이 문제를 가장 운영 효율적인 방식으로 해결할 수 있는 솔루션은 무엇입니까?

A. EC2 인스턴스를 Auto Scaling 그룹에서 실행되는 T3 EC2 인스턴스로 교체합니다. AWS Management Console을 사용하여 변경합니다.
B. CloudFormation 템플릿을 수정하여 EC2 인스턴스를 Auto Scaling 그룹에서 실행되도록 합니다. 필요할 때 수동으로 Auto Scaling 그룹의 원하는 용량(desired capacity)과 최대 용량을 늘립니다.
C. CloudFormation 템플릿을 수정합니다. EC2 인스턴스를 R5 EC2 인스턴스로 교체합니다. 향후 용량 계획을 위해 애플리케이션 성능을 추적하도록 Amazon CloudWatch의 기본 제공 EC2 메모리 지표를 사용합니다.
D. CloudFormation 템플릿을 수정합니다. EC2 인스턴스를 R5 EC2 인스턴스로 교체합니다. 향후 용량 계획을 위해 사용자 지정 애플리케이션 지연 시간 지표를 생성하도록 EC2 인스턴스에 Amazon CloudWatch 에이전트를 배포합니다.

```
A company’s application is having performance issues. The application is stateful and needs to complete in-memory tasks on Amazon EC2 instances. The company used AWS CloudFormation to deploy infrastructure and used the M5 EC2 instance family. As traffic increased, the application performance degraded. Users are reporting delays when the users attempt to access the application.  
  
Which solution will resolve these issues in the MOST operationally efficient way?

- A. Replace the EC2 instances with T3 EC2 instances that run in an Auto Scaling group. Make the changes by using the AWS Management Console.
- B. Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group. Increase the desired capacity and the maximum capacity of the Auto Scaling group manually when an increase is necessary.
- C. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon CloudWatch built-in EC2 memory metrics to track the application performance for future capacity planning.
- D. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.
```

정답 : `D`

- 문제의 핵심은 stateful + in-memory 작업으로 CPU보다 메모리 용량/대역이 병목일 가능성이 큼
- 따라서 범용(M5)보다 메모리 최적화형(R5)로 변경하는 것이 직접적인 성능 개선책
- 동시에 운영 효율성을 요구하므로 CloudFormation 템플릿 수정으로 IaC 기준을 유지하고, CloudWatch Agent를 배포해 메모리/애플리케이션 지연(custom matric)을 수집하면 이후 데이터 기반 용량 계획 가능
- Ec2의 메모리 지표는 기본 내장되어 있지 않기 때문에(Agent 필요), D 항목처럼 에이전트를 통한 수집이 맞음

오답 이유

- **A. T3로 교체 + 콘솔 변경**
    - T3는 **버스터블 인스턴스**로 지속 고부하/메모리 집약 워크로드에 부적합하며 버스트 크레딧 소진 시 성능 저하. 또한 콘솔 수동 변경은 **운영 효율성(IaC)** 측면에서 부정적.
    
- **B. ASG로 수평 확장 + 수동 증설**
    - 애플리케이션이 **stateful**이라 수평 확장이 쉽지 않을 수 있고, **수동으로 용량 조절**은 운영 부담이 큼. in-memory 병목 자체 해결책이 아님.
    
- **C. R5 교체 + “기본 제공” 메모리 지표 사용**
    - EC2에는 **기본 메모리 지표가 제공되지 않습니다.** 메모리/애플리케이션 지표는 **CloudWatch Agent** 등으로 **커스텀 수집**해야 하므로 전제 자체가 틀렸습니다.

## #220
한 솔루션스 아키텍트가 Amazon API Gateway를 사용하여 사용자의 요청을 수신하는 새로운 API를 설계하고 있습니다. 요청량은 매우 가변적이며, 몇 시간 동안 단 한 건의 요청도 오지 않을 수 있습니다. 데이터 처리는 비동기적으로 수행되지만, 요청이 발생한 후 몇 초 이내에 완료되어야 합니다.

이 요구사항을 가장 낮은 비용으로 충족하기 위해 API가 호출해야 하는 컴퓨팅 서비스는 무엇입니까?

A. AWS Glue 작업
B. AWS Lambda 함수
C. Amazon Elastic Kubernetes Service(Amazon EKS)에 호스팅된 컨테이너형 서비스
D. Amazon EC2와 함께 Amazon ECS에 호스팅된 컨테이너형 서비스

```
A solutions architect is designing a new API using Amazon API Gateway that will receive requests from users. The volume of requests is highly variable; several hours can pass without receiving a single request. The data processing will take place asynchronously, but should be completed within a few seconds after a request is made.  
  
Which compute service should the solutions architect have the API invoke to deliver the requirements at the lowest cost?

- A. An AWS Glue job
- B. An AWS Lambda function
- C. A containerized service hosted in Amazon Elastic Kubernetes Service (Amazon EKS)
- D. A containerized service hosted in Amazon ECS with Amazon EC2
```

정답 : `B`

- 요청이 드물게 발생하고 몇 시간 동안 0 트래픽일 수 있으므로 유휴 시간에 비용이 들지 않는 서버리스, 스케일-투-제로 모델이 최적
- AWS Lambda는 초당 과금(밀리초 단위)과 자동 확장으로 비용 효율이 높고, 비동기 처리에서도 수 초 내 완료가 가능한 필요한 짧은 작업에 적합
- API Gateway와의 네이티브 통합으로 간단히 연동되고 필요 시 비동기 호출(Invoke async) 또는 SQS 이벤트 기반으로 버퍼링 가능

오답 이유

- **A. AWS Glue 작업**
    - Glue는 서버리스 ETL 배치 중심으로 **초 단위 응답** 요구에 부적합하며, **작업 기동 지연**과 최소 실행 시간 과금이 있어 비용/지연 모두 불리합니다.
    
- **C. Amazon EKS 컨테이너 서비스**
    - 강력한 오케스트레이션이 장점이나, **클러스터 제어 플레인 및 노드 유지 비용/운영 오버헤드**가 존재합니다. 몇 시간 무트래픽 상황에서 **상시 비용**이 발생할 수 있습니다.
    
- **D. Amazon ECS on EC2**
    - EC2 기반은 **인스턴스 상시 기동 비용**이 들며, 0 트래픽 구간에도 비용이 발생합니다. 요구사항의 “최저 비용” 및 **변동 트래픽**에 덜 적합합니다. (참고: Fargate였다면 상시 비용은 줄지만 보기에는 EC2와 함께라고 명시)