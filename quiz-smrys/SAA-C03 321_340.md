---
created: 2025-10-03 11:24:56
last_modified: 2025-10-03 14:51:28
---
## #321
솔루션스 아키텍트는 Amazon S3 버킷에 업로드되는 모든 객체가 암호화되도록 보장해야 합니다.  
이를 위해 무엇을 해야 합니까?

A. PutObject 요청에 s3:x-amz-acl 헤더가 설정되지 않은 경우 거부하도록 버킷 정책을 업데이트합니다.  
B. PutObject 요청에 s3:x-amz-acl 헤더가 private으로 설정되지 않은 경우 거부하도록 버킷 정책을 업데이트합니다.  
C. PutObject 요청에 aws:SecureTransport 헤더가 true로 설정되지 않은 경우 거부하도록 버킷 정책을 업데이트합니다.  
D. PutObject 요청에 x-amz-server-side-encryption 헤더가 설정되지 않은 경우 거부하도록 버킷 정책을 업데이트합니다.

```
What should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?

- A. Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set.
- B. Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private.
- C. Update the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true.
- D. Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set.
```

정답 : `D`

- 객체 업로드 시 서버 측 암호화(SSE)를 강제하려면 x-amz-server-side-encryption 헤더 필요
- 이 헤더가 없으면 업로드를 거부하도록 S3 버킷 정책 설정 가능
- 이를 통해 S3에 저장되는 모든 객체가 자동으로 SSE-S3 또는 SSE-KMS 방식으로 암호화되도록 강제 가능

오답 이유

- **A. s3:x-amz-acl 헤더 확인**
    - ACL(Access Control List)은 객체 접근 권한 제어와 관련 있음. 암호화와는 무관.
    
- **B. s3:x-amz-acl = private 확인**
    - 객체를 private으로 업로드하는 것은 접근 제어에 관한 설정이지 암호화를 보장하지 않음.
    
- **C. aws:SecureTransport 헤더 확인**
    - aws:SecureTransport는 HTTPS(TLS) 사용 여부를 확인하는 조건. 이는 **전송 중 암호화(데이터 인 트랜싯)**를 보장할 뿐, 저장 시 암호화(at rest)와는 관련 없음.


## #322
한 솔루션스 아키텍트가 한 회사의 멀티 티어 애플리케이션을 설계하고 있습니다. 애플리케이션의 사용자는 모바일 기기에서 이미지를 업로드합니다. 애플리케이션은 각 이미지의 썸네일을 생성하고, 이미지가 성공적으로 업로드되었음을 사용자에게 확인하는 메시지를 반환합니다.

썸네일 생성에는 최대 60초가 걸릴 수 있지만, 회사는 원본 이미지가 수신되었음을 사용자에게 알리는 더 빠른 응답 시간을 제공하고자 합니다. 솔루션스 아키텍트는 서로 다른 애플리케이션 계층으로 요청을 비동기적으로 디스패치하도록 애플리케이션을 설계해야 합니다.

이 요구 사항을 충족하려면 솔루션스 아키텍트는 무엇을 해야 합니까?

A. 사용자 정의 AWS Lambda 함수를 작성하여 썸네일을 생성하고 사용자에게 알립니다. 이미지 업로드 프로세스를 이벤트 소스로 사용하여 Lambda 함수를 호출합니다.
B. AWS Step Functions 워크플로를 생성합니다. Step Functions를 구성하여 애플리케이션 계층 간의 오케스트레이션을 처리하고 썸네일 생성이 완료되면 사용자에게 알립니다.
C. Amazon Simple Queue Service(Amazon SQS) 메시지 큐를 생성합니다. 이미지가 업로드될 때마다 썸네일 생성을 위해 SQS 큐에 메시지를 넣습니다. 이미지가 수신되었음을 애플리케이션 메시지를 통해 사용자에게 알립니다.
D. Amazon Simple Notification Service(Amazon SNS) 알림 주제와 구독을 생성합니다. 이미지 업로드가 완료된 후 애플리케이션이 썸네일을 생성하도록 하나의 구독을 사용합니다. 썸네일 생성이 완료된 후 사용자 모바일 앱에 푸시 알림으로 메시지를 보내기 위해 두 번째 구독을 사용합니다.

```
A solutions architect is designing a multi-tier application for a company. The application's users upload images from a mobile device. The application generates a thumbnail of each image and returns a message to the user to confirm that the image was uploaded successfully.  
  
The thumbnail generation can take up to 60 seconds, but the company wants to provide a faster response time to its users to notify them that the original image was received. The solutions architect must design the application to asynchronously dispatch requests to the different application tiers.  
  
What should the solutions architect do to meet these requirements?

- A. Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the image upload process as an event source to invoke the Lambda function.
- B. Create an AWS Step Functions workflow. Configure Step Functions to handle the orchestration between the application tiers and alert the user when thumbnail generation is complete.
- C. Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, place a message on the SQS queue for thumbnail generation. Alert the user through an application message that the image was received.
- D. Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions. Use one subscription with the application to generate the thumbnail after the image upload is complete. Use a second subscription to message the user's mobile app by way of a push notification after thumbnail generation is complete.
```

정답 : `C`

- SQS는 생산자(업로드 계층)와 소비자(썸네일 생성 워커)를 느슨하게 결합하여, 업로드 즉시 "수신 완료"를 사용자에게 반환하고 백그라운드에서 안전하게 처리 가능
- SQS는 내구성, 자동 재시도, 가시성 타임아웃, 확장성을 제공하므로 대량 업로드에도 안전하게 처리
- EC2/Lambda 워커를 수평 확장해 처리량을 쉽게 늘릴 수 있음

오답 이유

- **A. Lambda 단독(업로드 이벤트 → 썸네일 생성 + 사용자 알림)**
    - S3 이벤트로 Lambda를 트리거하는 설계 자체는 비동기이지만, 옵션 A는 “Lambda가 썸네일 생성과 사용자 알림을 모두 처리”하도록 요구합니다. 썸네일 생성이 길어지면(**최대 60초**) 알림 타이밍/신뢰성 관리가 복잡해지고, **큐잉/재시도 제어**가 제한적입니다. 대량 트래픽 시 완충(버퍼) 역할도 부족합니다.
    
- **B. Step Functions 오케스트레이션**
    - 강력한 상태 관리/분기/재시도 기능을 제공하지만, 이번 요구는 **단순 비동기 디커플링 + 빠른 수신 확인**입니다. Step Functions는 **과도한 오버헤드/비용**이 될 수 있으며, 즉시성 응답 자체는 별도 설계가 필요합니다. 기본 문제를 해결하는 최소 해법은 아님.
    
- **D. SNS 주제/구독만으로 처리**
    - SNS는 **푸시 기반 브로드캐스트**(fan-out)에 적합하나, **컨슈머 측 Pull/재처리 보장, 워커 스케일아웃, 처리 보장**에는 SQS가 더 적합합니다. SNS 단독은 **컨슈머 불가용 시 메시지 손실 위험**이 있고, 작업 큐 시맨틱을 충족하지 못합니다(물론 SNS→SQS 패턴은 유효하지만, 보기엔 SQS가 없음).


## #323
한 회사의 시설은 건물 전체의 모든 출입구에 배지 리더기를 갖추고 있습니다. 배지를 스캔하면, 리더기는 해당 출입구에 접근하려 한 사람이 누구인지 표시하기 위해 HTTPS를 통해 메시지를 전송합니다.

솔루션스 아키텍트는 이 센서로부터 온 메시지를 처리할 시스템을 설계해야 합니다. 이 솔루션은 고가용성이어야 하며, 결과는 회사의 보안 팀이 분석할 수 있도록 제공되어야 합니다.

솔루션스 아키텍트는 어떤 시스템 아키텍처를 권장해야 합니까?

A. HTTPS 엔드포인트 역할과 메시지 처리를 위해 Amazon EC2 인스턴스를 시작합니다. EC2 인스턴스가 결과를 Amazon S3 버킷에 저장하도록 구성합니다.
B. Amazon API Gateway에 HTTPS 엔드포인트를 생성합니다. API Gateway 엔드포인트가 AWS Lambda 함수를 호출하여 메시지를 처리하고 결과를 Amazon DynamoDB 테이블에 저장하도록 구성합니다.
C. Amazon Route 53을 사용하여 들어오는 센서 메시지를 AWS Lambda 함수로 라우팅합니다. Lambda 함수가 메시지를 처리하고 결과를 Amazon DynamoDB 테이블에 저장하도록 구성합니다.
D. Amazon S3에 대한 게이트웨이 VPC 엔드포인트를 생성합니다. 시설 네트워크에서 VPC로의 사이트 간(VPN) 연결을 구성하여 센서 데이터가 VPC 엔드포인트를 통해 직접 S3 버킷에 쓰일 수 있도록 합니다.

```
A company’s facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send a message over HTTPS to indicate who attempted to access that particular entrance.  
  
A solutions architect must design a system to process these messages from the sensors. The solution must be highly available, and the results must be made available for the company’s security team to analyze.  
  
Which system architecture should the solutions architect recommend?

- A. Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages. Configure the EC2 instance to save the results to an Amazon S3 bucket.
- B. Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.
- C. Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function. Configure the Lambda function to process the messages and save the results to an Amazon DynamoDB table.
- D. Create a gateway VPC endpoint for Amazon S3. Configure a Site-to-Site VPN connection from the facility network to the VPC so that sensor data can be written directly to an S3 bucket by way of the VPC endpoint.
```

정답 : `B`

- API Gateway(HTTPS 엔드포인트) + 람다(서버리스 처리) + DynamoDB(내구성/확장성 있는 저장) 조합은 완전관리형이며 자동으로 고가용성 제공
- 배지 리더가 HTTPS로 메시지를 전송하므로, API Gateway가 안전한 수신 지점이 됨
- 람다는 무서버로 탄력 확장하며, DynamoDB는 보안팀의 조회/대시보드 집계에 적합한 저지연 조회를 제공

오답 이유

- **A. 단일 EC2 엔드포인트**
    - 단일 인스턴스는 **단일 장애점(SPOF)** 이며, 오토스케일/멀티 AZ/패치/용량 계획 등 운영 부담이 큼. 고가용성 요건을 직접 구성해야 함.
    
- **C. Route 53 → Lambda 직접 라우팅**
    - Route 53은 **DNS 서비스**로, 레코드를 ALB/API Gateway/CloudFront 등으로 매핑할 수 있으나 **Lambda 함수로 직접 라우팅 불가**(중개 엔드포인트 필요). 설계 자체가 부정확.
    
- **D. S3 게이트웨이 엔드포인트 + 사이트 간 VPN**
    - **게이트웨이 VPC 엔드포인트는 VPC 내부 → S3 통신용**이며, 온프레미스에서 직접 쓰는 경로가 아님.
    - 센서가 **S3 PutObject API**를 직접 호출하고 VPN 터널을 유지해야 하는 등 현실적이지 않고 복잡. 또한 **HTTPS 수신 애플리케이션 로직**을 대체하지 못함.


## #324
한 회사는 기본 온프레미스 파일 스토리지 볼륨에 대한 재해 복구 계획을 구현하려고 합니다. 파일 스토리지 볼륨은 로컬 스토리지 서버의 Internet Small Computer Systems Interface(iSCSI) 장치에서 마운트됩니다. 파일 스토리지 볼륨에는 수백 테라바이트(TB)의 데이터가 저장되어 있습니다.

회사는 최종 사용자들이 지연을 경험하지 않고 온프레미스 시스템에서 모든 파일 유형에 즉시 접근할 수 있도록 보장하고자 합니다.

회사의 기존 인프라에 대한 변경을 최소화하면서 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 온프레미스에서 호스팅되는 가상 머신(VM)으로 Amazon S3 File Gateway를 프로비저닝합니다. 로컬 캐시를 10 TB로 설정합니다. 기존 애플리케이션을 수정하여 NFS 프로토콜을 통해 파일에 접근하도록 합니다. 재해로부터 복구하려면 Amazon EC2 인스턴스를 프로비저닝하고 파일이 들어 있는 S3 버킷을 마운트합니다.
B. AWS Storage Gateway 테이프 게이트웨이를 프로비저닝합니다. 데이터 백업 솔루션을 사용하여 모든 기존 데이터를 가상 테이프 라이브러리에 백업합니다. 초기 백업이 완료된 후 데이터 백업 솔루션이 매일 밤 실행되도록 구성합니다. 재해로부터 복구하려면 Amazon EC2 인스턴스를 프로비저닝하고 가상 테이프 라이브러리의 볼륨에서 데이터를 Amazon Elastic Block Store(Amazon EBS) 볼륨으로 복원합니다.
C. AWS Storage Gateway Volume Gateway 캐시형(cached) 볼륨을 프로비저닝합니다. 로컬 캐시를 10 TB로 설정합니다. iSCSI를 사용하여 기존 파일 서버에 Volume Gateway 캐시형 볼륨을 마운트하고 모든 파일을 스토리지 볼륨으로 복사합니다. 스토리지 볼륨의 예약 스냅샷을 구성합니다. 재해로부터 복구하려면 스냅샷을 Amazon Elastic Block Store(Amazon EBS) 볼륨으로 복원하고 EBS 볼륨을 Amazon EC2 인스턴스에 연결합니다.
D. 기존 파일 스토리지 볼륨과 동일한 디스크 용량으로 AWS Storage Gateway Volume Gateway 저장형(stored) 볼륨을 프로비저닝합니다. iSCSI를 사용하여 기존 파일 서버에 Volume Gateway 저장형 볼륨을 마운트하고 모든 파일을 스토리지 볼륨으로 복사합니다. 스토리지 볼륨의 예약 스냅샷을 구성합니다. 재해로부터 복구하려면 스냅샷을 Amazon Elastic Block Store(Amazon EBS) 볼륨으로 복원하고 EBS 볼륨을 Amazon EC2 인스턴스에 연결합니다.

```
A company wants to implement a disaster recovery plan for its primary on-premises file storage volume. The file storage volume is mounted from an Internet Small Computer Systems Interface (iSCSI) device on a local storage server. The file storage volume holds hundreds of terabytes (TB) of data.  
  
The company wants to ensure that end users retain immediate access to all file types from the on-premises systems without experiencing latency.  
  
Which solution will meet these requirements with the LEAST amount of change to the company's existing infrastructure?

- A. Provision an Amazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set the local cache to 10 TB. Modify existing applications to access the files through the NFS protocol. To recover from a disaster, provision an Amazon EC2 instance and mount the S3 bucket that contains the files.
- B. Provision an AWS Storage Gateway tape gateway. Use a data backup solution to back up all existing data to a virtual tape library. Configure the data backup solution to run nightly after the initial backup is complete. To recover from a disaster, provision an Amazon EC2 instance and restore the data to an Amazon Elastic Block Store (Amazon EBS) volume from the volumes in the virtual tape library.
- C. Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10 TB. Mount the Volume Gateway cached volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.
- D. Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of disk space as the existing file storage volume. Mount the Volume Gateway stored volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.
```

정답 : `D`

- Volume Gateway 저장형(stored volumes)은 원본 데이터를 온프레미스에 로컬로 보관하고, 비동기식으로 AWS(S3)로 백업/스냅샷을 올림
	- 따라서 사용자는 평소와 동일한 로컬 디스크 성능/지연으로 접근 가능
- 저장형은 iSCSI 블록 장치를 그대로 노출하므로, 현재 iSCSI로 마운트하는 기존 파일 서버 워크플로우와 가장 잘맞고 애플리케이션 변경 최소화
- DR 시에는 저장형 볼륨의 스냅샷을 EBS로 복원해 EC2에 연결

오답 이유

- **A. S3 File Gateway (NFS/SMB)**
    - 파일 게이트웨이는 **객체(S3) 백엔드 + 파일 프로토콜(NFS/SMB)** 이므로, 기존의 **iSCSI 블록 스토리지** 사용 방식과 다릅니다. 애플리케이션을 **NFS/SMB로 수정**해야 하며, “모든 파일 유형에 즉시 접근” 요구에서 **캐시 미스 시 지연**이 발생할 수 있습니다. 변경량이 큽니다.
    
- **B. 테이프 게이트웨이**
    - 가상 테이프 라이브러리(VTL)는 **백업/아카이브** 용도입니다. 운영 워크로드의 **즉시 접근 스토리지**로 쓰지 않으며, DR 시에도 테이프에서 복원해야 하므로 **RTO가 길고 즉시성 불충족**입니다.
    
- **C. Volume Gateway 캐시형(cached)**
    - 캐시형은 **전체 데이터는 S3, 자주 쓰는 데이터만 로컬 캐시**에 두는 구조입니다. **캐시 미스 시 S3에서 페치**해야 하므로 지연이 커질 수 있어 “모든 파일에 대한 즉시 접근” 요구에 부합하지 않습니다. 또한 온프레미스에 수백 TB 전량을 로컬로 유지하지 않기 때문에 레이턴시 목표와 상충합니다.


## #325
한 회사가 Amazon S3 버킷에서 웹 애플리케이션을 호스팅하고 있습니다. 이 애플리케이션은 Amazon Cognito를 ID 공급자로 사용하여 사용자를 인증하고, 보호된 리소스(다른 S3 버킷에 저장됨)에 접근할 수 있는 JSON Web Token(JWT)을 반환합니다.

애플리케이션 배포 후, 사용자들은 오류를 보고하며 보호된 콘텐츠에 접근할 수 없습니다. 솔루션스 아키텍트는 사용자가 보호된 콘텐츠에 접근할 수 있도록 적절한 권한을 제공하여 이 문제를 해결해야 합니다.

이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. Amazon Cognito ID 풀을 업데이트하여 보호된 콘텐츠에 접근할 수 있는 적절한 IAM 역할을 가정하도록 합니다.  
B. S3 ACL을 업데이트하여 애플리케이션이 보호된 콘텐츠에 접근할 수 있도록 합니다.  
C. 애플리케이션을 Amazon S3에 다시 배포하여 S3 버킷의 최종적 일관성 읽기가 사용자 접근에 영향을 주지 않도록 합니다.  
D. Amazon Cognito 풀을 업데이트하여 ID 풀 내에서 사용자 지정 속성 매핑을 사용하고 사용자에게 보호된 콘텐츠에 접근할 수 있는 적절한 권한을 부여합니다.

```
A company is hosting a web application from an Amazon S3 bucket. The application uses Amazon Cognito as an identity provider to authenticate users and return a JSON Web Token (JWT) that provides access to protected resources that are stored in another S3 bucket.  
  
Upon deployment of the application, users report errors and are unable to access the protected content. A solutions architect must resolve this issue by providing proper permissions so that users can access the protected content.  
  
Which solution meets these requirements?

- A. Update the Amazon Cognito identity pool to assume the proper IAM role for access to the protected content.
- B. Update the S3 ACL to allow the application to access the protected content.
- C. Redeploy the application to Amazon S3 to prevent eventually consistent reads in the S3 bucket from affecting the ability of users to access the protected content.
- D. Update the Amazon Cognito pool to use custom attribute mappings within the identity pool and grant users the proper permissions to access the protected content.
```

정답 : `A`

- Cognito는 사용자 풀(User Pool)과 ID 풀(Identity Pool)로 나뉨
	- 사용자 풀: 인증(Authentication)을 처리하고 JWT 토큰을 반환
	- ID 풀: 권한 부여(Authorization)를 위해 AWS IAM 역할을 연결
- 문제: 사용자는 인증은 성공했지만 S3 보호 콘텐츠 접근 권한이 없음 → ID 풀에 연결된 IAM 역할에 S3 버킷에 대한 올바른 권한 부여 필요
- 따라서 ID 풀을 업데이트하여 S3 접근이 가능한 IAM 역할을 할당

오답 이유

- **B. S3 ACL 업데이트**
    - ACL은 객체/버킷에 대한 저수준 접근 제어 방식. Cognito 기반 인증/권한 부여 시 일반적으로 IAM 정책을 통해 제어합니다. ACL 수정은 근본적인 해결책이 아님.
    
- **C. 애플리케이션을 다시 배포 (S3 eventual consistency 방지)**
    - 현재 S3는 대부분 작업에 대해 **강력한 일관성(strong consistency)** 을 제공. 문제는 일관성이 아니라 **권한 부여 실패**임.
    
- **D. Cognito 사용자 풀의 사용자 지정 속성 매핑**
    - 사용자 풀은 인증 전용. 권한(Authorization)을 제공하지 않음. 따라서 ID 풀과 IAM 역할 연결이 필요하며, 사용자 속성 매핑만으로는 S3 접근 권한이 주어지지 않음.


## #326
한 이미지 호스팅 회사가 큰 자산을 Amazon S3 Standard 버킷에 업로드합니다. 회사는 S3 API를 사용하여 멀티파트 업로드를 병렬로 수행하고, 동일한 객체가 다시 업로드되면 덮어씁니다. 업로드 후 처음 30일 동안은 객체에 자주 액세스됩니다. 30일 이후에는 객체 사용이 덜 빈번해지지만, 각 객체의 액세스 패턴은 일관되지 않습니다. 회사는 저장된 자산의 높은 가용성과 복원력을 유지하면서 S3 저장 비용을 최적화해야 합니다.

이 요구 사항을 충족하기 위해 솔루션스 아키텍트가 권장해야 하는 작업 조합은 무엇입니까? (두 가지를 선택하십시오.)

A. 30일 후 자산을 S3 Intelligent-Tiering으로 이동합니다.
B. 불완전한 멀티파트 업로드를 정리하도록 S3 수명 주기(Lifecycle) 정책을 구성합니다.
C. 만료된 객체 삭제 마커를 정리하도록 S3 수명 주기(Lifecycle) 정책을 구성합니다.
D. 30일 후 자산을 S3 Standard-Infrequent Access(S3 Standard-IA)로 이동합니다.
E. 30일 후 자산을 S3 One Zone-Infrequent Access(S3 One Zone-IA)로 이동합니다.

```
An image hosting company uploads its large assets to Amazon S3 Standard buckets. The company uses multipart upload in parallel by using S3 APIs and overwrites if the same object is uploaded again. For the first 30 days after upload, the objects will be accessed frequently. The objects will be used less frequently after 30 days, but the access patterns for each object will be inconsistent. The company must optimize its S3 storage costs while maintaining high availability and resiliency of stored assets.  
  
Which combination of actions should a solutions architect recommend to meet these requirements? (Choose two.)

- A. Move assets to S3 Intelligent-Tiering after 30 days.
- B. Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.
- C. Configure an S3 Lifecycle policy to clean up expired object delete markers.
- D. Move assets to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.
- E. Move assets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.
```

정답 : `A, B`

- 30일 이후 객체별로 액세스 패턴이 불규칙/예측 불가하므로 자동으로 적절한 계층으로 이동해 비용을 줄여주는 S3 Intelligent-Tiering이 적합
	- 다중 AZ 저장으로 높은 가용성/복원력 유지
- 병렬 멀티파트 업로드를 사용하는 워크로드에서는 완료되지 않은 파트가 비용을 발생시킬 수 있음
	- 라이프사이클의 Incomplete Multipart Uploads 정리로 쓸데없는 스토리지 비용을 제거

오답 이유

- **C. 만료된 삭제 마커 정리**
    - 이는 **버전 관리(versioning)를 사용하는 경우**에 주로 버킷 정리/목록 단순화 목적입니다. 비용 최적화 효과는 제한적이며, 본 요구의 핵심(예측 불가 접근 패턴 비용 절감, 멀티파트 잔여 파트 비용 제거)과 직접적 연관이 적습니다.
    
- **D. 30일 후 Standard-IA 이동**
    - Standard-IA는 드물게 접근되는 객체에 유리하지만 **예측 불가**한 접근 패턴에서는 **불필요한 retrieval 비용**이 발생할 수 있습니다. Intelligent-Tiering이 자동 계층화로 더 적합합니다.
    
- **E. 30일 후 One Zone-IA 이동**
    - **단일 AZ** 저장으로 비용은 저렴하지만 문제에서 요구한 **높은 가용성과 복원력** 조건과 상충합니다. 멀티 AZ 내구성을 유지해야 하므로 부적절합니다.


## #327
한 솔루션스 아키텍트는 Amazon EC2 인스턴스를 호스팅하는 VPC 네트워크를 보호해야 합니다. EC2 인스턴스에는 매우 민감한 데이터가 있으며 프라이빗 서브넷에서 실행됩니다. 회사 정책에 따르면, VPC에서 실행되는 EC2 인스턴스는 서드파티의 URL을 사용하는 소프트웨어 제품 업데이트를 위해 인터넷의 승인된 서드파티 소프트웨어 저장소에만 액세스할 수 있습니다. 다른 인터넷 트래픽은 차단되어야 합니다.

다음 중 어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. 프라이빗 서브넷의 라우트 테이블을 업데이트하여 아웃바운드 트래픽을 AWS Network Firewall 방화벽으로 라우팅합니다. 도메인 목록 규칙 그룹을 구성합니다.
B. AWS WAF 웹 ACL을 설정합니다. 소스 및 대상 IP 주소 범위 집합을 기반으로 트래픽 요청을 필터링하는 사용자 지정 규칙 세트를 생성합니다.
C. 엄격한 인바운드 보안 그룹 규칙을 구현합니다. URL을 지정하여 인터넷의 승인된 소프트웨어 저장소로만 트래픽을 허용하는 아웃바운드 규칙을 구성합니다.
D. EC2 인스턴스 앞에 Application Load Balancer(ALB)를 구성합니다. 모든 아웃바운드 트래픽을 ALB로 전달합니다. 인터넷에 대한 아웃바운드 액세스를 위해 ALB의 타깃 그룹에서 URL 기반 규칙 리스너를 사용합니다.

```
A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party’s URL. Other internet traffic must be blocked.  
  
Which solution meets these requirements?

- A. Update the route table for the private subnet to route the outbound traffic to an AWS Network Firewall firewall. Configure domain list rule groups.
- B. Set up an AWS WAF web ACL. Create a custom set of rules that filter traffic requests based on source and destination IP address range sets.
- C. Implement strict inbound security group rules. Configure an outbound rule that allows traffic only to the authorized software repositories on the internet by specifying the URLs.
- D. Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound traffic to the ALB. Use a URL-based rule listener in the ALB’s target group for outbound access to the internet.
```

정답 : `A`

- AWS Network Firewall은 VPC 내 아웃바운드 트래픽에 대해 도메인 목록(FQDN) 기반 허용/차단 정책을 적용할 수 있는 도메인 리스트 규칙 그룹(stateful)을 제공
- 프라이빗 서브넷의 라우트 테이블을 방화벽 엔드포인트로 향하게 하고(일반적으로 전용 검사 서브넷/egress 경유)
	- 방화벽에서 승인 도메인만 허용(allow list)하여 나머지 인터넷 트래픽을 차단
- 고가용성을 위해 다중 AZ에 방화벽 엔드포인트를 배치할 수 있고, NAT 게이트웨이와 조합해 안전한 egress 아키텍처 구성 가능

오답 이유

- **B. AWS WAF**
    - WAF는 **웹 애플리케이션으로 유입되는 HTTP(S) 인바운드 트래픽**(CloudFront, ALB, API Gateway 등 프런트도어)에 적용합니다. **VPC 인스턴스의 아웃바운드 인터넷 트래픽 제어** 용도가 아니므로 부적합.
    
- **C. 보안 그룹(URL 지정)**
    - 보안 그룹은 **IP/포트 레벨**만 제어 가능하며 **URL/FQDN을 직접 지정할 수 없습니다.** 정책을 URL로 허용하는 요구사항을 충족하지 못함.
    
- **D. ALB로 아웃바운드 프록시**
    - ALB는 **인바운드 로드밸런서**이며 **클라이언트의 아웃바운드 egress 프록시**로 사용하지 않습니다. 또한 “URL 기반 리스너 규칙”은 인바운드 트래픽 라우팅에 적용되는 기능입니다. 요구와 맞지 않습니다.



## #328
한 회사가 AWS 클라우드에 3계층 전자상거래 애플리케이션을 호스팅하고 있습니다. 회사는 웹사이트를 Amazon S3에 호스팅하고, 판매 요청을 처리하는 API와 웹사이트를 통합합니다. 회사는 API를 Application Load Balancer(ALB) 뒤의 세 개의 Amazon EC2 인스턴스에서 호스팅합니다. API는 정적 및 동적 프런트엔드 콘텐츠와 함께 비동기적으로 판매 요청을 처리하는 백엔드 워커로 구성됩니다.

회사는 신제품 출시 이벤트 동안 판매 요청 수가 상당하고 갑작스럽게 증가할 것으로 예상합니다.

모든 요청이 성공적으로 처리되도록 보장하기 위해 솔루션스 아키텍트는 무엇을 권장해야 합니까?

A. 동적 콘텐츠에 대해 Amazon CloudFront 배포를 추가합니다. 트래픽 증가를 처리하기 위해 EC2 인스턴스 수를 늘립니다.
B. 정적 콘텐츠에 대해 Amazon CloudFront 배포를 추가합니다. 네트워크 트래픽을 기준으로 새 인스턴스를 시작하도록 EC2 인스턴스를 Auto Scaling 그룹에 배치합니다.
C. 동적 콘텐츠에 대해 Amazon CloudFront 배포를 추가합니다. API가 처리해야 하는 트래픽을 줄이기 위해 ALB 앞에 Amazon ElastiCache 인스턴스를 추가합니다.
D. 정적 콘텐츠에 대해 Amazon CloudFront 배포를 추가합니다. 웹사이트로부터의 요청을 수신하기 위해 Amazon Simple Queue Service(Amazon SQS) 큐를 추가하여 나중에 EC2 인스턴스가 처리하도록 합니다.

```
A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously.  
  
The company is expecting a significant and sudden increase in the number of sales requests during events for the launch of new products.  
  
What should a solutions architect recommend to ensure that all the requests are processed successfully?

- A. Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in traffic.
- B. Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network traffic.
- C. Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in front of the ALB to reduce traffic for the API to handle.
- D. Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances.
```

정답 : `D`

- 요구사항: 갑작스러운 대규모 급증에도 모든 요청이 유실 없이 처리되도록 보장 → 버스트 흡수(버퍼링)와 비동기 처리가 가능한 아키텍처 필요
- SQS 큐를 API 앞단(웹사이트 → 큐)으로 두면 요청을 내구성 있게 적재
- 뒤단의 EC2 워커가 비동기적으로 풀링하여 처리량에 맞춰 안정적으로 소화 가능
	- 가시성 타임아웃/재시도/DLQ 등으로 신뢰성 강화
- 동시에 정적 콘텐츠는 CloudFront로 캐싱해 원본(S3/ALB) 부하와 지연을 줄여 프런트 응답성을 개선

오답 이유

- **A. 동적 콘텐츠에 CloudFront + EC2 수 증설**
    - 동적 API는 일반적으로 캐싱 이점이 제한적입니다. 또한 **인스턴스 수 동적 증설만으로는 순간 버스트**에서 스케일 아웃 지연 동안 요청 유실/스로틀링이 발생할 수 있습니다. **버퍼(큐)** 부재.
    
- **B. 정적 콘텐츠에 CloudFront + Auto Scaling**
    - 정적 캐싱은 적절하지만, 핵심인 **비동기 처리 보장**이 없습니다. Auto Scaling은 급증 시 **콜드 스타트/프로비저닝 지연**이 발생할 수 있어 “모든 요청 처리 보장”에 부족.
    
- **C. 동적 콘텐츠에 CloudFront + ALB 앞 ElastiCache**
    - ElastiCache는 **API 호출 자체를 캐시하기 어려운 동적 요청**(판매 요청과 같은 쓰기/주문 처리)에 부적합합니다. 주문 요청은 캐시 적중 대상이 아니며, **버스트 흡수 기능(큐)** 을 제공하지 않습니다.


## #329
보안 감사에서 Amazon EC2 인스턴스가 정기적으로 패치되지 않고 있다는 사실이 드러났습니다. 
솔루션스 아키텍트는 많은 수의 EC2 인스턴스에 대해 정기적인 보안 스캔을 실행해야 하는 솔루션을 제공해야 합니다. 
이 솔루션은 또한 EC2 인스턴스를 정기 일정에 따라 패치하고 각 인스턴스의 패치 상태 보고서를 제공해야 합니다.

이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. Amazon Macie를 설정하여 EC2 인스턴스의 소프트웨어 취약성을 스캔합니다. 각 EC2 인스턴스에 cron 작업을 설정하여 정기 일정에 따라 인스턴스를 패치합니다.  
B. 계정에서 Amazon GuardDuty를 켭니다. GuardDuty를 구성하여 EC2 인스턴스의 소프트웨어 취약성을 스캔합니다. AWS Systems Manager Session Manager를 설정하여 EC2 인스턴스를 정기 일정에 따라 패치합니다.  
C. Amazon Detective를 설정하여 EC2 인스턴스의 소프트웨어 취약성을 스캔합니다. Amazon EventBridge 예약 규칙을 설정하여 EC2 인스턴스를 정기 일정에 따라 패치합니다.  
D. 계정에서 Amazon Inspector를 켭니다. Amazon Inspector를 구성하여 EC2 인스턴스의 소프트웨어 취약성을 스캔합니다. AWS Systems Manager Patch Manager를 설정하여 EC2 인스턴스를 정기 일정에 따라 패치합니다.

```
A security audit reveals that Amazon EC2 instances are not being patched regularly. A solutions architect needs to provide a solution that will run regular security scans across a large fleet of EC2 instances. The solution should also patch the EC2 instances on a regular schedule and provide a report of each instance’s patch status.  
  
Which solution will meet these requirements?

- A. Set up Amazon Macie to scan the EC2 instances for software vulnerabilities. Set up a cron job on each EC2 instance to patch the instance on a regular schedule.
- B. Turn on Amazon GuardDuty in the account. Configure GuardDuty to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Session Manager to patch the EC2 instances on a regular schedule.
- C. Set up Amazon Detective to scan the EC2 instances for software vulnerabilities. Set up an Amazon EventBridge scheduled rule to patch the EC2 instances on a regular schedule.
- D. Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Patch Manager to patch the EC2 instances on a regular schedule.
```

정답 : `D`

- Amazon Inspector: 자동으로 EC2 인스턴스와 컨테이너 워크로드에 대해 취약성 스캔 및 보안 평가를 수행, 취약점 보고서를 제공해 컴플라이언스와 보안 감사 요건 충족
- AWS System Manager Patch Manager: EC2 인스턴스 패치를 자동화된 일정에 따라 적용하고 각 인스턴스의 패치 컴플라이언스 리포트 제공

오답 이유

- **A. Amazon Macie + cron**
    - Macie는 **S3의 민감 데이터 식별/분류**에 특화되어 있으며, EC2 취약성 스캔을 지원하지 않습니다.
    - cron 기반 패치는 개별 인스턴스 관리가 필요해 **대규모 플릿 관리 비효율**.
    
- **B. GuardDuty + Session Manager**
    - GuardDuty는 **위협 탐지(악성 활동, 비정상 API 호출)** 서비스이지 소프트웨어 취약성 스캔 도구가 아닙니다.
    - Session Manager는 원격 접속 도구이지 패치 자동화 도구가 아님.
    
- **C. Detective + EventBridge**
    - Detective는 **보안 이벤트 관계 분석/조사 도구**이지 취약성 스캔이나 패치와 관련이 없습니다.
    - EventBridge 규칙만으로는 패치 관리 및 보고 기능을 제공하지 않음.


## #330
한 회사가 Amazon RDS DB 인스턴스에 데이터를 저장할 계획입니다. 
회사는 데이터를 저장 시(at rest) 암호화해야 합니다.

이 요구 사항을 충족하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?

A. AWS Key Management Service(AWS KMS)에서 키를 생성합니다. DB 인스턴스에 대해 암호화를 활성화합니다.  
B. 암호화 키를 생성합니다. 키를 AWS Secrets Manager에 저장합니다. 해당 키를 사용하여 DB 인스턴스를 암호화합니다.  
C. AWS Certificate Manager(ACM)에서 인증서를 생성합니다. 인증서를 사용하여 DB 인스턴스에서 SSL/TLS를 활성화합니다.  
D. AWS Identity and Access Management(IAM)에서 인증서를 생성합니다. 인증서를 사용하여 DB 인스턴스에서 SSL/TLS를 활성화합니다.

```
A company is planning to store data on Amazon RDS DB instances. The company must encrypt the data at rest.  
  
What should a solutions architect do to meet this requirement?

- A. Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances.
- B. Create an encryption key. Store the key in AWS Secrets Manager. Use the key to encrypt the DB instances.
- C. Generate a certificate in AWS Certificate Manager (ACM). Enable SSL/TLS on the DB instances by using the certificate.
- D. Generate a certificate in AWS Identity and Access Management (IAM). Enable SSL/TLS on the DB instances by using the certificate.
```

정답 : `A`

- Amazon RDS에서 데이터 저장 시 암호화는 AWS Key Management Service(KMS)와 통합되어 제공
- RDS 인스턴스 생성 시 KMS CMK(Customer Managed Key 또는 AWS Managed Key)를 지정해 암호화 활성 가능
- 이 방식은 데이터 파일, 자동 백업, 스냅샷, 읽기 복제본까지 모두 암호화

오답 이유

- **B. Secrets Manager에 키 저장**
    - Secrets Manager는 암호/자격 증명 관리용 서비스입니다. RDS 인스턴스 암호화 키 저장/관리에는 사용되지 않습니다.
    
- **C. ACM 인증서 + SSL/TLS**
    - ACM 인증서는 **전송 중 암호화(in transit)** 용도로 사용합니다. 저장 시 암호화와는 무관합니다.
    
- **D. IAM 인증서 + SSL/TLS**
    - IAM 인증서 기능은 주로 이전 세대의 SSL 인증서 저장소였으며, 역시 **전송 중 암호화**용이지 저장 시 암호화 기능과는 관계가 없습니다.


## #331
한 회사는 20TB의 데이터를 데이터 센터에서 AWS 클라우드로 30일 이내에 마이그레이션해야 합니다. 
회사의 네트워크 대역폭은 15Mbps로 제한되며, 70% 이상 사용할 수 없습니다.

솔루션스 아키텍트는 이 요구 사항을 충족하기 위해 무엇을 해야 합니까?

A. AWS Snowball을 사용합니다.  
B. AWS DataSync를 사용합니다.  
C. 보안 VPN 연결을 사용합니다.  
D. Amazon S3 Transfer Acceleration을 사용합니다.

```
A company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The company’s network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization.  
  
What should a solutions architect do to meet these requirements?

- A. Use AWS Snowball.
- B. Use AWS DataSync.
- C. Use a secure VPN connection.
- D. Use Amazon S3 Transfer Acceleration.
```

정답 : `A`

- 네트워크 대역폭은 15Mbps이며 70%만 사용 가능 → 실제 사용 가능 대역폭은 약 10.5Mbps
- 10.5Mbps ≈ 1.31MB/s → 하루 전송량 약 113GB → 30일 동안 최대 전송량 = 약 3.4TB
- 따라서 20TB는 네트워크로 전송 불가능
- AWS Snowball은 물리적 디바이스를 데이터센터로 배송받아 데이터를 로컬에 복사 후 AWS로 반송하는 방식으로, 대규모 데이터 마이그레이션에 최적

오답 이유

- **B. AWS DataSync**
    - 네트워크를 통해 데이터 마이그레이션을 자동화할 수 있지만, 현재 대역폭 제한(15 Mbps)으로 20 TB 전송은 30일 내 불가능.
    
- **C. VPN 연결**
    - VPN은 네트워크 전송을 암호화하지만, **대역폭 한계를 해결하지 못함**. 전송 속도 계산상 불가능.
    
- **D. S3 Transfer Acceleration**
    - 전송 속도를 글로벌 엣지 로케이션을 통해 가속하지만, **네트워크 대역폭 자체의 제한**은 극복하지 못함. 결국 불가능.


## #332
한 회사가 직원들에게 기밀하고 민감한 파일에 대한 안전한 액세스를 제공해야 합니다. 회사는 파일이 승인된 사용자만 액세스할 수 있도록 보장하고자 합니다. 파일은 직원들의 디바이스에 안전하게 다운로드되어야 합니다.

파일은 온프레미스 Windows 파일 서버에 저장되어 있습니다. 그러나 원격 사용 증가로 인해 파일 서버의 용량이 부족해지고 있습니다.
.
이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 파일 서버를 퍼블릭 서브넷의 Amazon EC2 인스턴스로 마이그레이션합니다. 보안 그룹을 구성하여 인바운드 트래픽을 직원들의 IP 주소로 제한합니다.
B. 파일을 Amazon FSx for Windows File Server 파일 시스템으로 마이그레이션합니다. Amazon FSx 파일 시스템을 온프레미스 Active Directory와 통합합니다. AWS Client VPN을 구성합니다.
C. 파일을 Amazon S3로 마이그레이션하고, 프라이빗 VPC 엔드포인트를 생성합니다. 다운로드를 허용하기 위해 서명된 URL을 생성합니다.
D. 파일을 Amazon S3로 마이그레이션하고, 퍼블릭 VPC 엔드포인트를 생성합니다. 직원들이 AWS IAM Identity Center(AWS Single Sign-On)로 로그인하도록 허용합니다.

```
A company needs to provide its employees with secure access to confidential and sensitive files. The company wants to ensure that the files can be accessed only by authorized users. The files must be downloaded securely to the employees’ devices.  
  
The files are stored in an on-premises Windows file server. However, due to an increase in remote usage, the file server is running out of capacity.  
.  
Which solution will meet these requirements?

- A. Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security group to limit inbound traffic to the employees’ IP addresses.
- B. Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon FSx file system with the on-premises Active Directory. Configure AWS Client VPN.
- C. Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow download.
- D. Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on with AWS IAM Identity Center (AWS Single Sign-On).
```

정답 : `B`

- FSx for Windows File Server는 완전관리형 Windows 네이티브 SMB 파일 시스템으로 NTFS 권한/ACL, AD 통합, 파일 잠금 등 기업 파일 서버 기능을 그대로 제공
- 승인된 사용자만 접근하도록 온프레미스 Active Directory와 통합해 기존 그룹/권한을 재사용 가능
- AWS Client VPN을 사용하면 원격 직원 단말과 VPC 간 트래픽이 암호화된 터널로 전송되어 안전한 다운로드 가능하며 사내망처럼 SMB 공유 사용 가능

오답 이유

- **A. EC2(퍼블릭 서브넷)로 마이그레이션**
    - 퍼블릭 서브넷에 Windows 파일 서버를 직접 노출하는 것은 보안상 바람직하지 않으며, **IP 기반 차단만으로 ‘승인 사용자’ 보장을 충족하기 어렵고** AD 기반 세밀 권한/감사를 유지하기 힘듭니다. 운영 오버헤드도 큽니다.
    
- **C. S3 + 프라이빗 엔드포인트 + 서명 URL**
    - S3는 객체 스토리지로 **SMB/NTFS 권한 모델을 대체하지 못함**. 서명 URL은 링크를 가진 사람이 접근할 수 있어 **사용자/그룹 단위 세밀 권한 및 감사** 요구를 충족하기 어렵습니다. 또한 단말이 VPCE에 도달하려면 별도 네트워킹(예: VPN/Direct Connect)이 필요합니다.
    
- **D. S3 + 퍼블릭 엔드포인트 + IAM Identity Center**
    - “퍼블릭 VPC 엔드포인트”는 개념적으로도 부정확(엔드포인트는 VPC 내부 리소스). 무엇보다 **SMB 기반 기업 파일 공유, NTFS/AD 권한을 유지**하는 요구와 맞지 않습니다. 객체 단위 접근 제어로의 재설계가 필요합니다.



## #333
한 회사의 애플리케이션은 Application Load Balancer(ALB) 뒤에서 Amazon EC2 인스턴스에서 실행됩니다. 인스턴스는 여러 가용 영역에 걸친 Amazon EC2 Auto Scaling 그룹에서 실행됩니다. 매달 첫째 날 자정에 월말 재무 계산 배치가 실행되면 애플리케이션이 훨씬 느려집니다. 이로 인해 EC2 인스턴스의 CPU 사용률이 즉시 100%로 치솟아 애플리케이션에 장애가 발생합니다.

애플리케이션이 작업 부하를 처리하고 다운타임을 피할 수 있도록 솔루션스 아키텍트는 무엇을 권장해야 합니까?

A. ALB 앞에 Amazon CloudFront 배포를 구성합니다.
B. CPU 사용률을 기반으로 EC2 Auto Scaling의 단순 스케일링 정책을 구성합니다.
C. 월간 일정에 따라 EC2 Auto Scaling의 예약 스케일링 정책을 구성합니다.
D. 일부 워크로드를 EC2 인스턴스에서 제거하기 위해 Amazon ElastiCache를 구성합니다.

```
A company’s application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. On the first day of every month at midnight, the application becomes much slower when the month-end financial calculation batch runs. This causes the CPU utilization of the EC2 instances to immediately peak to 100%, which disrupts the application.  
  
What should a solutions architect recommend to ensure the application is able to handle the workload and avoid downtime?

- A. Configure an Amazon CloudFront distribution in front of the ALB.
- B. Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.
- C. Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.
- D. Configure Amazon ElastiCache to remove some of the workload from the EC2 instances.
```

정답 : `C`

- 부하가 매달 1일 00시 00분에 예측 가능하게 급증하므로, 오토스케일링의 Scheduled Scaling을 사용해 배치 시작 직전에 용량을 선제적으로 상향
- 즉시 충분한 인스턴스가 가동되어 CPU 포화와 다운타임 방지 가능
- 배치 종료 시점에 다시 용량을 하양 조정해 비용도 최적화

오답 이유

- **A. CloudFront**: 정적/캐시 가능 콘텐츠의 지연·원본 부하를 줄이는 용도입니다. 문제의 병목은 **EC2에서의 CPU 집약적 배치 처리**로 캐싱으로 해결되지 않습니다.
    
- **B. 단순 스케일링(CPU 기반)**: **반응형**이므로 CPU가 이미 100%에 도달한 뒤에 스케일아웃이 시작됩니다. 콜드스타트/프로비저닝 지연 동안 성능 저하가 발생할 수 있어 본 시나리오에 부적합합니다.
    
- **D. ElastiCache**: 읽기 캐시/세션 분산에는 효과적이지만, **월말 정산 배치 같은 계산형 부하**를 제거하지 못합니다. 근본 원인 해결책이 아닙니다.


## #334
한 회사는 고객이 온프레미스 Microsoft Active Directory를 사용하여 Amazon S3에 저장된 파일을 다운로드할 수 있도록 하려 합니다. 고객의 애플리케이션은 파일을 다운로드하기 위해 SFTP 클라이언트를 사용합니다.

운영 오버헤드를 최소화하고 고객 애플리케이션에 변경 없이 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. Amazon S3용 SFTP가 있는 AWS Transfer Family를 설정합니다. 통합된 Active Directory 인증을 구성합니다.
B. AWS Database Migration Service(AWS DMS)를 설정하여 온프레미스 클라이언트를 Amazon S3와 동기화합니다. 통합된 Active Directory 인증을 구성합니다.
C. AWS DataSync을 설정하여 AWS IAM Identity Center(AWS Single Sign-On)를 사용해 온프레미스 위치와 S3 위치 간 동기화를 수행합니다.
D. Windows Amazon EC2 인스턴스에 SFTP를 설정하여 온프레미스 클라이언트를 Amazon S3와 연결합니다. AWS Identity and Access Management(IAM)와 통합합니다.

```
A company wants to give a customer the ability to use on-premises Microsoft Active Directory to download files that are stored in Amazon S3. The customer’s application uses an SFTP client to download the files.  
  
Which solution will meet these requirements with the LEAST operational overhead and no changes to the customer’s application?

- A. Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory authentication.
- B. Set up AWS Database Migration Service (AWS DMS) to synchronize the on-premises client with Amazon S3. Configure integrated Active Directory authentication.
- C. Set up AWS DataSync to synchronize between the on-premises location and the S3 location by using AWS IAM Identity Center (AWS Single Sign-On).
- D. Set up a Windows Amazon EC2 instance with SFTP to connect the on-premises client with Amazon S3. Integrate AWS Identity and Access Management (IAM).
```

정답 : `A`

- AWS Transfer Family는 SFTP(또는 FTPS/FTP) 프로토콜로 S3를 백엔드로 제공
- AWS Directory Service for Microsoft AD와 통합하여 온프레미스 AD 자격 증명으로 인증 수행 가능
- 고객 측은 기존 SFTP 클라이언트 그대로 사용해 애플리케이션 변경이 필요 없고, 완전관리형 서비스로 운영 오버헤드가 적음

오답 이유

- **B. AWS DMS 동기화**
    - DMS는 **데이터베이스 마이그레이션/복제** 서비스입니다. SFTP 클라이언트로 파일을 내려받는 요구와 무관하며 S3 파일 전송/인증 문제를 해결하지 못합니다.
    
- **C. DataSync + IAM Identity Center**
    - DataSync은 디렉터리 간 **파일 동기화/이관**용으로, **클라이언트의 SFTP 접근**을 제공하지 않습니다. 또한 IAM Identity Center는 SFTP 클라이언트 인증에 직접 쓰지 않습니다.
    
- **D. EC2에 직접 SFTP 서버 구축**
    - SFTP 서버 운영, 패치, 확장/가용성 관리 등 **운영 부담이 큼**. S3 연동도 별도 구현이 필요할 수 있어 **최소 오버헤드** 요건에 부적합합니다.


## #335
한 회사가 수요가 갑작스럽게 증가하고 있습니다. 회사는 Amazon Machine Image(AMI)에서 대형 Amazon EC2 인스턴스를 프로비저닝해야 합니다. 인스턴스는 Auto Scaling 그룹에서 실행될 것입니다. 회사는 수요를 충족하기 위해 초기화 지연 시간을 최소화하는 솔루션이 필요합니다.

어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. aws ec2 register-image 명령을 사용하여 스냅샷에서 AMI를 생성합니다. AWS Step Functions를 사용하여 Auto Scaling 그룹의 AMI를 교체합니다.
B. 스냅샷에서 Amazon Elastic Block Store(Amazon EBS) 빠른 스냅샷 복구(Fast Snapshot Restore)를 활성화합니다. 해당 스냅샷을 사용하여 AMI를 프로비저닝합니다. 새로운 AMI로 Auto Scaling 그룹의 AMI를 교체합니다.
C. Amazon Data Lifecycle Manager(Amazon DLM)에서 AMI 생성을 활성화하고 수명 주기 규칙을 정의합니다. Auto Scaling 그룹의 AMI를 수정하는 AWS Lambda 함수를 생성합니다.
D. Amazon EventBridge를 사용하여 AMI를 프로비저닝하는 AWS Backup 수명 주기 정책을 호출합니다. EventBridge에서 Auto Scaling 그룹 용량 한도를 이벤트 소스로 구성합니다.

```
A company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand.  
  
Which solution meets these requirements?

- A. Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step Functions to replace the AMI in the Auto Scaling group.
- B. Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.
- C. Enable AMI creation and define lifecycle rules in Amazon Data Lifecycle Manager (Amazon DLM). Create an AWS Lambda function that modifies the AMI in the Auto Scaling group.
- D. Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure Auto Scaling group capacity limits as an event source in EventBridge.
```

정답 : `B`

- 대형 EC2 인스턴스(대용량 EBS)를 급증하는 수요에 맞춰 빠르게 기동하려면 EBS 볼륨 초기화 시간(초기화/프리워밍)을 최소화
- Amazon EBS Fast Snapshot Restore(FSR)를 스냅샷에 활성화하면, 해당 스냅샷에서 생성되는 볼륨이 즉시 최대 성능으로 준비
	- 인스턴스 부팅/초기화 지연을 크게 단축(대용량/고IO 볼륨일 수록 효과가 큼)
- 이 스냅샷을 루트/데이터 볼륨으로 사용하는 AMI로 교체하면 오토 스케일링이 스케일 아웃 시 최소한의 초기화 지연으로 새 인스턴스 가동 가능

오답 이유

- **A. register-image + Step Functions**
    - AMI를 만드는 과정의 자동화는 가능하나, **볼륨 초기화 지연 자체를 줄이지는 못함**. 요구사항 핵심(초기화 지연 최소화)에 직결되지 않음.
    
- **C. DLM로 AMI 수명주기 + Lambda 교체**
    - AMI의 **생성/회전 자동화**에 초점. 역시 **런치 시점의 볼륨 초기화 지연** 문제를 해결하지 못함. 운영 자동화는 도움이 되지만 정답 요건과 불일치.
    
- **D. EventBridge + AWS Backup 정책**
    - 백업/AMI 생성 워크플로우 트리거일 뿐, **인스턴스 기동 지연**을 줄이는 메커니즘이 아님. Auto Scaling 용량 한도 이벤트와도 직접 연관이 약함.


## #336
한 회사가 스토리지로 Amazon Aurora MySQL DB 클러스터를 사용하는 다중 계층 웹 애플리케이션을 호스팅하고 있습니다. 애플리케이션 계층은 Amazon EC2 인스턴스에서 호스팅됩니다. 회사의 IT 보안 가이드라인은 데이터베이스 자격 증명이 암호화되어야 하며 14일마다 회전되어야 한다고 규정합니다.

가장 적은 운영 노력으로 이 요구 사항을 충족하려면 솔루션스 아키텍트는 무엇을 해야 합니까?

A. 새로운 AWS Key Management Service(AWS KMS) 암호화 키를 생성합니다. AWS Secrets Manager를 사용하여 적절한 자격 증명을 갖는 KMS 키를 사용하는 새로운 시크릿을 생성합니다. 시크릿을 Aurora DB 클러스터와 연결합니다. 사용자 지정 회전 기간을 14일로 구성합니다.
B. AWS Systems Manager Parameter Store에 두 개의 파라미터를 생성합니다. 하나는 문자열 파라미터로 사용자 이름을, 다른 하나는 SecureString 유형으로 비밀번호를 저장합니다. 비밀번호 파라미터에 대해 AWS Key Management Service(AWS KMS) 암호화를 선택하고, 이 파라미터들을 애플리케이션 계층에서 로드합니다. 14일마다 비밀번호를 회전하는 AWS Lambda 함수를 구현합니다.
C. 자격 증명이 포함된 파일을 AWS Key Management Service(AWS KMS)로 암호화된 Amazon Elastic File System(Amazon EFS) 파일 시스템에 저장합니다. EFS 파일 시스템을 애플리케이션 계층의 모든 EC2 인스턴스에 마운트합니다. 파일 시스템에서 애플리케이션이 파일을 읽을 수 있고 슈퍼 사용자만 파일을 수정할 수 있도록 파일 접근을 제한합니다. 14일마다 Aurora의 키를 회전하고 새 자격 증명을 파일에 기록하는 AWS Lambda 함수를 구현합니다.
D. 자격 증명이 포함된 파일을 애플리케이션이 자격 증명을 로드하는 데 사용하는 AWS Key Management Service(AWS KMS)로 암호화된 Amazon S3 버킷에 저장합니다. 올바른 자격 증명이 사용되도록 파일을 정기적으로 애플리케이션에 다운로드합니다. 14일마다 Aurora 자격 증명을 회전하고 해당 자격 증명을 S3 버킷의 파일에 업로드하는 AWS Lambda 함수를 구현합니다.

```
A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company’s IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days.  
  
What should a solutions architect do to meet this requirement with the LEAST operational effort?

- A. Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.
- B. Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the SecureString type for the password. Select AWS Key Management Service (AWS KMS) encryption for the password parameter, and load these parameters in the application tier. Implement an AWS Lambda function that rotates the password every 14 days.
- C. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all EC2 instances of the application tier. Restrict the access to the file on the file system so that the application can read the file and that only super users can modify the file. Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials into the file.
- D. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials are used. Implement an AWS Lambda function that rotates the Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket.
```

정답 : `A`

- AWS Secrets Manager는 RDS/Aurora 자격 증명에 대해 내장 회전 기능(관리형 람다 포홤)을 제공하며 KMS로 암호화를 기본 통합
- 시크릿을 Aurora MySQL과 연결하고 회전 주기를 14일로 설정하면, 자동으로 보안 기준(암호화 + 정기 회전 + 상태 보고) 충족
- 애플리케이션은 SDK/환경변수/프록시 등을 통해 시크릿을 동적으로 조회하므로 운영 오버헤드 최소

오답 이유

- **B. Parameter Store + 커스텀 Lambda 회전**
    - 가능하나 **회전 로직을 직접 구현/운영**해야 하고, 데이터베이스 비밀번호 회전에 특화된 관리형 통합이 없어 **운영 부담**이 큼. Secrets Manager가 요구사항을 더 간단히 만족.
    
- **C. EFS에 파일 저장 + 커스텀 회전**
    - 자격 증명 파일 관리/권한/배포, 회전 시 파일 동기화 등 **운영 복잡도**가 큼. 또한 애플리케이션 재로딩/캐싱 이슈까지 고려 필요.
    
- **D. S3에 파일 저장 + 커스텀 회전**
    - 마찬가지로 파일 배포/동기화/버전 관리/캐시 갱신 처리 등 **운영 부담**이 큼. RDS/Aurora와의 **관리형 회전 통합 부재**.


## #337
한 회사가 AWS에 웹 애플리케이션을 배포했습니다. 회사는 확장 요구를 지원하기 위해 Amazon RDS for MySQL에 기본(Primary) DB 인스턴스와 5개의 읽기 전용 복제본(Read Replica)을 호스팅합니다. 읽기 전용 복제본은 기본 DB 인스턴스보다 1초 이상 지연되어서는 안 됩니다. 데이터베이스는 정기적으로 예약된 저장 프로시저를 실행합니다.

웹사이트 트래픽이 증가함에 따라, 피크 부하 기간 동안 복제본에 추가 지연이 발생합니다. 솔루션스 아키텍트는 가능한 한 복제 지연을 줄여야 합니다. 또한 애플리케이션 코드 변경을 최소화하고 지속적인 운영 오버헤드를 최소화해야 합니다.

이 요구 사항을 충족할 수 있는 솔루션은 무엇입니까?

A. 데이터베이스를 Amazon Aurora MySQL로 마이그레이션합니다. 읽기 전용 복제본을 Aurora Replica로 대체하고, Aurora Auto Scaling을 구성합니다. 저장 프로시저를 Aurora MySQL 네이티브 함수로 교체합니다.
B. 데이터베이스 앞단에 Amazon ElastiCache for Redis 클러스터를 배포합니다. 애플리케이션이 데이터베이스를 쿼리하기 전에 캐시를 확인하도록 수정합니다. 저장 프로시저를 AWS Lambda 함수로 대체합니다.
C. 데이터베이스를 Amazon EC2 인스턴스에서 실행되는 MySQL 데이터베이스로 마이그레이션합니다. 모든 복제 노드에 대해 대형, 컴퓨팅 최적화 EC2 인스턴스를 선택합니다. 저장 프로시저는 EC2 인스턴스에 유지합니다.
D. 데이터베이스를 Amazon DynamoDB로 마이그레이션합니다. 필요한 처리량을 지원하기 위해 많은 읽기 용량 단위(RCU)를 프로비저닝하고, 온디맨드 용량 확장을 구성합니다. 저장 프로시저를 DynamoDB Streams로 대체합니다.

```
A company has deployed a web application on AWS. The company hosts the backend database on Amazon RDS for MySQL with a primary DB instance and five read replicas to support scaling needs. The read replicas must lag no more than 1 second behind the primary DB instance. The database routinely runs scheduled stored procedures.  
  
As traffic on the website increases, the replicas experience additional lag during periods of peak load. A solutions architect must reduce the replication lag as much as possible. The solutions architect must minimize changes to the application code and must minimize ongoing operational overhead.  
  
Which solution will meet these requirements?

- A. Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and configure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions.
- B. Deploy an Amazon ElastiCache for Redis cluster in front of the database. Modify the application to check the cache before the application queries the database. Replace the stored procedures with AWS Lambda functions.
- C. Migrate the database to a MySQL database that runs on Amazon EC2 instances. Choose large, compute optimized EC2 instances for all replica nodes. Maintain the stored procedures on the EC2 instances.
- D. Migrate the database to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput, and configure on-demand capacity scaling. Replace the stored procedures with DynamoDB streams.
```

정답 : `A`

- Aurora MySQL은 공유 스토리지(분산 로그-구동 스토리지) 위에서 복제본과 기본 인스턴스가 동일 스토리지를 공유하므로, 전통적 MySQL 리플리케이션(RDS MySQL의 빈동기/준동기) 대비 복제 지연이 밀리초 단위로 매우 작음
- Aurora Replicas는 최대 15개까지 가능하며, 리더 엔드포인트로 자동 분산 및 장애 조치가 가능해 운영 오버헤드를 줄임
- Aurora Auto Scaling(읽기 복제 자동 확장) 으로 피크 부하 시 읽기 용량을 자동으로 늘려 지연을 추가로 완화
- 저장 프로시저 호환성: Aurora MySQL은 MySQL 호환이므로 대부분의 프로시저가 동작

오답 이유

- **B. ElastiCache 도입 + 캐시 퍼스트 + SP를 Lambda로 교체**
    - 캐싱은 읽기 부하를 줄일 수 있지만, **쓰기/리플리케이션 경로의 지연** 문제를 근본적으로 해결하지 못합니다.
    - 게다가 **애플리케이션 코드 변경**과 **저장 프로시저를 Lambda로 재작성**은 큰 리팩토링으로 요구사항(코드 변경 최소화)에 반합니다.
    
- **C. EC2 자가 관리 MySQL로 마이그레이션**
    - 인스턴스 스펙을 키워도 **MySQL 복제 지연의 구조적 한계**는 남습니다.
    - 패치/백업/장애조치/모니터링 등 **운영 오버헤드 급증** → 요구조건에 부적합.
    
- **D. DynamoDB로 마이그레이션**
    - 스키마, 쿼리, 트랜잭션 모델이 달라 **광범위한 애플리케이션 재작성**이 필요합니다.
    - 저장 프로시저 대체 등 변경 폭이 매우 큼. 요구조건 위반.


## #338
한 솔루션스 아키텍트가 대량 트래픽 SaaS 플랫폼의 재해 복구(DR) 계획을 수립해야 합니다. 이 플랫폼의 모든 데이터는 Amazon Aurora MySQL DB 클러스터에 저장되어 있습니다.

DR 계획은 데이터를 보조(세컨더리) AWS 리전에 복제해야 합니다.

다음 중 가장 비용 효율적으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 보조 리전의 Aurora 클러스터로 MySQL 바이너리 로그 복제를 사용합니다. 보조 리전의 Aurora 클러스터에 DB 인스턴스 하나를 프로비저닝합니다.
B. DB 클러스터에 대해 Aurora 글로벌 데이터베이스를 설정합니다. 설정이 완료되면 보조 리전의 DB 인스턴스를 제거합니다.
C. AWS Database Migration Service(AWS DMS)를 사용하여 보조 리전의 Aurora 클러스터로 지속적으로 데이터를 복제합니다. 보조 리전의 DB 인스턴스를 제거합니다.
D. DB 클러스터에 대해 Aurora 글로벌 데이터베이스를 설정합니다. 보조 리전에 최소 하나의 DB 인스턴스를 지정합니다.

```
A solutions architect must create a disaster recovery (DR) plan for a high-volume software as a service (SaaS) platform. All data for the platform is stored in an Amazon Aurora MySQL DB cluster.  
  
The DR plan must replicate data to a secondary AWS Region.  
  
Which solution will meet these requirements MOST cost-effectively?

- A. Use MySQL binary log replication to an Aurora cluster in the secondary Region. Provision one DB instance for the Aurora cluster in the secondary Region.
- B. Set up an Aurora global database for the DB cluster. When setup is complete, remove the DB instance from the secondary Region.
- C. Use AWS Database Migration Service (AWS DMS) to continuously replicate data to an Aurora cluster in the secondary Region. Remove the DB instance from the secondary Region.
- D. Set up an Aurora global database for the DB cluster. Specify a minimum of one DB instance in the secondary Region.
```

정답 : `D`

- Aurora Global Database는 리전 간 저지연, 스토리지 레벨 복제를 제공하는 Aurura의 네이티브 기능
	- DR 요구에 가장 적합하고 운영 오버헤드가 최소
- 보조 리전에는 최소 1개의 읽기 인스턴스를 두어야 정상적인 복제 상태 확인 및 DR 전환(프로모션)이 가능
- 필요시 읽기 트래픽에도 활용 가능
- 이 구조는 일반적인 권장 아키텍처이자 가장 비용 대비 효과적인 DR 구성

오답 이유

- **A. 바이너리 로그 복제 → Aurora**
    - Aurora ↔ MySQL 바이너리 로그 기반 복제는 가능하지만, **구성/운영 복잡도와 지연**이 증가하고 장애 처리 자동화 수준이 낮습니다. Aurora Global Database 대비 **운영 오버헤드와 리스크가 큼**.
    
- **B. 글로벌 DB 후 보조 인스턴스 제거**
    - Aurora Global Database의 보조 리전 **클러스터에는 최소 1개 인스턴스가 일반적으로 필요**합니다. 인스턴스를 제거하면 **모니터링/접근/프로모션 준비**에 제약이 생기며 실질적인 DR 가치를 잃습니다. 비용을 아끼려다 **DR 목적을 훼손**.
    
- **C. AWS DMS 지속 복제**
    - DMS는 마이그레이션/변경 데이터 캡처에 유용하지만, **장기 DR 동기화로 쓰면 비용과 운영 복잡도**가 커집니다. 또한 Aurora Global Database가 제공하는 **저지연 스토리지 복제/간편한 프로모션** 이점을 제공하지 못합니다.


## #339
한 회사는 임베디드 자격 증명을 포함한 커스텀 애플리케이션을 보유하고 있으며, 이 애플리케이션은 Amazon RDS MySQL DB 인스턴스에서 정보를 검색합니다. 경영진은 최소한의 프로그래밍 노력으로 애플리케이션을 더 안전하게 만들 것을 요구합니다.

이 요구 사항을 충족하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?

A. AWS Key Management Service(AWS KMS)를 사용하여 키를 생성합니다. 애플리케이션이 AWS KMS에서 데이터베이스 자격 증명을 로드하도록 구성합니다. 자동 키 교체(회전)을 활성화합니다.
B. 애플리케이션 사용자에 대한 자격 증명을 RDS for MySQL 데이터베이스에서 생성하고 해당 자격 증명을 AWS Secrets Manager에 저장합니다. 애플리케이션이 Secrets Manager에서 데이터베이스 자격 증명을 로드하도록 구성합니다. Secrets Manager에서 자격 증명을 회전하는 AWS Lambda 함수를 생성합니다.
C. 애플리케이션 사용자에 대한 자격 증명을 RDS for MySQL 데이터베이스에서 생성하고 해당 자격 증명을 AWS Secrets Manager에 저장합니다. 애플리케이션이 Secrets Manager에서 데이터베이스 자격 증명을 로드하도록 구성합니다. Secrets Manager를 사용하여 RDS for MySQL 데이터베이스에서 애플리케이션 사용자에 대한 자격 증명 회전 일정을 설정합니다.
D. 애플리케이션 사용자에 대한 자격 증명을 RDS for MySQL 데이터베이스에서 생성하고 해당 자격 증명을 AWS Systems Manager Parameter Store에 저장합니다. 애플리케이션이 Parameter Store에서 데이터베이스 자격 증명을 로드하도록 구성합니다. Parameter Store를 사용하여 RDS for MySQL 데이터베이스에서 애플리케이션 사용자에 대한 자격 증명 회전 일정을 설정합니다.

```
A company has a custom application with embedded credentials that retrieves information from an Amazon RDS MySQL DB instance. Management says the application must be made more secure with the least amount of programming effort.  
  
What should a solutions architect do to meet these requirements?

- A. Use AWS Key Management Service (AWS KMS) to create keys. Configure the application to load the database credentials from AWS KMS. Enable automatic key rotation.
- B. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Create an AWS Lambda function that rotates the credentials in Secret Manager.
- C. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Secrets Manager.
- D. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Systems Manager Parameter Store. Configure the application to load the database credentials from Parameter Store. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Parameter Store.
```

정답 : `C`

- AWS Secrets Manager는 RDS MySQL과 네이티브 통합된 자동 자격 증명 회전(관리형 람다 템플릿 포함)을 제공
- 애플리케이션은 기존 임베디드 자격 증명을 제거하고 Secrets Manager API 호출로 시크릿을 조회하도록 변경만 하면 되므로 코드 변경 최소화
- 시크릿은 KMS로 자동 암호화되며, 회전 주기 설정만으로 자격 증명 생성/갱신/검증을 자동화

오답 이유

- **A (KMS로 자격 증명 로드)**: KMS는 **키 관리/암호화 서비스**일 뿐, 자격 증명 저장·회전·버전 관리 기능을 제공하지 않습니다. 애플리케이션 측 로직과 저장소를 따로 구현해야 하므로 작업량이 큽니다.
    
- **B (Secrets Manager + 사용자 정의 Lambda 회전)**: 작동 가능하지만, **회전 Lambda를 직접 구현/유지**해야 하므로 “최소 프로그래밍 노력”에 위배됩니다. RDS 통합 **관리형 회전**을 사용하면 Lambda 템플릿과 절차가 자동화됩니다(선택지 C).
    
- **D (Parameter Store 사용)**: Parameter Store의 SecureString은 보안 저장은 가능하지만 **RDS 자격 증명 자동 회전의 네이티브 통합**이 없습니다. 회전/검증 로직을 별도로 구현해야 하므로 운영·개발 부담이 큽니다.


## #340
한 미디어 회사가 AWS에서 웹사이트를 호스팅하고 있습니다. 웹사이트 애플리케이션의 아키텍처에는 Application Load Balancer(ALB) 뒤에 있는 Amazon EC2 인스턴스 플릿과 Amazon Aurora에 호스팅된 데이터베이스가 포함됩니다. 회사의 사이버 보안 팀은 애플리케이션이 SQL 인젝션에 취약하다고 보고했습니다.

회사는 이 문제를 어떻게 해결해야 합니까?

A. ALB 앞에 AWS WAF를 사용합니다. AWS WAF에 적절한 웹 ACL을 연결합니다.
B. ALB 리스너 규칙을 생성하여 SQL 인젝션에 대해 고정 응답으로 회신합니다.
C. SQL 인젝션 시도를 자동으로 모두 차단하기 위해 AWS Shield Advanced를 구독합니다.
D. SQL 인젝션 시도를 자동으로 모두 차단하기 위해 Amazon Inspector를 설정합니다.

```
A media company hosts its website on AWS. The website application’s architecture includes a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB) and a database that is hosted on Amazon Aurora. The company’s cybersecurity team reports that the application is vulnerable to SQL injection.  
  
How should the company resolve this issue?

- A. Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.
- B. Create an ALB listener rule to reply to SQL injections with a fixed response.
- C. Subscribe to AWS Shield Advanced to block all SQL injection attempts automatically.
- D. Set up Amazon Inspector to block all SQL injection attempts automatically.
```

정답 : `A`

- AWS WAF는 웹 애플리케이션 방화벽으로, SQLi/XSS 같은 일반적인 웹 공격을 탐지 및 차단할 수 있는 관리형 규칙 집합 제공
- ALB와 통합하여 들어오는 웹 요청을 검사하고 필터링하므로 SQLi 방어에 직접적이고 적합
- 운영 오버헤드가 낮고, 관리형 규칙을 통해 보안팀의 요구사항 충족 가능

오답 이유

- **B. ALB 리스너 규칙 (고정 응답)**
    - ALB 리스너 규칙은 단순히 조건 기반 라우팅이나 응답을 반환할 수 있을 뿐, **SQL injection 패턴 인식/차단 기능이 없음**.
    - 취약점 완화 불가능.
    
- **C. AWS Shield Advanced**
    - Shield는 **DDoS 방어 서비스**이며 SQL injection 같은 **애플리케이션 계층 공격**은 차단하지 못합니다.
    
- **D. Amazon Inspector**
    - Inspector는 **취약성 평가 서비스**(소프트웨어 패치, 보안 모범 사례, 네트워크 노출 스캔 등).
    - 공격 실시간 차단 기능은 없으며 **SQL injection 방어와 무관**.