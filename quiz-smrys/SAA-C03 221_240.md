---
created: 2025-10-01 12:00:33
last_modified: 2025-10-10 17:51:21
---
## #221
한 회사가 Amazon Linux EC2 인스턴스 그룹에서 애플리케이션을 실행합니다. 컴플라이언스 이유로, 회사는 모든 애플리케이션 로그 파일을 7년 동안 보존해야 합니다. 로그 파일은 모든 파일에 동시에 접근할 수 있어야 하는 리포팅 도구에 의해 분석됩니다.

어떤 스토리지 솔루션이 이러한 요구사항을 가장 비용 효율적으로 충족합니까?

A. Amazon Elastic Block Store (Amazon EBS)
B. Amazon Elastic File System (Amazon EFS)
C. Amazon EC2 인스턴스 스토어
D. Amazon S3

```
A company runs an application on a group of Amazon Linux EC2 instances. For compliance reasons, the company must retain all application log files for 7 years. The log files will be analyzed by a reporting tool that must be able to access all the files concurrently.  
  
Which storage solution meets these requirements MOST cost-effectively?

- A. Amazon Elastic Block Store (Amazon EBS)
- B. Amazon Elastic File System (Amazon EFS)
- C. Amazon EC2 instance store
- D. Amazon S3
```

정답 : `D`

- 7년 장기 보관 + 높은 내구성/가용성 + 대규모 동시 접근 요구는 S3가 최적
- S3는 객체 단위로 무제한 병렬 읽기가 가능해 리포팅 도구의 동시 접근 요구 충족
- 저비용(특히 S3 Standard -> 수명주기 정책으로 IA/Glacier 계열 전환)으로 장기 보관 비용 크게 절감 가능

오답 이유

- **A. Amazon EBS**
    - 개별 EC2 인스턴스에 종속되며, 여러 인스턴스가 **동시에 동일 볼륨을 마운트하여 읽기**하는 데 제약이 큼(멀티어태치도 한계). 장기 보관 비용도 S3 대비 불리.
    
- **B. Amazon EFS**
    - 다중 EC2 동시 접근에는 적합하지만, **GB당 비용이 S3보다 높음**. 7년 장기 보관 요건에서는 비용 비효율적.
    
- **C. EC2 인스턴스 스토어**
    - **비영구(ephemeral)** 스토리지로 인스턴스 중지/종료 시 데이터 손실. 7년 보존 불가.

## #222
한 회사가 외부 벤더를 고용하여 회사의 AWS 계정에서 작업을 수행하도록 했습니다. 벤더는 벤더가 소유한 AWS 계정에서 호스팅되는 자동화 도구를 사용합니다. 벤더는 회사의 AWS 계정에 대한 IAM 접근 권한을 가지고 있지 않습니다.

솔루션스 아키텍트는 벤더에게 어떻게 이 접근 권한을 부여해야 합니까?

A. 회사 계정에 IAM 역할을 생성하여 벤더의 IAM 역할에 위임(access delegate)합니다. 벤더가 필요한 권한에 대해 적절한 IAM 정책을 역할에 연결합니다.  
B. 회사 계정에 IAM 사용자를 생성하고, 비밀번호 복잡성 요구사항을 충족하는 비밀번호를 설정합니다. 벤더가 필요한 권한에 대해 적절한 IAM 정책을 사용자에 연결합니다.  
C. 회사 계정에 IAM 그룹을 생성합니다. 벤더 계정의 도구의 IAM 사용자를 그룹에 추가합니다. 벤더가 필요한 권한에 대해 적절한 IAM 정책을 그룹에 연결합니다.  
D. IAM 콘솔에서 공급자 유형으로 "AWS 계정"을 선택하여 새 ID 공급자를 생성합니다. 벤더의 AWS 계정 ID와 사용자 이름을 제공합니다. 벤더가 필요한 권한에 대해 적절한 IAM 정책을 새 공급자에 연결합니다.  

```
A company has hired an external vendor to perform work in the company’s AWS account. The vendor uses an automated tool that is hosted in an AWS account that the vendor owns. The vendor does not have IAM access to the company’s AWS account.  
  
How should a solutions architect grant this access to the vendor?

- A. Create an IAM role in the company’s account to delegate access to the vendor’s IAM role. Attach the appropriate IAM policies to the role for the permissions that the vendor requires.
- B. Create an IAM user in the company’s account with a password that meets the password complexity requirements. Attach the appropriate IAM policies to the user for the permissions that the vendor requires.
- C. Create an IAM group in the company’s account. Add the tool’s IAM user from the vendor account to the group. Attach the appropriate IAM policies to the group for the permissions that the vendor requires.
- D. Create a new identity provider by choosing “AWS account” as the provider type in the IAM console. Supply the vendor’s AWS account ID and user name. Attach the appropriate IAM policies to the new provider for the permissions that the vendor requires.
```

정답 : `A`

- 크로스 계정 액세스의 올바른 방식은 신뢰 정책을 사용해 다른 계정(벤더 계정)에서 역할을 AssumeRole 할 수 있도록 허용하는 것
- 회사 계정에서 역할을 생성하고, 벤더 계정의 IAM 주체(역할/사용자)를 신뢰 주체로 지정하면 벤더의 자동화 도구가 해당 역할을 Assume 하여 필요한 권한을 위임받을 수 있음
- 이 방식은 비밀번호/액세스 키 공유 불필요, 세분화된 권한 위임 가능, 중앙화된 관리가 가능하므로 AWS 모범 사례

오답 이유

- **B. IAM 사용자 생성 후 비밀번호 공유**
    - 벤더에 직접 IAM 사용자 자격 증명을 제공하는 것은 **보안 위험**이며 AWS 모범 사례 위배. 액세스 키/비밀번호 관리 부담도 증가.
    
- **C. IAM 그룹에 외부 사용자 추가**
    - **외부 AWS 계정의 사용자/역할을 직접 그룹에 추가할 수 없음.** IAM 그룹은 동일한 AWS 계정 내 사용자만 가능.
    
- **D. ID 공급자로 “AWS 계정” 사용**
    - ID 공급자 방식은 **SAML/OIDC** 연동용이며, 단순한 **AWS 계정 간 권한 위임**에는 사용하지 않음. 불필요하게 복잡.

## #223
한 회사가 Java Spring Boot 애플리케이션을 Amazon Elastic Kubernetes Service(Amazon EKS)의 프라이빗 서브넷에서 실행되는 파드(pod)로 배포했습니다. 애플리케이션은 Amazon DynamoDB 테이블에 데이터를 써야 합니다. 솔루션스 아키텍트는 인터넷에 트래픽을 노출하지 않고 애플리케이션이 DynamoDB 테이블과 상호작용할 수 있도록 보장해야 합니다.

이 목표를 달성하기 위해 솔루션스 아키텍트가 수행해야 할 단계의 조합은 무엇입니까? (두 개 선택)

A. 충분한 권한을 가진 IAM 역할을 EKS 파드에 연결합니다.
B. 충분한 권한을 가진 IAM 사용자를 EKS 파드에 연결합니다.
C. 프라이빗 서브넷의 네트워크 ACL을 통해 DynamoDB 테이블로의 아웃바운드 연결을 허용합니다.
D. DynamoDB에 대한 VPC 엔드포인트를 생성합니다.
E. 액세스 키를 Java Spring Boot 코드에 임베드합니다.

```
A company has deployed a Java Spring Boot application as a pod that runs on Amazon Elastic Kubernetes Service (Amazon EKS) in private subnets. The application needs to write data to an Amazon DynamoDB table. A solutions architect must ensure that the application can interact with the DynamoDB table without exposing traffic to the internet.  
  
Which combination of steps should the solutions architect take to accomplish this goal? (Choose two.)

- A. Attach an IAM role that has sufficient privileges to the EKS pod.
- B. Attach an IAM user that has sufficient privileges to the EKS pod.
- C. Allow outbound connectivity to the DynamoDB table through the private subnets’ network ACLs.
- D. Create a VPC endpoint for DynamoDB.
- E. Embed the access keys in the Java Spring Boot code.
```

정답 : `A, D`

- IAM 역할을 파드에 연결: EKS에서는 IRSA(IAM Roles for Service Accounts)를 사용해 쿠버네티스 서비스 어카운트에 IAM 역할을 바운딩함으로써 파드에 임시 자격 증명을 안전하게 주입 가능
	- 이 역할에 DynamoDB에 대한 최소 권한(예: dynamodb:PutItem, UpdateItem 등)을 부여하면 애플리케이션이 안전하게 테이블 접근 가능
- DynamoDB용 VPC 엔드포인트 생성: DynamoDB는 게이트웨이 VPC 엔드포인트를 지원
	- 프라이빗 서브넷의 라우트 테이블에 엔드포인트 대상으로 경로를 추가하면 트래픽이 NAT 게이트웨이/인터넷 게이트웨이 없이 VPC 내부 경로로 DynamoDB에 도달하므로 인터넷 노출 없음

오답 이유

- **B. IAM 사용자 연결**: 파드에 IAM 사용자를 “연결”하는 개념은 없고, 장기 키(Access key/Secret) 사용은 보안상 비권장입니다. EKS에는 **IRSA**가 모범 사례입니다.
    
- **C. NACL 아웃바운드 허용**: NACL만 열어도 **라우팅이 인터넷(예: NAT GW)으로 나가면** 여전히 공용 엔드포인트로 통신합니다. 요건은 “인터넷 노출 없이”이므로 **게이트웨이 VPC 엔드포인트**가 필요합니다. 또한 NACL 기본 허용 상태라면 별도 변경이 필수도 아닙니다.
    
- **E. 코드에 키 임베드**: 하드코딩된 액세스 키는 유출 위험이 크고 롤오버/감사도 어렵습니다. AWS 베스트 프랙티스에 위배되며 요건(보안/운영 효율)에도 부적합합니다.


## #224
한 회사가 최근 웹 애플리케이션을 단일 AWS 리전의 Amazon EC2 인스턴스에 리호스팅하여 AWS로 마이그레이션했습니다. 회사는 애플리케이션 아키텍처를 고가용성(HA)과 장애 허용(FT)이 되도록 재설계하고자 합니다. 트래픽은 실행 중인 모든 EC2 인스턴스로 무작위로 도달해야 합니다.

이 요구사항을 충족하기 위해 회사는 어떤 단계의 조합을 수행해야 합니까? (두 개 선택)

A. Amazon Route 53 장애 조치(failover) 라우팅 정책을 생성합니다.
B. Amazon Route 53 가중치(weighted) 라우팅 정책을 생성합니다.
C. Amazon Route 53 다중값 응답(multivalue answer) 라우팅 정책을 생성합니다.
D. EC2 인스턴스 3대를 시작합니다: 하나의 가용 영역에 2대, 다른 가용 영역에 1대.
E. EC2 인스턴스 4대를 시작합니다: 하나의 가용 영역에 2대, 다른 가용 영역에 2대.

```
A company recently migrated its web application to AWS by rehosting the application on Amazon EC2 instances in a single AWS Region. The company wants to redesign its application architecture to be highly available and fault tolerant. Traffic must reach all running EC2 instances randomly.  
  
Which combination of steps should the company take to meet these requirements? (Choose two.)

- A. Create an Amazon Route 53 failover routing policy.
- B. Create an Amazon Route 53 weighted routing policy.
- C. Create an Amazon Route 53 multivalue answer routing policy.
- D. Launch three EC2 instances: two instances in one Availability Zone and one instance in another Availability Zone.
- E. Launch four EC2 instances: two instances in one Availability Zone and two instances in another Availability Zone.
```

정답 : `C, E`

- Route 53의 다중값 응답 라우팅은 레코드마다 헬스 체크를 붙여 건강한 여러 IP(인스턴스)를 응답으로 반환하고, DNS 리졸버가 무작위로 선택하도록 해 "실행 중인 모든 인스턴스로 무작위 분산" 요구 충족
- 두 개의 가용 영역에 대칭(2+2)로 배치하면, 단일 AZ 장애 시에도 각 AZ에 적어도 한 대 이상이 남아 고가용성과 장애 허용을 극대화

오답 이유

- **A. Failover 라우팅**: 1차(프라이머리)와 2차(세컨더리) 엔드포인트 간 **전환**에 초점이 있어, 모든 실행 중 인스턴스로 **무작위 분산**하지 않습니다.
    
- **B. Weighted 라우팅**: 가중치에 따라 트래픽을 분배하는 용도로 **랜덤 균등 분산** 요건과 직접적이지 않습니다(헬스 체크와 조합 가능하나, 요구사항의 “무작위로 모든 실행 인스턴스”에는 multivalue가 더 적합).
    
- **D. 2+1 배치**: 다중 AZ이긴 하나 한 AZ 장애 시 **1대만 남는** 불균형 구조로, **E(2+2)** 대비 내구성과 처리 여유가 떨어집니다.

## #225
한 미디어 회사는 온프레미스에서 사용자 활동 데이터를 수집·분석하고 있습니다. 회사는 이 기능을 AWS로 마이그레이션하고자 합니다. 사용자 활동 데이터 저장소는 계속 증가하여 페타바이트 규모가 될 것입니다. 회사는 기존 데이터와 신규 데이터를 SQL로 온디맨드 분석할 수 있도록, 고가용성의 데이터 수집(ingestion) 솔루션을 구축해야 합니다.

다음 중 최소한의 운영 오버헤드로 이러한 요구사항을 충족하는 솔루션은 무엇입니까?

A. 활동 데이터를 Amazon Kinesis 데이터 스트림으로 보냅니다. 스트림을 Amazon S3 버킷으로 데이터를 전달하도록 구성합니다.
B. 활동 데이터를 Amazon Kinesis Data Firehose 전송 스트림으로 보냅니다. 스트림을 Amazon Redshift 클러스터로 데이터를 전달하도록 구성합니다.
C. 활동 데이터를 Amazon S3 버킷에 저장합니다. S3 버킷에 데이터가 도착할 때 AWS Lambda 함수를 실행하도록 Amazon S3를 구성합니다.
D. 여러 가용 영역에 분산된 Amazon EC2 인스턴스에서 수집 서비스를 생성합니다. 이 서비스를 Amazon RDS Multi-AZ 데이터베이스로 데이터를 전달하도록 구성합니다.

```
A media company collects and analyzes user activity data on premises. The company wants to migrate this capability to AWS. The user activity data store will continue to grow and will be petabytes in size. The company needs to build a highly available data ingestion solution that facilitates on-demand analytics of existing data and new data with SQL.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Send activity data to an Amazon Kinesis data stream. Configure the stream to deliver the data to an Amazon S3 bucket.
- B. Send activity data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to deliver the data to an Amazon Redshift cluster.
- C. Place activity data in an Amazon S3 bucket. Configure Amazon S3 to run an AWS Lambda function on the data as the data arrives in the S3 bucket.
- D. Create an ingestion service on Amazon EC2 instances that are spread across multiple Availability Zones. Configure the service to forward data to an Amazon RDS Multi-AZ database.
```

정답 : `A`

- 페타바이트 규모의 원시 이벤트/활동 로그는 Amazon S3에 적재하는 데이터 레이크가 가장 비용 효율적이며 내구성인 높음
- Kiensis(Streams/Firehose)를 프런트 수집 계층으로 사용하면 고가용성/확장성으로 대량 스트림 데이터를 버퍼링, 전송할 수 있고 운영 오버헤드가 낮은 서버리스 구성이 가능
- S3에 적재된 데이터는 Amazon Athena(서버리스 Presto/Trino 기반)로 온디맨드 SQL 분석(기존+신규 데이터)을 즉시 수행 가능해 클러스터 관리 부담이 없음

오답 이유

- **B. Firehose → Redshift**
    - Redshift는 강력한 DW지만 **클러스터(또는 서버리스 설정) 관리/튜닝**이 필요하고 저장 비용도 큽니다. 페타바이트급 장기 원시 로그와 **온디맨드 분석**에는 **S3+Athena**가 더 **운영 오버헤드가 낮고 비용 효율적**입니다.
    
- **C. S3 + S3 이벤트 Lambda**
    - 단순 도착 트리거는 가능하지만 **수집 파이프라인(버퍼링/재시도/스루풋 제어)** 로서의 기능이 부족합니다. 또한 SQL 분석 경로(Athena 등)가 제시되지 않아 요건 충족이 불명확합니다.
    
- **D. EC2 수집 서비스 + RDS**
    - 자체 수집 서비스를 EC2로 운영하면 **운영 부담**이 크고, RDS는 **페타바이트 규모**의 원시 로그 저장 및 동시 분석에 부적합합니다.


## #226
한 회사는 Amazon EC2 인스턴스에서 실행되는 RESTful 웹 서비스 애플리케이션을 사용하여 수천 개의 원격 장치로부터 데이터를 수집합니다. EC2 인스턴스는 원시 데이터를 수신하고, 원시 데이터를 변환하며, 모든 데이터를 Amazon S3 버킷에 저장합니다. 원격 장치 수는 곧 수백만 대로 증가할 것입니다. 회사는 운영 오버헤드를 최소화하면서 고도로 확장 가능한 솔루션이 필요합니다.

이 요구사항을 충족하기 위해 솔루션스 아키텍트가 수행해야 할 단계의 조합은 무엇입니까? (두 개 선택)

A. AWS Glue를 사용하여 Amazon S3의 원시 데이터를 처리합니다.
B. Amazon Route 53을 사용하여 트래픽을 서로 다른 EC2 인스턴스로 라우팅합니다.
C. 들어오는 데이터 증가에 대응하기 위해 더 많은 EC2 인스턴스를 추가합니다.
D. 원시 데이터를 Amazon Simple Queue Service(Amazon SQS)로 보냅니다. EC2 인스턴스를 사용하여 데이터를 처리합니다.
E. Amazon API Gateway를 사용하여 원시 데이터를 Amazon Kinesis 데이터 스트림으로 보냅니다. Amazon Kinesis Data Firehose가 해당 데이터 스트림을 소스로 사용하여 데이터를 Amazon S3로 전달하도록 구성합니다.

```
A company collects data from thousands of remote devices by using a RESTful web services application that runs on an Amazon EC2 instance. The EC2 instance receives the raw data, transforms the raw data, and stores all the data in an Amazon S3 bucket. The number of remote devices will increase into the millions soon. The company needs a highly scalable solution that minimizes operational overhead.  
  
Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Use AWS Glue to process the raw data in Amazon S3.
- B. Use Amazon Route 53 to route traffic to different EC2 instances.
- C. Add more EC2 instances to accommodate the increasing amount of incoming data.
- D. Send the raw data to Amazon Simple Queue Service (Amazon SQS). Use EC2 instances to process the data.
- E. Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to use the data stream as a source to deliver the data to Amazon S3.
```

정답 : `A, E`

- API Gateway -> Kinesis Data Streams -> Kinesis Data Firehose -> S3
	- 수백만 디바이스로 확장되는 수집 계층을 완전관리형·서버리스로 전환
	- API Gateway가 대규모 REST 수신을 처리하고, Kinesis Data Streams가 탄력적으로 버퍼링/스루풋을 흡수, Firehose가 운영 없이 자동 배치/재시도/압축/파티셔닝으로 S3 데이터 레이크에 적재
- AWS Glue on S3 : 적재된 원시 데이터를 서버리스 ETL로 변환/정제해 분석 가능한 포맷(Parquet)로 저장. 크롤러+작업(ETL)로 스키마 추론과 배치 처리를 자동화해 운영 오버헤드 최소화
오답 이유

- **B. Route 53 로드 분산**: DNS 레벨 라우팅은 EC2 인스턴스 앞에서 일부 트래픽 분산을 돕지만, **수집/처리 파이프라인의 근본 확장성/내고장성**을 해결하지 못합니다. 운영 부담도 해소되지 않습니다.
    
- **C. EC2 수평 확장**: 인스턴스 증설은 **용량 계획/오토스케일/패치/모니터링** 등 운영 복잡도를 키우며, 수백만 디바이스 규모에는 비효율적입니다.
    
- **D. SQS + EC2 처리**: 큐로 버퍼링은 좋지만, 여전히 **EC2 기반 워커 운영**(스케일, AMI 관리, 보안 패치 등)이 필요해 **운영 오버헤드**가 큽니다. 서버리스 대안(E, A)이 더 적합합니다.


## #227
한 회사는 AWS CloudTrail 로그를 3년 동안 보존해야 합니다. 회사는 상위(Parent) 계정에서 AWS Organizations를 사용하여 일련의 AWS 계정 전체에 CloudTrail을 강제 적용하고 있습니다. CloudTrail 대상 S3 버킷은 S3 버전 관리(Versioning)가 활성화되어 구성되어 있습니다. S3 수명 주기(Lifecycle) 정책은 3년 후 현재(Current) 객체를 삭제하도록 설정되어 있습니다.

S3 버킷을 4년째 사용한 후, S3 버킷 지표는 객체 수가 계속 증가하고 있음을 보여줍니다. 그러나 S3 버킷으로 전달되는 새로운 CloudTrail 로그의 수는 일정하게 유지되고 있습니다.

가장 비용 효율적인 방식으로 3년이 지난 객체를 삭제하려면 어떤 솔루션을 사용해야 합니까?

A. 조직의 중앙 집중식 CloudTrail 트레일이 3년 후 객체가 만료되도록 구성합니다.
B. S3 수명 주기 정책이 현재 버전뿐만 아니라 이전 버전(Previous versions)도 삭제하도록 구성합니다.
C. 3년이 지난 객체를 Amazon S3에서 열거하고 삭제하는 AWS Lambda 함수를 생성합니다.
D. S3 버킷에 전달되는 모든 객체의 소유자가 상위(Parent) 계정이 되도록 구성합니다.

```
A company needs to retain its AWS CloudTrail logs for 3 years. The company is enforcing CloudTrail across a set of AWS accounts by using AWS Organizations from the parent account. The CloudTrail target S3 bucket is configured with S3 Versioning enabled. An S3 Lifecycle policy is in place to delete current objects after 3 years.  
  
After the fourth year of use of the S3 bucket, the S3 bucket metrics show that the number of objects has continued to rise. However, the number of new CloudTrail logs that are delivered to the S3 bucket has remained consistent.  
  
Which solution will delete objects that are older than 3 years in the MOST cost-effective manner?

- A. Configure the organization’s centralized CloudTrail trail to expire objects after 3 years.
- B. Configure the S3 Lifecycle policy to delete previous versions as well as current versions.
- C. Create an AWS Lambda function to enumerate and delete objects from Amazon S3 that are older than 3 years.
- D. Configure the parent account as the owner of all objects that are delivered to the S3 bucket.
```

정답 : `B`

- 버킷에 S3 버전 관리가 활성화되어 있고, 라이프사이클 정책은 현재 버전만 3년 후 삭제 -> 이전 버전은 계속 남아 객체 수가 증가
- 따라서 라이프사이클 규칙에 이전 버전 삭제를 추가해 3년 경과 이전 버전도 자동 삭제
- 완전관리형·무서버·추가 비용 거의 없음으로 가장 비용 효율적이고 운영 오버헤드가 낮음

오답 이유

- **A. CloudTrail 트레일에서 만료 설정**
    - CloudTrail은 S3에 전달된 객체의 **보존/삭제를 직접 관리하지 않습니다.** 데이터 수명 관리는 **S3 Lifecycle**로 수행해야 합니다.
    
- **C. Lambda로 나이 든 객체 열거/삭제**
    - 가능은 하지만 **주기 실행·권한·리트라이·비용** 등 운영 오버헤드가 증가합니다. S3 Lifecycle이 제공하는 **네이티브 기능을 중복 구현**하는 셈이므로 비효율적입니다.
    
- **D. 상위 계정을 객체 소유자로 설정**
    - 객체 소유권은 **삭제 누락의 근본 원인(버전 관리로 인한 이전 버전 잔존)** 을 해결하지 못합니다. 여전히 이전 버전은 남습니다.


## #228
한 회사에는 모니터링 장치 플릿으로부터 실시간 데이터를 수신하는 API가 있습니다. 이 API는 이후 분석을 위해 이 데이터를 Amazon RDS DB 인스턴스에 저장합니다. 모니터링 장치가 API로 보내는 데이터의 양은 변동합니다. 트래픽이 많은 기간 동안 API는 종종 타임아웃 오류를 반환합니다.

로그를 점검한 결과, 회사는 데이터베이스가 API에서 오는 쓰기 트래픽의 볼륨을 처리할 수 없다는 것을 확인했습니다. 솔루션스 아키텍트는 데이터베이스로의 연결 수를 최소화하고, 트래픽이 많은 기간 동안 데이터가 손실되지 않도록 해야 합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. 더 많은 메모리를 사용할 수 있는 인스턴스 유형으로 DB 인스턴스의 크기를 늘립니다.
B. DB 인스턴스를 Multi-AZ DB 인스턴스로 수정합니다. 애플리케이션이 모든 활성 RDS DB 인스턴스에 쓰도록 구성합니다.
C. API를 수정하여 들어오는 데이터를 Amazon Simple Queue Service(Amazon SQS) 큐에 기록합니다. Amazon SQS가 호출하는 AWS Lambda 함수를 사용하여 큐에서 데이터베이스로 데이터를 씁니다.
D. API를 수정하여 들어오는 데이터를 Amazon Simple Notification Service(Amazon SNS) 주제에 기록합니다. Amazon SNS가 호출하는 AWS Lambda 함수를 사용하여 주제에서 데이터베이스로 데이터를 씁니다.

```
A company has an API that receives real-time data from a fleet of monitoring devices. The API stores this data in an Amazon RDS DB instance for later analysis. The amount of data that the monitoring devices send to the API fluctuates. During periods of heavy traffic, the API often returns timeout errors.  
  
After an inspection of the logs, the company determines that the database is not capable of processing the volume of write traffic that comes from the API. A solutions architect must minimize the number of connections to the database and must ensure that data is not lost during periods of heavy traffic.  
  
Which solution will meet these requirements?

- A. Increase the size of the DB instance to an instance type that has more available memory.
- B. Modify the DB instance to be a Multi-AZ DB instance. Configure the application to write to all active RDS DB instances.
- C. Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function that Amazon SQS invokes to write data from the queue to the database.
- D. Modify the API to write incoming data to an Amazon Simple Notification Service (Amazon SNS) topic. Use an AWS Lambda function that Amazon SNS invokes to write data from the topic to the database.
```

정답 : `C`

- SQS를 버퍼로 사용하면 급증하는 쓰기 부하를 흡수하여 데이터 손실 없이 내구성 있게 적재 가능
- SQS 이벤트 소스의 람다는 배치 처리와 동시성 제어가 가능해, 데이터베이스로 가는 동시 연결 수를 크게 줄이고 안정적으로 쓰기 작업 수행
- 이 구조는 API와 DB를 비동기로 디커플링해 타임아웃을 완화하고 스파이크에 탄력적 대응

오답 이유

- **A. 인스턴스 사이즈 업**: 순간 스파이크나 커넥션 폭증 문제의 근본 원인(버퍼 부재)을 해결하지 못합니다. 연결 최소화·데이터 손실 방지 요구도 직접 충족하지 못합니다.
    
- **B. Multi-AZ로 변경 후 다중 쓰기**: 전통적 RDS Multi-AZ 배포는 **단일 writer/동기 스탠바이**이며 “모든 활성 인스턴스에 쓰기”는 개념적으로 잘못입니다. 가용성은 향상되지만 **쓰기 처리량/연결 수 문제**를 해결하지 못합니다.
    
- **D. SNS 사용**: SNS는 퍼브/섭 알림 서비스로 **큐잉·백로그 관리에 최적이 아님**(일시적 실패 재시도는 있으나 큐의 내구성/컨슈머 풀링이 아님). 스파이크 흡수·배치/동시성 제어 측면에서 **SQS+Lambda**가 더 적합합니다.

## #229
한 회사가 MySQL 데이터베이스를 실행하는 Amazon EC2 인스턴스를 자체적으로 관리하고 있습니다. 회사는 수요가 증가하거나 감소함에 따라 복제와 스케일링을 수동으로 관리하고 있습니다. 회사는 필요에 따라 데이터베이스 계층에 컴퓨팅 용량을 추가하거나 제거하는 과정을 단순화하는 새로운 솔루션이 필요합니다. 또한 이 솔루션은 운영의 최소한의 노력으로 향상된 성능, 스케일링 및 내구성을 제공해야 합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. 데이터베이스를 Amazon Aurora Serverless for Aurora MySQL로 마이그레이션합니다.
B. 데이터베이스를 Amazon Aurora Serverless for Aurora PostgreSQL로 마이그레이션합니다.
C. 데이터베이스를 하나의 더 큰 MySQL 데이터베이스로 결합합니다. 더 큰 EC2 인스턴스에서 더 큰 데이터베이스를 실행합니다.
D. 데이터베이스 계층을 위한 EC2 Auto Scaling 그룹을 생성합니다. 기존 데이터베이스를 새 환경으로 마이그레이션합니다.

```
A company manages its own Amazon EC2 instances that run MySQL databases. The company is manually managing replication and scaling as demand increases or decreases. The company needs a new solution that simplifies the process of adding or removing compute capacity to or from its database tier as needed. The solution also must offer improved performance, scaling, and durability with minimal effort from operations.  
  
Which solution meets these requirements?

- A. Migrate the databases to Amazon Aurora Serverless for Aurora MySQL.
- B. Migrate the databases to Amazon Aurora Serverless for Aurora PostgreSQL.
- C. Combine the databases into one larger MySQL database. Run the larger database on larger EC2 instances.
- D. Create an EC2 Auto Scaling group for the database tier. Migrate the existing databases to the new environment.
```

정답 : `A`

- 기존 MySQL 호환성을 유지하면서 자동/탄력 스케일링으로 용량 증감이 간편
- 복제, 고가용성, 내구성(다중 AZ에 6중 복제 스토리지)은 관리형으로 제공되어 운영 부담이 크게 감소
- 연결 처리와 읽기 확장(리더 엔드포인트) 등 성능･확장성이 표준 MySQL on EC2 대비 우수하며 수요 변화에 맞춘 세밀한 용량 조정이 가능

오답 이유

- **B. Aurora Serverless for PostgreSQL**: PostgreSQL 엔진으로의 **이기종 마이그레이션**이며, 문제에서 MySQL 워크로드 유지가 전제되어 적합하지 않습니다.
    
- **C. 더 큰 EC2로 수직 확장**: 수동 운영(복제/백업/장애조치) 부담이 계속되고, 단일 노드 의존도 증가로 **확장성·내구성** 측면에서 한계가 있습니다.
    
- **D. EC2 Auto Scaling으로 DB 계층 구성**: 상태ful DB는 단순 ASG로 해결되지 않습니다(데이터 복제/일관성/페일오버/백업 등 직접 관리 필요). 운영 복잡도와 리스크가 큽니다.


## #230
한 회사는 사용 중인 두 개의 NAT 인스턴스가 회사 애플리케이션에 필요한 트래픽을 더 이상 지원할 수 없을 것을 우려하고 있습니다. 솔루션스 아키텍트는 고가용성, 장애 허용, 자동 확장이 가능한 솔루션을 구현하고자 합니다.

솔루션스 아키텍트는 무엇을 권장해야 합니까?

A. 두 개의 NAT 인스턴스를 제거하고 동일한 가용 영역에 두 개의 NAT 게이트웨이로 교체합니다.
B. 서로 다른 가용 영역의 NAT 인스턴스에 대해 네트워크 로드 밸런서를 사용하여 Auto Scaling 그룹을 사용합니다.
C. 두 개의 NAT 인스턴스를 제거하고 서로 다른 가용 영역에 두 개의 NAT 게이트웨이로 교체합니다.
D. 두 개의 NAT 인스턴스를 서로 다른 가용 영역의 스팟 인스턴스로 교체하고 네트워크 로드 밸런서를 배포합니다.

```
A company is concerned that two NAT instances in use will no longer be able to support the traffic needed for the company’s application. A solutions architect wants to implement a solution that is highly available, fault tolerant, and automatically scalable.  
  
What should the solutions architect recommend?

- A. Remove the two NAT instances and replace them with two NAT gateways in the same Availability Zone.
- B. Use Auto Scaling groups with Network Load Balancers for the NAT instances in different Availability Zones.
- C. Remove the two NAT instances and replace them with two NAT gateways in different Availability Zones.
- D. Replace the two NAT instances with Spot Instances in different Availability Zones and deploy a Network Load Balancer.
```

정답 : `C`

- NAT 게이트웨이는 AWS 관리형으로 자동 확장, 고가용성, 내결함성을 제공하여 운영 부담이 거의 없음
- 각 가용 영역(AZ)에 NAT 게이트웨이를 배치하고, 해당 AZ의 프라이빗 서브넷 라우트를 동일 AZ의 NAT 게이트웨이로 지정하면 AZ 장애 시 영향 구획화와 신속한 복구가 가능
- NAT 인스턴스 대비 패치/용량 계획/장애 조치 등을 직접 관리할 필요가 없어 요구사항(HA, FT, Auto scaling, 운영 최소화)에 가장 부합

오답 이유

- **A. 동일 AZ에 2개 NAT 게이트웨이**
    - 동일 AZ에만 배치하면 **AZ 장애** 시 단일 장애 지점(SPOF)이 됩니다. 고가용성 요구를 충족하지 못합니다.
    
- **B. NAT 인스턴스 + NLB + ASG**
    - 구현 가능하나 NAT 인스턴스는 **수동 확장/패치/헬스체크/장애조치** 등 **운영 오버헤드**가 큽니다. “자동 확장되는 관리형” 요건과 거리가 멉니다.
    
- **D. 스팟 인스턴스 + NLB**
    - 스팟은 **중단 가능성**이 있어 NAT 용도로 부적합합니다. 또한 관리/확장 복잡성과 가용성 리스크가 큽니다.


## #231
애플리케이션이 VPC A의 Amazon EC2 인스턴스(Elastic IP 주소 보유)에서 실행되고 있습니다. 애플리케이션은 VPC B의 데이터베이스에 접근해야 합니다. 두 VPC는 동일한 AWS 계정에 있습니다.

가장 보안성이 높은 방식으로 필요한 접근을 제공하는 솔루션은 무엇입니까?

A. VPC A의 애플리케이션 서버의 공용 IP 주소에서 오는 모든 트래픽을 허용하는 DB 인스턴스 보안 그룹을 생성합니다.
B. VPC A와 VPC B 사이에 VPC 피어링 연결을 구성합니다.
C. DB 인스턴스를 퍼블릭 액세스 가능하도록 설정합니다. DB 인스턴스에 공용 IP 주소를 할당합니다.
D. VPC B에 Elastic IP 주소가 있는 EC2 인스턴스를 시작합니다. 모든 요청을 새로운 EC2 인스턴스를 통해 프록시합니다.

```
An application runs on an Amazon EC2 instance that has an Elastic IP address in VPC A. The application requires access to a database in VPC B. Both VPCs are in the same AWS account.  
  
Which solution will provide the required access MOST securely?

- A. Create a DB instance security group that allows all traffic from the public IP address of the application server in VPC A.
- B. Configure a VPC peering connection between VPC A and VPC B.
- C. Make the DB instance publicly accessible. Assign a public IP address to the DB instance.
- D. Launch an EC2 instance with an Elastic IP address into VPC B. Proxy all requests through the new EC2 instance.
```

정답 : `B`

- VPC피어링은 두 VPC 간에 사설 IP 기반의 직접, 암호화되지 않은(필요 시 별도 옵션) 프라이빗 라우팅을 제공
- 같은 계정･리전이라면 설정이 간단하고, 보안 그룹/라우팅 테이블만으로 인테닛을 통하지 않고 DB 접근 가능
- 동일 계정･리전 조건에서 피어링된 VPC 간 보안 그룹 참조를 사용할 수 있어 최소 권한의 네트워크 접근 제어 가능

오답 이유

- **A. 공용 IP 기준 허용**
    - 트래픽이 **인터넷(공인 경로)** 을 통과하므로 노출 면적이 커지고, NAT/경로 변화에 취약합니다. 사설 통신 요건에 부적합.
    
- **C. DB를 퍼블릭으로 노출**
    - 가장 위험한 선택입니다. DB에 공용 IP를 부여하면 공격 표면이 급격히 증가합니다.
    
- **D. 프록시 EC2 + EIP**
    - 불필요한 컴퓨팅/운영 오버헤드를 추가하고, 여전히 공용 경로 사용 가능성이 큽니다. 피어링의 단순·보안성 대비 열등합니다.


## #232
한 회사가 고객을 위한 데모 환경을 Amazon EC2 인스턴스에서 운영합니다. 각 환경은 자체 VPC에서 격리되어 있습니다. 회사의 운영 팀은 어느 환경이든 RDP 또는 SSH 액세스가 설정되었을 때 알림을 받아야 합니다.

A. Amazon CloudWatch Application Insights를 구성하여 RDP 또는 SSH 액세스가 감지될 때 AWS Systems Manager OpsItems를 생성합니다.
B. AmazonSSMManagedInstanceCore 정책이 연결된 IAM 역할을 가진 IAM 인스턴스 프로파일로 EC2 인스턴스를 구성합니다.
C. VPC 플로우 로그를 Amazon CloudWatch Logs로 게시합니다. 필요한 메트릭 필터를 생성합니다. 알람이 ALARM 상태일 때 알림 동작을 가지는 Amazon CloudWatch 메트릭 알람을 생성합니다.
D. Amazon EventBridge 규칙을 구성하여 EC2 Instance State-change Notification 유형의 이벤트를 수신합니다. Amazon Simple Notification Service(Amazon SNS) 주제를 대상으로 구성합니다. 운영 팀을 해당 주제에 구독시킵니다.

```
A company runs demonstration environments for its customers on Amazon EC2 instances. Each environment is isolated in its own VPC. The company’s operations team needs to be notified when RDP or SSH access to an environment has been established.  

- A. Configure Amazon CloudWatch Application Insights to create AWS Systems Manager OpsItems when RDP or SSH access is detected.
- B. Configure the EC2 instances with an IAM instance profile that has an IAM role with the AmazonSSMManagedInstanceCore policy attached.
- C. Publish VPC flow logs to Amazon CloudWatch Logs. Create required metric filters. Create an Amazon CloudWatch metric alarm with a notification action for when the alarm is in the ALARM state.
- D. Configure an Amazon EventBridge rule to listen for events of type EC2 Instance State-change Notification. Configure an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the operations team to the topic.
```

정답 : `C`

- VPC Flow Logs는 ENI 단에서의 네트워크 연결(소스/목적지 IP･포트, 프로토콜, ACCEPT/REJECT)을 기록
- 이를 CloudWatch Logs로 전송하고, TCP 22(SSH) 및 TCP 3389(RDP)에 대한 ACCEPT 레코드를 매칭하는 케트릭 필터를 만들면 실제 접속(수락된 세션)을 탐지 가능
- 해당 필터로 생성된 커스텀 메트릭에 CloudWatch 경보를 걸고 SNS 알림을 연결하면, 접속이 성립될 때 운영팀에 자동 통지가 이루어짐

오답 이유

- **A. CloudWatch Application Insights**
    - APM/애플리케이션 모니터링 및 운영 이슈(OpsItems) 생성에 초점이며, **RDP/SSH 네트워크 접속 성립**을 직접 탐지하는 용도로 적합하지 않습니다.
    
- **B. SSM 인스턴스 프로파일 부여**
    - SSM 관리는 가능해지지만, **RDP/SSH 접속 알림**이 자동으로 제공되는 것은 아닙니다. 요구사항 충족과 직접 관련이 없습니다.
    
- **D. EventBridge 인스턴스 상태 변경 이벤트**
    - 이는 **인스턴스 시작/중지/종료 등 상태 변화**에 대한 알림입니다. **RDP/SSH 접속 여부**와는 무관합니다.


## #233
한 솔루션스 아키텍트가 새로운 AWS 계정을 생성했으며, AWS 계정의 루트 사용자 접근을 보호해야 합니다.

다음 중 어떤 조치의 조합이 이를 달성할 수 있습니까? (두 개 선택)

A. 루트 사용자가 강력한 비밀번호를 사용하도록 합니다.
B. 루트 사용자에 대해 다중 인증(MFA)을 활성화합니다.
C. 루트 사용자 액세스 키를 암호화된 Amazon S3 버킷에 저장합니다.
D. 루트 사용자를 관리자 권한이 포함된 그룹에 추가합니다.
E. 루트 사용자에 필요한 권한을 인라인 정책 문서로 적용합니다.

```
A solutions architect has created a new AWS account and must secure AWS account root user access.  
  
Which combination of actions will accomplish this? (Choose two.)

- A. Ensure the root user uses a strong password.
- B. Enable multi-factor authentication to the root user.
- C. Store root user access keys in an encrypted Amazon S3 bucket.
- D. Add the root user to a group containing administrative permissions.
- E. Apply the required permissions to the root user with an inline policy document.
```

정답 : `A, B`

- 루트 사용자 계정은 AWS 계정 전체에 대한 무제한 권한을 가지므로 비밀번호가 취약하면 계정이 위험에 노출
- MFA 활성화 : 루트 사용자 보안의 핵심 권장사항. 비밀번호만으로 충분하지 않으며 MFA를 추가해 계정 탈취 위험을 줄임

오답 이유

- **C. 루트 사용자 액세스 키를 암호화된 S3 버킷에 저장**
    - AWS 보안 모범 사례는 **루트 사용자에 대해 액세스 키를 생성하지 말 것**을 권고합니다. 필요 시에도 장기적으로 저장하는 대신 즉시 삭제해야 합니다.
    
- **D. 루트 사용자를 그룹에 추가**
    - 루트 사용자는 IAM 사용자와 달리 그룹에 속할 수 없습니다. 또한 루트 사용자 자체는 이미 모든 권한을 가지고 있습니다.
    
- **E. 인라인 정책으로 권한 부여**    
    - 루트 사용자는 이미 계정의 모든 권한을 가지고 있으므로 추가 정책은 불필요합니다.

## #234
한 회사가 새로운 웹 기반 고객 관계 관리(CRM) 애플리케이션을 구축하고 있습니다. 애플리케이션은 Application Load Balancer(ALB) 뒤에서 Amazon Elastic Block Store(Amazon EBS) 볼륨을 사용하는 여러 Amazon EC2 인스턴스를 사용할 것입니다. 애플리케이션은 또한 Amazon Aurora 데이터베이스를 사용할 것입니다. 애플리케이션의 모든 데이터는 저장 시(at rest)와 전송 중(in transit) 모두 암호화되어야 합니다.

이 요구사항을 충족할 수 있는 솔루션은 무엇입니까?

A. ALB에서 데이터를 전송 중 암호화하기 위해 AWS Key Management Service(AWS KMS) 인증서를 사용합니다. 저장 시 EBS 볼륨과 Aurora 데이터베이스 스토리지를 암호화하기 위해 AWS Certificate Manager(ACM)를 사용합니다.
B. AWS 루트 계정으로 AWS Management Console에 로그인합니다. 회사의 암호화 인증서를 업로드합니다. 루트 계정에서 계정의 모든 데이터에 대해 저장 시 및 전송 중 암호화를 켜는 옵션을 선택합니다.
C. 저장 시 EBS 볼륨과 Aurora 데이터베이스 스토리지를 암호화하기 위해 AWS Key Management Service(AWS KMS)를 사용합니다. 전송 중 데이터를 암호화하기 위해 ALB에 AWS Certificate Manager(ACM) 인증서를 연결합니다.
D. 저장 시 모든 데이터를 BitLocker로 암호화합니다. 회사의 TLS 인증서 키를 AWS Key Management Service(AWS KMS)로 가져옵니다. 전송 중 데이터를 암호화하기 위해 ALB에 KMS 키를 연결합니다.

```
A company is building a new web-based customer relationship management application. The application will use several Amazon EC2 instances that are backed by Amazon Elastic Block Store (Amazon EBS) volumes behind an Application Load Balancer (ALB). The application will also use an Amazon Aurora database. All data for the application must be encrypted at rest and in transit.  
  
Which solution will meet these requirements?

- A. Use AWS Key Management Service (AWS KMS) certificates on the ALB to encrypt data in transit. Use AWS Certificate Manager (ACM) to encrypt the EBS volumes and Aurora database storage at rest.
- B. Use the AWS root account to log in to the AWS Management Console. Upload the company’s encryption certificates. While in the root account, select the option to turn on encryption for all data at rest and in transit for the account.
- C. Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora database storage at rest. Attach an AWS Certificate Manager (ACM) certificate to the ALB to encrypt data in transit.
- D. Use BitLocker to encrypt all data at rest. Import the company’s TLS certificate keys to AWS Key Management Service (AWS KMS) Attach the KMS keys to the ALB to encrypt data in transit.
```

정답 : `C`

- 저장 시 암호화(at rest): EBS, Aurora 모두 AWS KMS 기반 암호화를 네이티브로 지원
- 전송 중 암호화(in transit): ALB는 ACM 인증서를 탑재해 TLS 종료를 수행, 클라이언트 ↔ ALB 구간을 HTTPS(LTS)로 보호하고, 필요시 ALB ↔ EC2 간도 내부 인증서로 TLS 적용 가능

오답 이유

- **A**: 뒤바뀐 사용. **KMS는 인증서를 제공하지 않으며**, **ACM이 TLS 인증서를** 관리/배포합니다. 저장 시 암호화는 KMS가 담당.
    
- **B**: 루트 계정으로 “계정 전체 저장/전송 암호화”를 일괄 켜는 **전역 스위치 같은 기능은 없습니다**. 각 리소스별로 설정해야 합니다.
    
- **D**: BitLocker는 Windows 디스크 암호화 솔루션으로 AWS 관리형 EBS/Aurora 암호화 요구와 무관하며, **ALB에 KMS 키를 ‘연결’하는 기능은 없습니다**. ALB는 **ACM 인증서**를 사용합니다.



## #235
한 회사가 온프레미스 Oracle 데이터베이스를 Amazon Aurora PostgreSQL로 이전하려고 합니다. 해당 데이터베이스에는 동일한 테이블에 쓰기를 수행하는 여러 애플리케이션이 있습니다. 애플리케이션들은 각 마이그레이션 사이에 한 달 간격을 두고 하나씩 순차적으로 마이그레이션되어야 합니다. 경영진은 데이터베이스의 읽기 및 쓰기 횟수가 매우 많다는 점을 우려하고 있습니다. 마이그레이션 전 과정에서 두 데이터베이스 간 데이터는 동기화 상태로 유지되어야 합니다.

솔루션스 아키텍트는 무엇을 권장해야 합니까?

A. 초기 마이그레이션에 AWS DataSync를 사용합니다. AWS Database Migration Service(AWS DMS)를 사용하여 변경 데이터 캡처(CDC) 복제 작업과 모든 테이블을 선택하는 테이블 매핑을 생성합니다.
B. 초기 마이그레이션에 AWS DataSync를 사용합니다. AWS Database Migration Service(AWS DMS)를 사용하여 전체 로드 + 변경 데이터 캡처(CDC) 복제 작업과 모든 테이블을 선택하는 테이블 매핑을 생성합니다.
C. AWS Schema Conversion Tool과 메모리 최적화(replication instance)형 AWS Database Migration Service(AWS DMS)를 사용합니다. 전체 로드 + 변경 데이터 캡처(CDC) 복제 작업과 모든 테이블을 선택하는 테이블 매핑을 생성합니다.
D. AWS Schema Conversion Tool과 컴퓨팅 최적화(replication instance)형 AWS Database Migration Service(AWS DMS)를 사용합니다. 전체 로드 + 변경 데이터 캡처(CDC) 복제 작업과 가장 큰 테이블만 선택하는 테이블 매핑을 생성합니다.

```
A company is moving its on-premises Oracle database to Amazon Aurora PostgreSQL. The database has several applications that write to the same tables. The applications need to be migrated one by one with a month in between each migration. Management has expressed concerns that the database has a high number of reads and writes. The data must be kept in sync across both databases throughout the migration.  
  
What should a solutions architect recommend?

- A. Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a change data capture (CDC) replication task and a table mapping to select all tables.
- B. Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.
- C. Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a memory optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.
- D. Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a compute optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select the largest tables.
```

정답 : `C`

- 이기종(오라클 to Aurora PostgreSQL) 이므로 먼저 AWS Schema Converstion Tool(SCT)로 스키마/객체(저장 프로시저, 함수 등)를 변환하는게 정석
- 애플리케이션을 한 달 간격으로 순차 이전하면서 두 DB를 지속 동기화 => AWS DMS의 "전체 로드 + CDC" 작업 필요
	- 전체 데이터 적재 후, 소스의 변경(INSERT/UPDATE/DELETE)을 실시간으로 타겟에 반영해 양쪽을 동기화
- 트래픽(읽기/쓰기)이 많고 CDC 트래픽도 큰 편으로 예상되므로, 메모리 최적화형 DMS 복제 인스턴스가 대량 트랜잭션/LOB 처리 및 캐싱에 유리해 안정적

오답 이유

- **A. DataSync + DMS(CDC만)**
    - DataSync는 파일/객체 스토리지 전송에 적합하며 **데이터베이스 초기 적재 용도에 맞지 않습니다.** 또한 CDC만으로는 초기 풀 로드가 없어 타겟이 온전하지 않습니다.
    
- **B. DataSync + DMS(Full + CDC)**
    - Full + CDC 조합 자체는 맞지만, **초기 로드에 DataSync를 쓸 이유가 없습니다.** DB → DB 마이그레이션은 DMS가 **풀 로드와 CDC를 모두** 수행합니다.
    
- **D. SCT + DMS(컴퓨팅 최적화) + 일부 테이블만**
    - 가장 큰 테이블만 선택하면 **동일 테이블에 쓰는 여러 앱**이 존재하는 본 시나리오에서 **데이터 정합성이 깨질 수 있습니다.** 또한 트래픽이 많은 워크로드에는 메모리 최적화가 더 적절합니다.


## #236
한 회사는 이미지 공유를 위한 3계층 애플리케이션을 보유하고 있습니다. 애플리케이션은 프런트엔드 계층에 Amazon EC2 인스턴스를, 애플리케이션 계층에 또 다른 EC2 인스턴스를, MySQL 데이터베이스에 세 번째 EC2 인스턴스를 사용합니다. 솔루션스 아키텍트는 애플리케이션에 대한 변경을 최소화하면서 확장 가능하고 고가용성인 솔루션을 설계해야 합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. Amazon S3를 사용하여 프런트엔드 계층을 호스팅합니다. 애플리케이션 계층에는 AWS Lambda 함수를 사용합니다. 데이터베이스를 Amazon DynamoDB 테이블로 이동합니다. 사용자의 이미지를 저장하고 제공하기 위해 Amazon S3를 사용합니다.
B. 프런트엔드 계층과 애플리케이션 계층에 대해 로드 밸런싱된 Multi-AZ AWS Elastic Beanstalk 환경을 사용합니다. 데이터베이스를 Amazon RDS DB 인스턴스로 이동하고 여러 읽기 복제본을 사용하여 사용자의 이미지를 제공합니다.
C. Amazon S3를 사용하여 프런트엔드 계층을 호스팅합니다. 애플리케이션 계층에는 Auto Scaling 그룹의 EC2 인스턴스 플릿을 사용합니다. 데이터베이스를 메모리 최적화 인스턴스 유형으로 이동하여 사용자의 이미지를 저장하고 제공합니다.
D. 프런트엔드 계층과 애플리케이션 계층에 대해 로드 밸런싱된 Multi-AZ AWS Elastic Beanstalk 환경을 사용합니다. 데이터베이스를 Amazon RDS Multi-AZ DB 인스턴스로 이동합니다. 사용자의 이미지를 저장하고 제공하기 위해 Amazon S3를 사용합니다.

```
A company has a three-tier application for image sharing. The application uses an Amazon EC2 instance for the front-end layer, another EC2 instance for the application layer, and a third EC2 instance for a MySQL database. A solutions architect must design a scalable and highly available solution that requires the least amount of change to the application.  
  
Which solution meets these requirements?

- A. Use Amazon S3 to host the front-end layer. Use AWS Lambda functions for the application layer. Move the database to an Amazon DynamoDB table. Use Amazon S3 to store and serve users’ images.
- B. Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS DB instance with multiple read replicas to serve users’ images.
- C. Use Amazon S3 to host the front-end layer. Use a fleet of EC2 instances in an Auto Scaling group for the application layer. Move the database to a memory optimized instance type to store and serve users’ images.
- D. Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS Multi-AZ DB instance. Use Amazon S3 to store and serve users’ images.
```

정답 : `D`

- 변경 최소화: 기존 EC2 기반 웹/앱을 Elastic Beanstalk(로드밸런싱+오토스케일+멀티-AZ)으로 감싸 배포하면 애플리케이션 코드는 크게 바꾸지 않고 확장성과 고가용성 확보
- DB 고가용성: MySQL을 Amazon RDS Multi-AZ로 이전하면 자동 장애조치와 관리형 백업으로 내구성과 가용성을 확보
- 이미지 저장소 분리: 대용량 바이너리는 DB가 아닌 Amazon S3에 저장/서빙하는 것이 확장･비용･성능 면에서 최적이며 CloudFront 연동도 용이

오답 이유

- **A**: 서버리스/NoSQL 아키텍처로의 전환(Lambda, DynamoDB)은 **코드/데이터 모델 변경이 큼**. “변경 최소화” 요구에 부적합.
    
- **B**: Beanstalk까지는 좋지만, **이미지를 RDS 읽기 복제본으로 제공**하는 것은 비효율적(대용량 오브젝트를 DB로 제공 X). 이미지 저장/서빙은 S3가 적합.
    
- **C**: 프런트 S3 호스팅은 가능하나, **DB를 더 큰 EC2(메모리 최적화)로** 옮겨도 고가용성이 보장되지 않음(여전히 셀프매니지드). 이미지도 DB에서 제공한다고 되어 비효율적.


## #237
VPC-A의 Amazon EC2 인스턴스에서 실행 중인 애플리케이션이 VPC-B의 다른 EC2 인스턴스의 파일에 접근해야 합니다. 두 VPC는 서로 다른 AWS 계정에 있습니다. 네트워크 관리자는 VPC-A에서 VPC-B의 EC2 인스턴스로 안전하게 접근하도록 구성할 수 있는 솔루션을 설계해야 합니다. 연결은 단일 실패 지점(SPOF)이 없어야 하며, 대역폭 우려도 없어야 합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. VPC-A와 VPC-B 사이에 VPC 피어링 연결을 설정합니다.
B. VPC-B에서 실행 중인 EC2 인스턴스를 위해 VPC 게이트웨이 엔드포인트를 설정합니다.
C. VPC-B에 가상 프라이빗 게이트웨이를 연결하고 VPC-A에서 라우팅을 설정합니다.
D. VPC-B에서 실행 중인 EC2 인스턴스를 위해 프라이빗 가상 인터페이스(VIF)를 생성하고 VPC-A에서 적절한 라우트를 추가합니다.

```
An application running on an Amazon EC2 instance in VPC-A needs to access files in another EC2 instance in VPC-B. Both VPCs are in separate AWS accounts. The network administrator needs to design a solution to configure secure access to EC2 instance in VPC-B from VPC-A. The connectivity should not have a single point of failure or bandwidth concerns.  
  
Which solution will meet these requirements?

- A. Set up a VPC peering connection between VPC-A and VPC-B.
- B. Set up VPC gateway endpoints for the EC2 instance running in VPC-B.
- C. Attach a virtual private gateway to VPC-B and set up routing from VPC-A.
- D. Create a private virtual interface (VIF) for the EC2 instance running in VPC-B and add appropriate routes from VPC-A.
```

정답 : `A`

- VPC Peering은 두 VPC(동일/상이한 계정 간 모두 가능) 간 프라이빗 IP 기반의 직접 라우팅을 제공해 인터넷을 통하지 않고 보안적으로 통신
- 서비스 자체가 고가용이며, 전용 대역폭 한도 같은 대역폭 병목이 없음
	- 처리량은 주로 EC2/ENI 네트워크 성능과 경로상 링크에 의해 결정
- 라우 테이블과 보안 그룹/네트워크 ACL만으로 간단히 구성할 수 있어 변경 최소, 단일 실패 지점을 만들지 않음

오답 이유

- **B. VPC 게이트웨이 엔드포인트**
    - 게이트웨이 엔드포인트는 **S3/DynamoDB 전용**입니다. EC2↔EC2 통신에는 사용할 수 없습니다.
    
- **C. 가상 프라이빗 게이트웨이(VGW) + 라우팅**
    - 이는 **사이트-투-사이트 VPN/Direct Connect** 시나리오용입니다. VPC-to-VPC 연결로 쓰려면 추가적으로 고객 게이트웨이/소프트웨어 VPN 구성 등이 필요하고, **터널 당 대역폭 제한**과 **SPOF** 고려(이중 터널 구성 등)로 관리 부담이 큽니다.
    
- **D. 프라이빗 VIF 생성**
    - **프라이빗 VIF는 Direct Connect 전용**으로 VPC(및 그 안의 리소스)로의 온프레미스 연결을 위한 것입니다. 개별 EC2 인스턴스에 VIF를 붙일 수 없으며, 본 시나리오에 부적합합니다.



## #238
한 회사가 엔지니어 팀을 위해 개별 AWS 계정을 사용해 실험하려고 합니다. 회사는 각 계정별로 해당 월의 Amazon EC2 인스턴스 사용 비용이 특정 임계값을 초과하자마자 즉시 알림을 받고자 합니다.

이 요구사항을 가장 비용 효율적으로 충족하려면 솔루션스 아키텍트는 무엇을 해야 합니까?

A. Cost Explorer를 사용하여 서비스별 일일 비용 보고서를 생성합니다. 보고서를 EC2 인스턴스로 필터링합니다. 임계값을 초과하면 Amazon Simple Email Service(Amazon SES) 알림을 보내도록 Cost Explorer를 구성합니다.
B. Cost Explorer를 사용하여 서비스별 월간 비용 보고서를 생성합니다. 보고서를 EC2 인스턴스로 필터링합니다. 임계값을 초과하면 Amazon Simple Email Service(Amazon SES) 알림을 보내도록 Cost Explorer를 구성합니다.
C. AWS Budgets를 사용하여 각 계정에 대한 비용 예산을 생성합니다. 기간을 월간으로 설정합니다. 범위를 EC2 인스턴스로 설정합니다. 예산에 대한 알림 임계값을 설정합니다. 임계값이 초과되면 알림을 수신하도록 Amazon Simple Notification Service(Amazon SNS) 주제를 구성합니다.
D. AWS Cost and Usage Reports를 사용하여 시간 단위 세분성의 보고서를 생성합니다. 보고서 데이터를 Amazon Athena와 통합합니다. Amazon EventBridge를 사용하여 Athena 쿼리를 일정 실행합니다. 임계값이 초과되면 알림을 수신하도록 Amazon Simple Notification Service(Amazon SNS) 주제를 구성합니다.

```
A company wants to experiment with individual AWS accounts for its engineer team. The company wants to be notified as soon as the Amazon EC2 instance usage for a given month exceeds a specific threshold for each account.  
  
What should a solutions architect do to meet this requirement MOST cost-effectively?

- A. Use Cost Explorer to create a daily report of costs by service. Filter the report by EC2 instances. Configure Cost Explorer to send an Amazon Simple Email Service (Amazon SES) notification when a threshold is exceeded.
- B. Use Cost Explorer to create a monthly report of costs by service. Filter the report by EC2 instances. Configure Cost Explorer to send an Amazon Simple Email Service (Amazon SES) notification when a threshold is exceeded.
- C. Use AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the scope to EC2 instances. Set an alert threshold for the budget. Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded.
- D. Use AWS Cost and Usage Reports to create a report with hourly granularity. Integrate the report data with Amazon Athena. Use Amazon EventBridge to schedule an Athena query. Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded.
```

정답 : `C`

- AWS Budgets는 서비스별(EC2만) 로 범위를 제한한 월간 비용 예산과 임계값 초과 시 즉시 SNS 알림을 기본 제공, 설정이 간단하고 추가 인프라 필요 없음
- 각 계정에서 예산을 생성하면 계정별로 독립적으로 임계값을 감시할 수 있어 요구사항("각 계정별, 월 중 즉시 알림")을 가장 비용 효율적이고 운영 오버헤드 없이 충족

오답 이유

- **A/B (Cost Explorer + SES 알림)**
    - Cost Explorer는 **리포팅/분석** 도구이며 **임계값 기준의 실시간 알림 기능을 직접 제공하지 않습니다.** SES 연동으로 임계값 알림을 설정하는 기능도 없습니다. 알림 목적엔 Budgets가 정답.
    
- **D (CUR + Athena + EventBridge + SNS)**
    - 가능은 하지만 **구성 요소가 과도**(CUR, S3, Athena 테이블, 쿼리, 스케줄러, 알림)하여 **운영 복잡도와 비용**이 증가합니다. 요구사항은 Budgets 한 번으로 해결 가능.


## #239
솔루션스 아키텍트는 회사 애플리케이션을 위한 새로운 마이크로서비스를 설계해야 합니다. 클라이언트는 HTTPS 엔드포인트를 호출해 마이크로서비스에 도달할 수 있어야 합니다. 또한 마이크로서비스는 호출을 인증하기 위해 AWS Identity and Access Management(IAM)를 사용해야 합니다. 솔루션스 아키텍트는 Go 1.x로 작성된 단일 AWS Lambda 함수를 사용해 이 마이크로서비스의 로직을 작성할 것입니다.

다음 중 가장 운영 효율적으로 함수를 배포할 수 있는 솔루션은 무엇입니까?

A. Amazon API Gateway REST API를 생성합니다. 메서드를 Lambda 함수를 사용하도록 구성합니다. API에서 IAM 인증을 활성화합니다.
B. 함수에 대해 Lambda Function URL을 생성합니다. 인증 유형으로 AWS_IAM을 지정합니다.
C. Amazon CloudFront 배포를 생성합니다. 함수를 Lambda@Edge로 배포합니다. IAM 인증 로직을 Lambda@Edge 함수에 통합합니다.
D. Amazon CloudFront 배포를 생성합니다. 함수를 CloudFront Functions로 배포합니다. 인증 유형으로 AWS_IAM을 지정합니다.

```
A solutions architect needs to design a new microservice for a company’s application. Clients must be able to call an HTTPS endpoint to reach the microservice. The microservice also must use AWS Identity and Access Management (IAM) to authenticate calls. The solutions architect will write the logic for this microservice by using a single AWS Lambda function that is written in Go 1.x.  
  
Which solution will deploy the function in the MOST operationally efficient way?

- A. Create an Amazon API Gateway REST API. Configure the method to use the Lambda function. Enable IAM authentication on the API.
- B. Create a Lambda function URL for the function. Specify AWS_IAM as the authentication type.
- C. Create an Amazon CloudFront distribution. Deploy the function to Lambda@Edge. Integrate IAM authentication logic into the Lambda@Edge function.
- D. Create an Amazon CloudFront distribution. Deploy the function to CloudFront Functions. Specify AWS_IAM as the authentication type.
```

정답 : `B`

- Lambda Function URL은 람다가 직접 제공하는 HTTPS 엔드포인트로, 별도의 API 게이트웨이나 엣지 컴퓨팅 구성 없이도 호출 가능
- 인증을 AWS_IAM(SigV4)로 설정하면 IAM 정책으로 호출 권한을 제어할 수 있어 요구사항(HTTPS+IAM 인증) 충족
- 관리해야 할 리소스가 단일 람다 함수뿐이라 운영 오버헤드 최소

오답 이유

- **A. API Gateway REST API**
    - IAM 인증을 지원하고 Lambda 통합도 쉬우나, **추가 관리 리소스(API, 스테이지, 배포 등)** 가 생겨 **운영 효율** 면에서 Function URL보다 무겁습니다.
    
- **C. CloudFront + Lambda@Edge**
    - Lambda@Edge는 CloudFront 요청/응답 변형용입니다. **IAM 인증을 기본 제공하지 않으며** 인증 로직을 직접 구현해야 하고, 배포/전파에도 시간이 걸려 과합니다.
    
- **D. CloudFront + CloudFront Functions**
    - CloudFront Functions는 경량 JavaScript 엣지 함수로 **Lambda(Go) 실행을 대체할 수 없고**, AWS_IAM 같은 인증 타입을 **지정할 수 없습니다**. 또한 원래 목적은 헤더/URI 변형 등입니다.


## #240
한 회사는 이전에 데이터 웨어하우스 솔루션을 AWS로 마이그레이션했습니다. 이 회사는 AWS Direct Connect 연결도 보유하고 있습니다. 본사 사용자는 시각화 도구를 사용하여 데이터 웨어하우스를 쿼리합니다. 데이터 웨어하우스가 반환하는 쿼리의 평균 크기는 50MB이며, 시각화 도구가 전송하는 각 웹페이지는 대략 500KB입니다. 데이터 웨어하우스에서 반환되는 결과 집합은 캐시되지 않습니다.

다음 중 어떤 솔루션이 회사에 가장 낮은 데이터 전송 이그레스 비용을 제공합니다?

A. 시각화 도구를 온프레미스에 호스팅하고 인터넷을 통해 데이터 웨어하우스를 직접 쿼리합니다.
B. 시각화 도구를 데이터 웨어하우스와 동일한 AWS 리전에 호스팅하고, 인터넷을 통해 접근합니다.
C. 시각화 도구를 온프레미스에 호스팅하고, 동일한 AWS 리전의 위치에서 Direct Connect 연결을 통해 데이터 웨어하우스를 직접 쿼리합니다.
D. 시각화 도구를 데이터 웨어하우스와 동일한 AWS 리전에 호스팅하고, 동일한 리전의 위치에서 Direct Connect 연결을 통해 접근합니다.

```
A company previously migrated its data warehouse solution to AWS. The company also has an AWS Direct Connect connection. Corporate office users query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 50 MB and each webpage sent by the visualization tool is approximately 500 KB. Result sets returned by the data warehouse are not cached.  
  
Which solution provides the LOWEST data transfer egress cost for the company?

- A. Host the visualization tool on premises and query the data warehouse directly over the internet.
- B. Host the visualization tool in the same AWS Region as the data warehouse. Access it over the internet.
- C. Host the visualization tool on premises and query the data warehouse directly over a Direct Connect connection at a location in the same AWS Region.
- D. Host the visualization tool in the same AWS Region as the data warehouse and access it over a Direct Connect connection at a location in the same Region.
```

정답 : `D`

- 대용량(50MB) 쿼리 결과가 AWS 밖으로 나가지 않도록 시각화 도구를 데이터 웨어하우스와 동일 리전에 두면, DW ↔ 시각화 도구 간 트래픽은 리전 내부(동일 AZ이면 무료/동일 리전 간 저렴)로 처리되어 이그레스 비용이 거의/전혀 들지 않음
- 최종 사용자에게 나가는 것은 작은 웹 페이지(약 500KB) 뿐이므로, 이를 Direct Connect 로 전송하면 인터넷 이그레스보다 더 낮은 전송 단가를 적용받아 총 이그레스 비용 최소화

오답 이유

- **A. 온프레미스 호스팅 + 인터넷**: 50MB 결과 세트가 **인터넷 이그레스**로 지속 전송되어 **비용 최댓값**.
    
- **B. 동일 리전 호스팅 + 인터넷 접근**: 대용량 결과는 리전 내부라 좋지만, 사용자에게 가는 500KB 페이지들이 **인터넷 이그레스 단가**를 적용받아 D보다 비쌉니다.
    
- **C. 온프레미스 호스팅 + Direct Connect**: 인터넷보다는 싸지만 여전히 **50MB 결과**가 매번 **Direct Connect 이그레스**로 빠져나가 **트래픽량 자체가 큼**. D보다 비용이 큼.