---
created: 2025-09-25 14:36:22
last_modified: 2025-09-30 19:16:04
---
## #41
한 회사의 애플리케이션은 데이터 수집을 위해 여러 SaaS(Software-as-a-Service) 소스와 통합됩니다.  
회사는 데이터를 수신하고 분석을 위해 Amazon S3 버킷에 업로드하기 위해 Amazon EC2 인스턴스를 운영합니다.  
데이터를 수신하고 업로드하는 동일한 EC2 인스턴스가 업로드 완료 시 사용자에게 알림도 보냅니다.  
회사는 애플리케이션 성능이 느려지는 것을 관찰했고 가능한 한 성능을 개선하고자 합니다.  

어떤 솔루션이 최소한의 운영 오버헤드로 이러한 요구사항을 충족합니까?

A. Auto Scaling 그룹을 생성하여 EC2 인스턴스가 스케일 아웃할 수 있게 합니다. S3 이벤트 알림을 구성하여 S3 버킷으로의 업로드가 완료되면 Amazon Simple Notification Service(Amazon SNS) 주제로 이벤트를 전송합니다.  

B. 각 SaaS 소스와 S3 버킷 간에 데이터를 전송하도록 Amazon AppFlow 플로우를 생성합니다. S3 버킷으로의 업로드가 완료되면 S3 이벤트 알림을 구성하여 Amazon Simple Notification Service(Amazon SNS) 주제로 이벤트를 전송합니다.  

C. 각 SaaS 소스에 대해 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성하여 출력 데이터를 전송합니다. S3 버킷을 규칙의 대상으로 구성합니다. S3 버킷으로의 업로드가 완료되면 이벤트를 전송하도록 두 번째 EventBridge(CloudWatch Events) 규칙을 생성합니다. 두 번째 규칙의 대상으로 Amazon Simple Notification Service(Amazon SNS) 주제를 구성합니다.  

D. EC2 인스턴스 대신 사용할 Docker 컨테이너를 생성합니다. 컨테이너화된 애플리케이션을 Amazon Elastic Container Service(Amazon ECS)에 호스팅합니다. Amazon CloudWatch Container Insights를 구성하여 S3 버킷으로의 업로드가 완료되면 Amazon Simple Notification Service(Amazon SNS) 주제로 이벤트를 전송합니다.
```
A company's application integrates with multiple software-as-a-service (SaaS) sources for data collection. The company runs Amazon EC2 instances to receive the data and to upload the data to an Amazon S3 bucket for analysis. The same EC2 instance that receives and uploads the data also sends a notification to the user when an upload is complete. The company has noticed slow application performance and wants to improve the performance as much as possible.  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an Auto Scaling group so that EC2 instances can scale out. Configure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.
- B. Create an Amazon AppFlow flow to transfer data between each SaaS source and the S3 bucket. Configure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.
- C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for each SaaS source to send output data. Configure the S3 bucket as the rule's target. Create a second EventBridge (Cloud Watch Events) rule to send events when the upload to the S3 bucket is complete. Configure an Amazon Simple Notification Service (Amazon SNS) topic as the second rule's target.
- D. Create a Docker container to use instead of an EC2 instance. Host the containerized application on Amazon Elastic Container Service (Amazon ECS). Configure Amazon CloudWatch Container Insights to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.
```

정답 : `B`

- 현재 병목은 EC2 인스턴스가 데이터 수신,업로드,알림을 모두 담당하면서 발생
	- EC2에서 IO, 네트워크, 애플리케이션 로직을 모두 담당하니 성능 저하와 운영 부담
- Amazon AppFlow는 여러 SaaS(예: Salesforce, Slack, Zendesk 등)와의 안전한 커넥터를 제공하는 완전관리형 서비스
	- SaaS -> S3 데이터 전송을 EC2 없이 직접 처리 가능
	- 데이터 수집/전송 책임을 관리형 서비스로 옮겨 EC2 부하 제거와 지연 시간 개선
- 업로드 완료 후 알림은 S3 이벤트 알림 -> SNS 구성으로 처리
	- 업로드 완료 시 사용자 알림 요구사항도 충족

오답 이유

- **A. Auto Scaling + S3 이벤트 → SNS**
    - 장점: 수평 확장으로 EC2 처리량을 늘릴 수 있으나, EC2가 여전히 데이터 수신과 업로드 논리를 수행해야 하므로 **운영 부담과 관리(AMIs, 패치, 모니터링, 스케일 정책 등)**는 남습니다.
    - Auto Scaling은 처리량을 늘려 지연을 줄이지만, 관리형 SaaS 커넥터(인증, API 변화 등)를 계속 운영해야 하고 비용/운영 오버헤드가 더 큽니다. 따라서 **최소 운영 오버헤드** 요구에 부합하지 않습니다.
    
- **C. EventBridge 규칙을 이용하여 SaaS → S3 및 알림 처리**
    - 일부 SaaS는 EventBridge 직접 통합을 지원하지만 모든 SaaS에 대해 표준화되어 있지 않으며, SaaS별로 이벤트 매핑/권한/전송 방식이 달라 **설정·유지보수 복잡성**이 큽니다.
    - 또한, EventBridge 규칙을 통해 S3에 직접 데이터를 전송하는 작업은 일반적으로 이벤트 기반으로 메타데이터 전달에 적합하고 대량 데이터 페이로드 전송에는 적합하지 않습니다.
    - 따라서 운영적 단순화나 대량 데이터 수집 관점에서 비효율적입니다.
    
- **D. 컨테이너화(ECS) + Container Insights → SNS**
    - 컨테이너로 옮기면 인프라는 경량화될 수 있으나 여전히 애플리케이션 로직(데이터 수신/업로드/알림)을 직접 운영해야 합니다.
    - Container Insights는 모니터링용이지 업로드 완료 알림의 적절한 트리거가 아닙니다(또한, “S3 업로드 완료”를 자동으로 SNS로 보내려면 별도 구현 필요).
    - 컨테이너 오케스트레이션과 CI/CD, 이미지 관리 등 **운영 오버헤드**가 증가합니다.

## #42
한 회사가 단일 VPC에서 Amazon EC2 인스턴스로 고가용성 이미지 처리 애플리케이션을 실행합니다. EC2 인스턴스는 여러 가용 영역에 걸쳐 여러 서브넷 내부에서 실행됩니다. EC2 인스턴스는 서로 통신하지 않습니다. 그러나 EC2 인스턴스는 단일 NAT 게이트웨이를 통해 Amazon S3에서 이미지를 다운로드하고 Amazon S3에 이미지를 업로드합니다. 회사는 데이터 전송 요금에 대해 우려하고 있습니다.  
회사가 지역(Regional) 데이터 전송 요금을 피하기 위한 가장 비용 효율적인 방법은 무엇입니까?

A. 각 가용 영역에 NAT 게이트웨이를 시작합니다.  
B. NAT 게이트웨이를 NAT 인스턴스로 교체합니다.  
C. Amazon S3용 게이트웨이 VPC 엔드포인트를 배포합니다.  
D. EC2 인스턴스를 실행하기 위해 EC2 전용 호스트(Dedicated Host)를 프로비저닝합니다.

```
A company runs a highly available image-processing application on Amazon EC2 instances in a single VPC. The EC2 instances run inside several subnets across multiple Availability Zones. The EC2 instances do not communicate with each other. However, the EC2 instances download images from Amazon S3 and upload images to Amazon S3 through a single NAT gateway. The company is concerned about data transfer charges.  
What is the MOST cost-effective way for the company to avoid Regional data transfer charges?

- A. Launch the NAT gateway in each Availability Zone.
- B. Replace the NAT gateway with a NAT instance.
- C. Deploy a gateway VPC endpoint for Amazon S3.
- D. Provision an EC2 Dedicated Host to run the EC2 instances.
```

정답 : `C`

- Gateway VPC Endpoint (S3)는 VPC의 라우팅 테이블에 경로를 추가해 S3와의 트래픽이 퍼블릭 인터넷이나 NAT 게이트웨이를 통하지 않고 AWS 네트워크 내에서 직접 전달
- 이 구성은 인터넷 게이트웨이/NAT 게이트웨이 통한 전송에 따른 가용 영역 간(Regional) 또는 인터넷 전송 요금이 발생하지 않음
- 현재 모든 인스턴스가 단일 NAT 게이트웨이를 통해 S3에 접근함으로써 가용영역 간 트래픽(또는 NAT 처리 비용)이 발생할 수 있어 S3영 게이트웨이 엔드포인트로 바꾸면 데이터 전송 요금을 줄이면서 운영도 간단화할 수 있음

오답 이유

- **A. 각 가용 영역에 NAT 게이트웨이를 시작합니다.**
    - 장점: 각 AZ에 NAT 게이트웨이를 두면 AZ 간 트래픽으로 인한 추가 비용은 줄일 수 있음(인스턴스가 같은 AZ의 NAT를 사용하면 cross-AZ 요금 회피 가능).
    - 단점: NAT 게이트웨이는 시간당 요금 + 처리량(GB)당 요금이 발생하므로, 여러 AZ에 NAT를 배포하면 비용이 증가하고 관리 복잡성도 올라갑니다. S3 전용으로는 게이트웨이 엔드포인트가 더 저비용이며 단순합니다.
    
- **B. NAT 게이트웨이를 NAT 인스턴스로 교체합니다.**
    - 장점: 일부 시나리오에서 NAT 인스턴스가 비용면에서 저렴할 수 있음.
    - 단점: NAT 인스턴스는 고가용성/확장성/관리(패치, 모니터링, 스케일아웃) 측면에서 운영 부담이 큽니다. 또한 S3 트래픽이 여전히 NAT을 통해 나가면 데이터 처리/전송 비용은 남습니다. 따라서 “가장 비용 효율적이면서 운영 부담 최소” 조건을 만족시키지 못합니다.
    
- **D. EC2 Dedicated Host 프로비저닝**
    - Dedicated Host는 라이선스/물리서버 격리 요구가 있을 때 쓰는 옵션으로 네트워크 전송 요금이나 S3 접근 비용과 관련 없습니다.
    - 이 옵션은 문제의 목적(지역 데이터 전송 요금 회피 및 비용 절감)과 무관하며 운영/비용 측면에서도 부적합합니다.


## #43
한 회사는 많은 양의 시기 민감(time-sensitive) 데이터를 생성하는 온프레미스 애플리케이션을 운영하고 있으며, 이 데이터를 Amazon S3에 백업합니다. 애플리케이션이 성장하면서 내부 사용자들이 인터넷 대역폭 제한으로 불만을 제기하고 있습니다. 솔루션 설계자는 온프레미스 사용자들에게 미치는 영향을 최소화하면서 S3로 시기 적절한(timely) 백업을 장기적으로 수행할 수 있는 솔루션을 설계해야 합니다.  
어떤 솔루션이 이러한 요구사항을 충족합니까?

A. AWS VPN 연결을 설정하고 모든 트래픽을 VPC 게이트웨이 엔드포인트를 통해 프록시합니다.  
B. 새로운 AWS Direct Connect 연결을 설정하고 백업 트래픽을 이 새로운 연결을 통해 전송합니다.  
C. 매일 AWS Snowball 장치를 주문합니다. 데이터를 Snowball 장치에 적재한 후 매일 장치를 AWS에 반납합니다.  
D. AWS 관리 콘솔을 통해 지원 티켓을 제출합니다. 계정에서 S3 서비스 한도를 제거해 달라고 요청합니다.

```
A company has an on-premises application that generates a large amount of time-sensitive data that is backed up to Amazon S3. The application has grown and there are user complaints about internet bandwidth limitations. A solutions architect needs to design a long-term solution that allows for both timely backups to Amazon S3 and with minimal impact on internet connectivity for internal users.  
Which solution meets these requirements?

- A. Establish AWS VPN connections and proxy all traffic through a VPC gateway endpoint.
- B. Establish a new AWS Direct Connect connection and direct backup traffic through this new connection.
- C. Order daily AWS Snowball devices. Load the data onto the Snowball devices and return the devices to AWS each day.
- D. Submit a support ticket through the AWS Management Console. Request the removal of S3 service limits from the account.
```

정답 : `B`

- 요구사항
	- 장기적이고
	- 시기 민감한(빠른) 백업
	- 내부 인터넷 대역폭에 대한 영향 최소화
- AWS Direct Connect는 온프레미스와 AWS 간의 전용 네트워크 회선을 제공하므로 대용량 데이터 전송을 인터넷에 의존하지 않고 전송 가능
	- 인터넷 트래픽에 의한 대역폭 경쟁을 피할 수 있어 사용자 품질 저하 막음
	- 대역폭을 예약(1Gb/10Gb 등)할 수 있고 지연/인관성 개선
- 설정 후 전용 회선으로 안정적 전송 -> 장기적 관점에서도 운영 관리도 단순


오답 이유

- **A. AWS VPN + VPC 게이트웨이 엔드포인트로 모든 트래픽 프록시**
    - VPN은 일반적으로 인터넷을 통해 터널링되거나 (AWS Managed VPN의 경우) 역시 인터넷 경유하는 경우가 많아 **인터넷 대역폭 문제를 완전히 해결하지 못함**.
    - 또한 “모든 트래픽을 VPC 게이트웨이 엔드포인트로 프록시”한다는 개념이 실제 네트워크 라우팅/보안 관점에서 잘 맞지 않음(게이트웨이 엔드포인트는 VPC 내부에서 S3로 가는 경로를 제공하지만, 온프레→AWS 트래픽 자체는 물리적 회선이 필요).
    - 따라서 내부 인터넷 부하를 장기적으로 줄이는 데 한계가 있습니다.
    
- **C. 매일 Snowball 장치를 주문하여 반납**
    - Snowball/Snowball Edge는 대량 데이터 전송(오프라인 전송)에 유리하지만 **매일** 디바이스를 주문·적재·반납하는 것은 **운영 부담이 매우 크고(물류/스케줄링/비용)**, 또한 ‘시기 민감한(즉시성)’ 백업 요구를 충족하지 못합니다.
    - 일상적·지속적·시의성 있는 백업에는 부적절합니다.
    
- **D. S3 서비스 한도 제거 요청**
    - 단순히 서비스 한도(quotas) 해제는 인터넷 대역폭 문제를 해결하지 못합니다.
    - 또한 S3 한도은 보통 요청 레이트/동시성 등과 관련되나, 온프레 인터넷 회선 포화 문제와는 무관합니다.

## #44
한 회사에 중요한 데이터가 포함된 Amazon S3 버킷이 있습니다. 회사는 실수로 데이터가 삭제되는 것을 방지해야 합니다.  
솔루션 설계자가 이 요구사항을 충족하기 위해 어떤 조합의 단계를 수행해야 합니까? (둘을 선택하세요.)

A. S3 버킷에서 버저닝(Versioning)을 활성화합니다.  
B. S3 버킷에서 MFA Delete를 활성화합니다.  
C. S3 버킷에 버킷 정책을 생성합니다.  
D. S3 버킷에서 기본 암호화를 활성화합니다.  
E. S3 버킷의 객체에 대해 수명 주기(Lifecycle) 정책을 생성합니다.

```
A company has an Amazon S3 bucket that contains critical data. The company must protect the data from accidental deletion.  
Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Enable versioning on the S3 bucket.
- B. Enable MFA Delete on the S3 bucket.
- C. Create a bucket policy on the S3 bucket.
- D. Enable default encryption on the S3 bucket.
- E. Create a lifecycle policy for the objects in the S3 bucket.
```

정답 : `A, B`

- 버저닝을 활성화 하면 객체가 삭제되거나 덮어써질 때 기존 버전이 보존
- 실수로 삭제된 객체는 이전 버전을 복원하거나 delete marker를 제거해 복구 가능
- MFA Delete는 객체의 영구 삭제(버전 삭제) 및 버저닝 상태 변경(예: 버저닝 비활성화)을 수행하려면 다중 인증(MFA)을 요구해 실수 또는 악의적인 삭제로부터 추가 보호층 제공
- 두 가지를 함께 사용해 복구 가능성(버전 보존) + 영구 삭제 방지(MFA)라는 방어 ㅅ미층화를 구축 가능

오답 이유

- **C. S3 버킷에 버킷 정책을 생성합니다. — 오답**
    - 이유: 버킷 정책으로 삭제 작업을 거부(deny)하도록 설정할 수는 있으나, 권한이 있는 관리자가 정책을 변경할 수 있고, 실수로 객체를 삭제한 뒤에도 복구 가능한 이전 버전을 자동으로 제공하지는 않습니다. 버전 보존·영구 삭제 방지를 직접 제공하는 A/B보다 목적에 덜 직접적이며 단독으로는 권장되지 않습니다.
    
- **D. S3 버킷에서 기본 암호화를 활성화합니다. — 오답**
    - 이유: 기본 암호화는 데이터 기밀성(암호화)을 제공합니다. 삭제 방지와는 관련이 없어 요구사항을 충족하지 않습니다.
    
- **E. S3 버킷의 객체에 대해 수명 주기 정책을 생성합니다. — 오답**
    - 이유: 수명주기 정책은 객체의 이전 버전 만료나 객체 자동 삭제를 수행할 수 있으므로 **오히려 삭제를 자동화**할 수 있습니다. 실수 삭제 방지 목적과 상충하므로 단독 사용은 부적절합니다(단, 버전 보존 후 오래된 버전만 일정 기간 후 삭제하는 식으로 보완적으로 사용 가능).


## #45
한 회사에는 다음으로 구성된 데이터 수집 워크플로가 있습니다:
• 새 데이터 전달에 대한 알림을 위한 Amazon Simple Notification Service (Amazon SNS) 주제
• 데이터를 처리하고 메타데이터를 기록하기 위한 AWS Lambda 함수

회사는 네트워크 연결 문제로 인해 데이터 수집 워크플로가 가끔 실패하는 것을 관찰했습니다. 이러한 실패가 발생하면 회사가 수동으로 작업을 다시 실행하지 않는 한 Lambda 함수는 해당 데이터를 수집하지 않습니다.

솔루션 설계자가 향후 Lambda 함수가 모든 데이터를 수집하도록 보장하려면 어떤 조치 조합을 취해야 합니까? (두 가지를 선택하세요.)

A. 여러 가용 영역(Availability Zones)에 Lambda 함수를 배포합니다.  
B. Amazon Simple Queue Service (Amazon SQS) 대기열을 생성하고, 해당 대기열을 SNS 주제에 구독시킵니다.  
C. Lambda 함수에 할당된 CPU와 메모리를 늘립니다.  
D. Lambda 함수의 프로비저닝 처리량을 늘립니다.  
E. Lambda 함수가 Amazon Simple Queue Service (Amazon SQS) 대기열에서 읽도록 수정합니다.

```
A company has a data ingestion workflow that consists of the following:  
• An Amazon Simple Notification Service (Amazon SNS) topic for notifications about new data deliveries  
• An AWS Lambda function to process the data and record metadata  
The company observes that the ingestion workflow fails occasionally because of network connectivity issues. When such a failure occurs, the Lambda function does not ingest the corresponding data unless the company manually reruns the job.  
Which combination of actions should a solutions architect take to ensure that the Lambda function ingests all data in the future? (Choose two.)

- A. Deploy the Lambda function in multiple Availability Zones.
- B. Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.
- C. Increase the CPU and memory that are allocated to the Lambda function.
- D. Increase provisioned throughput for the Lambda function.
- E. Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue.
```

정답 : `B, E`

- SNS는 메시지 전송은 보장하지만, 소비자가 메시지 처리에 실패할 경우 재시도하거나 보존하는 기능은 제한적
- SQS를 SNS의 구독자로 연결해 메시지를 큐에 보관하고 람다가 SQS를 소비하도록해 람다 함수에서 장애에서 복구된 후에 큐에 남아있는 메시지를 소비해 모든 데이터 수집 보장

오답 이유

- **A. 여러 AZ에 Lambda 함수를 배포**
    - 오답. Lambda는 기본적으로 멀티 AZ 고가용성으로 관리되며, 수동으로 여러 AZ 배포를 고려할 필요가 없습니다. 문제의 원인은 AZ 가용성 문제가 아니라 메시지 유실이므로 해당하지 않습니다.
    
- **C. Lambda 함수 CPU/메모리 증가**
    - 오답. CPU/메모리는 성능을 높이는 데 영향을 주지만, 네트워크 장애로 인한 메시지 유실 문제를 해결하지 못합니다.
    
- **D. Lambda 함수 프로비저닝 처리량 증가**
    - 오답. Lambda에는 “프로비저닝 처리량”이라는 개념이 없습니다. 대신 **Provisioned Concurrency**가 있지만 이는 콜드 스타트 지연을 줄이는 용도로, 메시지 보존/재처리 보장과 무관합니다.

## #46
한 회사는 매장에 대한 마케팅 서비스를 제공하는 애플리케이션을 운영합니다. 이 서비스는 매장 고객의 과거 구매를 기반으로 합니다. 매장들은 SFTP를 통해 거래 데이터를 회사에 업로드하고, 그 데이터는 처리 및 분석되어 새로운 마케팅 제안을 생성합니다. 일부 파일은 200GB를 초과할 수 있습니다.

최근 회사는 일부 매장이 포함해서는 안 되는 개인 식별 정보(PII)를 포함한 파일을 업로드한 사실을 발견했습니다. 회사는 관리자가 이후에 PII가 다시 공유되면 경고를 받기를 원합니다. 또한 회사는 자동 복구(remediation)를 원합니다.

최소한의 개발 작업으로 이러한 요구사항을 충족하려면 솔루션 설계자는 무엇을 해야 합니까?

A. Amazon S3 버킷을 보안 전송 지점으로 사용합니다. 버킷의 객체를 스캔하기 위해 Amazon Inspector를 사용합니다. 객체에 PII가 포함되어 있으면 해당 PII를 포함한 객체를 제거하도록 S3 수명주기 정책을 트리거합니다.

B. Amazon S3 버킷을 보안 전송 지점으로 사용합니다. 버킷의 객체를 스캔하기 위해 Amazon Macie를 사용합니다. 객체에 PII가 포함되어 있으면 Amazon Simple Notification Service(Amazon SNS)를 사용하여 관리자가 해당 PII를 포함한 객체를 제거하도록 알립니다.

C. 사용자 지정 스캔 알고리즘을 AWS Lambda 함수로 구현합니다. 객체가 버킷에 로드될 때 함수를 트리거합니다. 객체에 PII가 포함되어 있으면 Amazon Simple Notification Service(Amazon SNS)를 사용하여 관리자가 해당 PII를 포함한 객체를 제거하도록 알립니다.

D. 사용자 지정 스캔 알고리즘을 AWS Lambda 함수로 구현합니다. 객체가 버킷에 로드될 때 함수를 트리거합니다. 객체에 PII가 포함되어 있으면 Amazon Simple Email Service(Amazon SES)를 사용하여 관리자를 알리고 PII를 포함한 객체를 제거하도록 S3 수명주기 정책을 트리거합니다.

```
A company has an application that provides marketing services to stores. The services are based on previous purchases by store customers. The stores upload transaction data to the company through SFTP, and the data is processed and analyzed to generate new marketing offers. Some of the files can exceed 200 GB in size.  
Recently, the company discovered that some of the stores have uploaded files that contain personally identifiable information (PII) that should not have been included. The company wants administrators to be alerted if PII is shared again. The company also wants to automate remediation.  
What should a solutions architect do to meet these requirements with the LEAST development effort?

- A. Use an Amazon S3 bucket as a secure transfer point. Use Amazon Inspector to scan the objects in the bucket. If objects contain PII, trigger an S3 Lifecycle policy to remove the objects that contain PII.
- B. Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.
- C. Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.
- D. Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Email Service (Amazon SES) to trigger a notification to the administrators and trigger an S3 Lifecycle policy to remove the meats that contain PII.
```

정답 : `B`

- 요구사항
	- PII 포함 시 관리자 경고
	- 자동화된 복구(또는 자동화된 조치)
	- 최소 개발 노력
	- 대용량 파일(200GB+) 처리 가능성
- Amazon Macie는 S3 객체 내 PII 및 민감 데이터 탐지에 특화된 관리형 서비스
	- 사전 정의된 PII 식별 기능을 제공하므로 자체적으로 탐지 로직 개발 필요 X -> 개발 비용 최소화
- Macie의 결과는 EventBridge/SNS 등으로 통지할 수 있어 관리자에게 즉시 알림을 보낼 수 있음
	- Macie Findings -> EventBridge -> Lambda 흐름으로 구성하면 자동화된 조치를 최소한의 개발 작업으로 구성 가능

오답 이유

- **A. S3 + Amazon Inspector + S3 수명주기 트리거 — 오답**
    - **Amazon Inspector**는 EC2/컨테이너 취약성, 이미지 보안 검사에 초점을 맞춘 서비스로 **S3 객체의 PII 스캔용 서비스가 아님**. Inspector는 S3 민감정보 검색을 제공하지 않으므로 요구사항(PII 탐지)에 부적합합니다.
    - 또한 S3 수명주기 규칙은 자동 삭제/전환을 수행하지만, 단순 수명주기 트리거로 즉시 탐지 기반 자동 격리·삭제를 구현하기 어렵습니다.    
    
- **C. Lambda로 맞춤형 스캐너 + SNS 알림 — 오답**
    - 가능하긴 하나, **대규모(200GB+) 파일을 다루는 맞춤형 PII 스캔을 직접 구현하면 개발·테스트·성능(메모리/타임아웃)·정확도 조정에 많은 노력이 필요**합니다. 또한 대용량 파일을 람다로 처리하면 시간/리소스 제한과 비용/복잡성이 증가합니다. 따라서 ‘최소 개발 노력’ 조건에 부합하지 않습니다.
    
- **D. Lambda로 맞춤형 스캐너 + SES + 수명주기 트리거 — 오답**
    - C와 동일하게 **맞춤 스캔 개발 부담이 크고**, SES는 알림(메일) 전송에는 적절하지만 자동 복구(remediation) 후속 조치(객체 격리/이동/삭제 등)를 구현하려면 추가 로직이 필요합니다. 또한 “수명주기 정책을 트리거”하여 자동 삭제하는 설계는 잘못 구성되면 민감 데이터의 영구 삭제로 이어져 감사/컴플라이언스 문제를 만들 수 있습니다.


## #47
한 회사는 다가오는 이벤트를 위해 특정 AWS 리전 내 세 개의 특정 가용 영역(Availability Zones)에서 Amazon EC2 용량을 보장받아야 합니다. 이벤트 기간은 1주일입니다.

회사가 EC2 용량을 보장받으려면 무엇을 해야 합니까?

A. 필요한 리전을 지정하는 예약 인스턴스(Reserved Instances)를 구매합니다.  
B. 필요한 리전을 지정하는 온디맨드 용량 예약(On-Demand Capacity Reservation)을 생성합니다.  
C. 필요한 리전과 세 개의 가용 영역을 지정하는 예약 인스턴스(Reserved Instances)를 구매합니다.  
D. 필요한 리전과 세 개의 가용 영역을 지정하는 온디맨드 용량 예약(On-Demand Capacity Reservation)을 생성합니다.

```
A company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a specific AWS Region for an upcoming event that will last 1 week.  
What should the company do to guarantee the EC2 capacity?

- A. Purchase Reserved Instances that specify the Region needed.
- B. Create an On-Demand Capacity Reservation that specifies the Region needed.
- C. Purchase Reserved Instances that specify the Region and three Availability Zones needed.
- D. Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed.
```

정답 : `D`

- 요구사항 : 특정 리전과 3개의 AZ에서 EC2 용량을 1주일 동안 보장
- Reserved Instances(RI) : 장기 계약 기반 요금 할인 제공, 특정 AZ/인스턴스 유형의 용량을 보장하지 않음 -> RI는 용량 예약이 아닌 요금 할인을 위한 상품
- On-Demand Capacity Reservation(CR) : 지정한 AZ, 인스턴스 유형, 용량 수량을 실제로 확보 가능하며 즉시 사용 가능
- 따라서 특정 AZ에서 용량 보장은 Capacity Reservation만 가능하며 이벤트처럼 단기 용량 확보에는 On-Demand CR이 적합

오답 이유

- **A. RI(리전 지정) 구매 — 오답**
    - RI는 용량 보장을 제공하지 않으며, AZ 내 용량 충족 여부는 보장되지 않습니다. 단순 요금 할인용입니다.
    
- **B. On-Demand Capacity Reservation(리전 지정) — 오답**
    - CR은 리전 단위가 아니라 **가용 영역(AZ) 단위로 지정**해야 용량이 보장됩니다. 단순 리전 지정만으로는 특정 AZ의 용량을 확보할 수 없습니다.
    
- **C. RI(리전 + 3개 AZ 지정) — 오답**
    - RI는 AZ를 지정할 수 있지만, 실제 용량을 확보하는 기능은 없으며 단순히 요금 할인용입니다. 따라서 단기 이벤트 기간에 맞춘 용량 보장은 불가능합니다.

## #48
한 회사의 웹사이트는 상품 카탈로그를 위해 Amazon EC2 인스턴스 스토어를 사용합니다. 회사는 카탈로그가 **고가용성(Highly Available)**을 보장하고, 카탈로그가 **내구성 있는(Durable)** 위치에 저장되도록 하고 싶습니다.

이 요구사항을 충족하려면 솔루션 설계자는 무엇을 해야 합니까?

A. 카탈로그를 Amazon ElastiCache for Redis로 이동합니다.  
B. 더 큰 인스턴스 스토어를 가진 EC2 인스턴스를 배포합니다.  
C. 카탈로그를 인스턴스 스토어에서 Amazon S3 Glacier Deep Archive로 이동합니다.  
D. 카탈로그를 Amazon Elastic File System(Amazon EFS) 파일 시스템으로 이동합니다.

```
A company's website uses an Amazon EC2 instance store for its catalog of items. The company wants to make sure that the catalog is highly available and that the catalog is stored in a durable location.  
What should a solutions architect do to meet these requirements?

- A. Move the catalog to Amazon ElastiCache for Redis.
- B. Deploy a larger EC2 instance with a larger instance store.
- C. Move the catalog from the instance store to Amazon S3 Glacier Deep Archive.
- D. Move the catalog to an Amazon Elastic File System (Amazon EFS) file system.
```

정답 : `D`

- 요구사항 : 고가용성 + 내구성 있는 저장소
- EC2 인스턴스 스토어는 인스턴스 종료 시 데이터가 사라지는 휘발성 저장소이므로 고가용성과 내구성 보장할 수 없음
- Amazon EFS는 다중 AZ에 걸쳐 자동으로 고가용성 및 내구성 제공, EC2와 쉽게 연동 가능하며 POSIX 호환 파일 시스템으로 기존 애플리케이션이 접근 가능

오답 이유

- **A. ElastiCache for Redis — 오답**
    - Redis는 메모리 기반 인메모리 데이터 스토어로 빠른 읽기/쓰기 제공.
    - 그러나 Redis는 기본적으로 내구성이 낮음(데이터 영속성은 RDB/AOF 설정 필요) → 요구된 **내구성(Durability)**을 완전히 보장하지 못함.
    
- **B. 더 큰 인스턴스 스토어 — 오답**
    - 인스턴스 스토어는 여전히 **휘발성**이므로 인스턴스 종료나 장애 발생 시 데이터 손실 가능. 고가용성/내구성 확보 불가.
    
- **C. S3 Glacier Deep Archive — 오답**
    - Glacier Deep Archive는 **극저비용 장기 보관용**으로 설계됨.
    - 즉시 액세스가 필요 없는 백업용으로 적합하며, 웹 애플리케이션에서 실시간 사용 불가 → 요구사항인 **웹사이트 카탈로그의 고가용성** 충족 불가.


## 49
한 회사는 통화 녹취 파일을 월 단위로 저장합니다.  
사용자는 통화 후 1년 동안 파일에 무작위로 액세스하지만, 1년이 지난 후에는 액세스 빈도가 낮습니다.  
회사는 1년 미만의 파일에 대해서는 사용자가 가능한 한 **빠르게 조회 및 검색**할 수 있도록 하고 싶습니다. 1년이 지난 파일을 검색할 때는 **지연이 허용**됩니다.

이 요구사항을 가장 비용 효율적으로 충족할 수 있는 솔루션은 무엇입니까?

A. 개별 파일을 Amazon S3 Glacier Instant Retrieval에 태그와 함께 저장합니다. 태그를 조회하여 S3 Glacier Instant Retrieval에서 파일을 검색합니다.  

B. 개별 파일을 Amazon S3 Intelligent-Tiering에 저장합니다. S3 수명 주기 정책을 사용하여 1년 후 파일을 S3 Glacier Flexible Retrieval로 이동합니다. S3에 있는 파일은 Amazon Athena를 사용하여 조회하고 검색합니다. S3 Glacier에 있는 파일은 S3 Glacier Select를 사용하여 조회하고 검색합니다.  

C. 개별 파일을 Amazon S3 Standard 스토리지에 태그와 함께 저장합니다. 각 아카이브에 대한 검색 메타데이터도 S3 Standard에 저장합니다. S3 수명 주기 정책을 사용하여 1년 후 파일을 S3 Glacier Instant Retrieval로 이동합니다. Amazon S3에서 메타데이터를 검색하여 파일을 조회합니다.  

D. 개별 파일을 Amazon S3 Standard 스토리지에 저장합니다. S3 수명 주기 정책을 사용하여 1년 후 파일을 S3 Glacier Deep Archive로 이동합니다. 검색 메타데이터는 Amazon RDS에 저장합니다. RDS에서 파일을 조회하고, S3 Glacier Deep Archive에서 파일을 검색합니다.

```
A company stores call transcript files on a monthly basis. Users access the files randomly within 1 year of the call, but users access the files infrequently after 1 year. The company wants to optimize its solution by giving users the ability to query and retrieve files that are less than 1-year-old as quickly as possible. A delay in retrieving older files is acceptable.  
Which solution will meet these requirements MOST cost-effectively?

- A. Store individual files with tags in Amazon S3 Glacier Instant Retrieval. Query the tags to retrieve the files from S3 Glacier Instant Retrieval.
- B. Store individual files in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the files to S3 Glacier Flexible Retrieval after 1 year. Query and retrieve the files that are in Amazon S3 by using Amazon Athena. Query and retrieve the files that are in S3 Glacier by using S3 Glacier Select.
- C. Store individual files with tags in Amazon S3 Standard storage. Store search metadata for each archive in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Instant Retrieval after 1 year. Query and retrieve the files by searching for metadata from Amazon S3.
- D. Store individual files in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Deep Archive after 1 year. Store search metadata in Amazon RDS. Query the files from Amazon RDS. Retrieve the files from S3 Glacier Deep Archive.
```

정답 : `B`

- 요구사항
	- 1년 미만 파일 : 즉시/빠른 조회 -> S3 Standard급 성능
	- 1년 이상 파일 : 저비용 아카이브 기능 -> Glacier Flexible Retrieval
	- 비용 효율성을 최적화하고 자동 스토리지 계층화 및 조회 가능성 필요
- S3 Intelligent-Tiering
	- 액세스 기반 패턴 자동 계층화 (Frequent, Infrequent Access)
	- 1년 후 Glacier 계층(Flexible Retrieval)으로 자동 이동 가능
- Athena & Glacier Select
	- Athena : S3에 있는 파일에 대해 서버리스 SQL 쿼리 수행
	- Glacier Select : Glacier 계층에서도 데이터 검색 가능

오답 이유

- **A. S3 Glacier Instant Retrieval — 오답**
    - Glacier Instant Retrieval은 빠른 조회를 제공하지만 **모든 파일이 Instant Retrieval 계층에 저장되므로 비용이 높음**
    - 1년 이상 자주 접근하지 않는 파일까지 Instant Retrieval 계층에 저장하는 것은 비용 비효율적
    
- **C. S3 Standard + 태그 + Glacier Instant Retrieval — 오답**
    - 1년 이상 파일을 Glacier Instant Retrieval로 이동 → Instant Retrieval은 **비용이 높음**
    - 메타데이터를 기반으로 검색 가능하지만, **비용 최적화 측면에서 B보다 효율적이지 않음**
    
- **D. S3 Standard + Glacier Deep Archive + RDS 메타데이터 — 오답**
    - Glacier Deep Archive는 **저렴하지만 조회 속도가 매우 느림 (시간 단위 지연)**
    - RDS에 검색 메타데이터 저장 → 추가 관리 비용 발생
    - 1년 미만 파일 조회 속도를 최적화할 수 없음 → 요구사항 불충족

## #50
한 회사는 1,000개의 Amazon EC2 Linux 인스턴스에서 프로덕션 워크로드를 실행합니다.  
이 워크로드는 서드파티 소프트웨어로 구동됩니다.  
회사는 **모든 EC2 인스턴스에서 서드파티 소프트웨어를 가능한 한 빠르게 패치**하여 중요한 보안 취약점을 해결해야 합니다.

이 요구사항을 충족하려면 솔루션 설계자는 무엇을 해야 합니까?

A. 모든 EC2 인스턴스에 패치를 적용하는 AWS Lambda 함수를 생성합니다.  
B. AWS Systems Manager Patch Manager를 구하성여 모든 EC2 인스턴스에 패치를 적용합니다.  
C. AWS Systems Manager 유지 관리 창(Maintenance Window)을 예약하여 모든 EC2 인스턴스에 패치를 적용합니다.  
D. AWS Systems Manager Run Command를 사용하여 모든 EC2 인스턴스에 패치를 적용하는 사용자 지정 명령을 실행합니다.

```
A company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The workload is powered by third-party software. The company needs to patch the third-party software on all EC2 instances as quickly as possible to remediate a critical security vulnerability.  
What should a solutions architect do to meet these requirements?

- A. Create an AWS Lambda function to apply the patch to all EC2 instances.
- B. Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.
- C. Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.
- D. Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances.
```

정답 : `B`

- 요구사항 : 100대의 EC2 인스턴스에서 서드파티 소프트웨어 패치를 가능한 한 빠르게 적용
- Patch Manager
	- EC2 인스턴스 및 온프레미스 서버에 대한 패치 관리 자동화
	- OS 및 서드파티 소프트웨어 패치 가능
	- 대규모 환경에서 빠르고 안전하게 패치를 적용 가능

오답 이유

- **A. Lambda — 오답**
    - Lambda는 인스턴스 관리 자동화에 사용할 수 있지만, **1,000대 EC2에 설치된 서드파티 소프트웨어를 패치하는 것은 관리 및 실행에 복잡**
    - 상태 관리, 오류 재시도, 동시 실행 제한 등 고려 필요 → 비효율적
    
- **C. Maintenance Window — 오답**
    - Maintenance Window는 Patch Manager 또는 Run Command와 결합해 사용 가능
    - 단독으로는 **패치 적용을 자동화하지 않으며, 스케줄 기반으로만 실행 가능**
    - 긴급 패치 시 즉시 적용에는 적합하지 않음
    
- **D. Run Command — 오답**
    - Run Command는 개별 명령 실행 가능, 동시 실행 제어 가능
    - 하지만 **패치 관리, 재시도, 누락 관리 등 자동화 기능 부족** → 대규모 환경에는 효율적이지 않음