---
created: 2025-09-24 13:56:28
last_modified: 2025-09-24 18:34:30
---
## #31
```
한 회사가 AWS에서 웹 애플리케이션을 호스팅하고 있으며, 모든 Amazon EC2 인스턴스, Amazon RDS DB 인스턴스 및 Amazon Redshift 클러스터가 태그로 구성되었는지 확인하고자 합니다. 회사는 이 검사를 구성하고 운영하는 데 드는 노력을 최소화하고자 합니다.
솔루션 아키텍트는 이 목표를 어떻게 달성해야 합니까?

A. AWS Config 규칙을 사용하여 올바르게 태그되지 않은 리소스를 정의하고 감지합니다.  
B. Cost Explorer를 사용하여 올바르게 태그되지 않은 리소스를 표시하고, 해당 리소스를 수동으로 태그합니다.  
C. 모든 리소스를 확인하여 적절한 태그 할당을 확인하는 API 호출을 작성하고, EC2 인스턴스에서 주기적으로 코드를 실행합니다.  
D. 모든 리소스를 확인하여 적절한 태그 할당을 확인하는 API 호출을 작성하고, Amazon CloudWatch를 통해 Lambda 함수를 예약하여 주기적으로 코드를 실행합니다.

---

A company that hosts its web application on AWS wants to ensure all Amazon EC2 instances. Amazon RDS DB instances. and Amazon Redshift clusters are configured with tags. The company wants to minimize the effort of configuring and operating this check.  
What should a solutions architect do to accomplish this?

- A. Use AWS Config rules to define and detect resources that are not properly tagged.
- B. Use Cost Explorer to display resources that are not properly tagged. Tag those resources manually.
- C. Write API calls to check all resources for proper tag allocation. Periodically run the code on an EC2 instance.
- D. Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda function through Amazon CloudWatch to periodically run the code.
```

정답 : `A`

- AWS Config는 AWS 리소스의 구성 상태를 지속적으로 평가하고 규정 준수 여부를 자동으로 확인 가능
- 태그 규칙을 생성하면 EC2, RDS, Redshift 등 지정된 리소스에서 태그가 올바르게 적용되지 않은 경우 자동으로 감지 가능
- 수동 검사나 코드 작성 없이도 지속적으로 모니터링이 가능하므로 운영 부담 최소화

오답 이유
**B - Cost Explorer**
- 비용 분석 도구일 뿐, 태그 규정 준수 여부를 지속적으로 모니터링하거나 자동으로 감지하지 못합니다.
- 수동으로 태그를 설정해야 하므로 운영 부담이 큽니다.

**C - API 호출 + EC2 실행**
- 직접 코드 작성과 EC2 인스턴스 운영이 필요하여 관리 오버헤드가 높습니다.
- 자동화 및 지속적인 모니터링 측면에서 비효율적입니다.

**D - API 호출 + Lambda 실행**
- C와 달리 Lambda를 사용해 주기적 자동 실행 가능하지만, 규칙 기반 모니터링을 제공하는 Config보다 구현 및 유지보수가 더 복잡합니다.
- AWS Config를 사용하면 별도 코드 작성 없이 바로 규정 준수 검사가 가능하므로 최소 노력 목표에는 부적합합니다.


## #32
```
한 개발팀이 다른 팀이 접근할 수 있는 웹사이트를 호스팅해야 합니다. 웹사이트 콘텐츠는 HTML, CSS, 클라이언트 측 JavaScript, 이미지로 구성되어 있습니다.  
이 웹사이트를 호스팅하는 가장 비용 효율적인 방법은 무엇입니까?

A. 웹사이트를 컨테이너화하여 AWS Fargate에서 호스팅합니다.  
B. Amazon S3 버킷을 생성하고 웹사이트를 그곳에서 호스팅합니다.  
C. Amazon EC2 인스턴스에서 웹 서버를 배포하여 웹사이트를 호스팅합니다.  
D. Express.js 프레임워크를 사용하는 AWS Lambda 대상을 가진 Application Load Balancer(ALB)를 구성합니다.

---

A development team needs to host a website that will be accessed by other teams. The website contents consist of HTML, CSS, client-side JavaScript, and images.  
Which method is the MOST cost-effective for hosting the website?

- A. Containerize the website and host it in AWS Fargate.
- B. Create an Amazon S3 bucket and host the website there.
- C. Deploy a web server on an Amazon EC2 instance to host the website.
- D. Configure an Application Load Balancer with an AWS Lambda target that uses the Express.js framework.
```

정답 : `B`

- S3는 정적 콘텐츠를 제공하는데 최적화되어 있으며 서버나 컨테이너 관리가 필요 없음
- 사용한 스토리지 및 요청 수에 대해서만 비용이 발생하므로 가장 비용 효율적

오답 이유

**A - Fargate**
- Fargate는 컨테이너 기반 애플리케이션 실행에 적합하지만, 단순 정적 콘텐츠를 위해 컨테이너를 운영하는 것은 과도합니다.
- 비용도 S3에 비해 훨씬 비쌉니다.

**C - EC2**
- EC2 인스턴스에 웹 서버(Apache, Nginx 등)를 설치해 정적 콘텐츠를 호스팅할 수 있지만, 인스턴스 관리와 유지보수 필요.
- 항상 실행되는 EC2 비용이 발생해 비효율적입니다.

**D - ALB + Lambda**
- 동적 API 처리에는 유용할 수 있지만, 정적 콘텐츠를 위해 ALB와 Lambda를 사용하는 것은 불필요하게 복잡하고 비용이 더 많이 듭니다.
- S3가 훨씬 간단하고 저렴합니다.


## #33
```
한 회사가 AWS에서 온라인 마켓플레이스 웹 애플리케이션을 운영하고 있습니다. 이 애플리케이션은 피크 시간 동안 수십만 명의 사용자에게 서비스를 제공합니다. 회사는 수백만 건의 금융 거래 세부 정보를 여러 내부 애플리케이션과 공유하기 위한 확장 가능하고 거의 실시간에 가까운 솔루션이 필요합니다. 또한 거래는 문서 데이터베이스에 저장되기 전에 민감한 데이터를 제거하도록 처리되어야 하며, 문서 데이터베이스는 저지연 검색을 지원해야 합니다.  
솔루션 설계자가 이러한 요구 사항을 충족하기 위해 무엇을 권장해야 합니까?

- A. 거래 데이터를 Amazon DynamoDB에 저장합니다. 쓰기 시 각 거래에서 민감한 데이터를 제거하는 규칙을 DynamoDB에 설정합니다. DynamoDB Streams를 사용하여 거래 데이터를 다른 애플리케이션과 공유합니다.
- B. 거래 데이터를 Amazon Kinesis Data Firehose로 스트리밍하여 데이터를 Amazon DynamoDB 및 Amazon S3에 저장합니다. Kinesis Data Firehose와 AWS Lambda 통합을 사용하여 민감한 데이터를 제거합니다. 다른 애플리케이션은 Amazon S3에 저장된 데이터를 소비할 수 있습니다.
- C. 거래 데이터를 Amazon Kinesis Data Streams로 스트리밍합니다. AWS Lambda 통합을 사용하여 각 거래에서 민감한 데이터를 제거한 후 거래 데이터를 Amazon DynamoDB에 저장합니다. 다른 애플리케이션은 Kinesis 데이터 스트림에서 거래 데이터를 소비할 수 있습니다.
- D. 배치된 거래 데이터를 파일 형태로 Amazon S3에 저장합니다. AWS Lambda를 사용하여 각 파일을 처리하고 민감한 데이터를 제거한 후 Amazon S3의 파일을 업데이트합니다. 그런 다음 Lambda 함수가 데이터를 Amazon DynamoDB에 저장합니다. 다른 애플리케이션은 Amazon S3에 저장된 거래 파일을 소비할 수 있습니다.

---

A company runs an online marketplace web application on AWS. The application serves hundreds of thousands of users during peak hours. The company needs a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. Transactions also need to be processed to remove sensitive data before being stored in a document database for low-latency retrieval.  
What should a solutions architect recommend to meet these requirements?

- A. Store the transactions data into Amazon DynamoDB. Set up a rule in DynamoDB to remove sensitive data from every transaction upon write. Use DynamoDB Streams to share the transactions data with other applications.
- B. Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3.
- C. Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store the transactions data in Amazon DynamoDB. Other applications can consume the transactions data off the Kinesis data stream.
- D. Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and remove sensitive data before updating the files in Amazon S3. The Lambda function then stores the data in Amazon DynamoDB. Other applications can consume transaction files stored in Amazon S3.
```

정답 : `C`

- 요구사항 : 수백만 검의 금융 거래를 확장 가능하고 실시간에 가깝게 처리
- Kinesis Data Streams는 대규모 이벤트 스트리밍과 실시간 처리에 최적화
- 람다를 통합해 민감한 데이터를 제거하는 ETL(Extract, Transform, Load)
- 정제된 데이터를 DynamoDB(문서 기반 데이터베이스)로 저장하면 저지연 검색 가능
- 동시에 Kinesis Data Streams는 여러 소비자를 지원하므로 다른 내부 애플리케이션이 거래 데이터를 직접 스트림에서 소비 가능 -> 확장성, 실시간 처리, 다중 소비자 요구사항 충족

오답 이유

- **A. DynamoDB + DynamoDB Streams**
    - DynamoDB는 쓰기 시점에 “규칙”을 적용해 데이터 필터링하는 기능이 없습니다.
    - Streams는 변경 사항을 스트리밍할 수 있지만, 민감 데이터 제거를 사전에 보장하지 못합니다.
    - 따라서 민감 데이터 제거 요건을 만족하지 않습니다.

- **B. Kinesis Data Firehose**
    - Firehose는 데이터를 최종 목적지(S3, Redshift, OpenSearch 등)에 전송하는 데 적합하며, **DynamoDB를 직접 대상으로 지원하지 않습니다.**
    - Firehose는 거의 실시간이지만 최소 60초 버퍼링 후 전송되므로 **거의 실시간 처리 요구사항**에는 적합하지 않습니다.
    - 또한 다른 애플리케이션이 데이터를 실시간으로 소비할 수 없고, 저장된 S3 파일만 접근 가능합니다.
    
- **D. Amazon S3 + Lambda**
    - 배치(batch) 기반 처리 방식으로 실시간 성능을 만족하지 못합니다.
    - S3에 저장 후 처리하면 민감 데이터 제거가 늦게 적용되고, 내부 애플리케이션도 즉시 접근 불가합니다.
    - 실시간 요구사항과 다중 소비자 요구사항을 충족하지 않습니다.


### #34
```
한 회사가 AWS에서 다계층(Multi-tier) 애플리케이션을 호스팅하고 있습니다.  
컴플라이언스, 거버넌스, 감사(auditing), 보안을 위해, 회사는 AWS 리소스의 구성(Configuration) 변경 사항을 추적하고, 이러한 리소스에 대한 API 호출 기록을 저장해야 합니다.  

솔루션 설계자가 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?

A. AWS CloudTrail을 사용하여 구성 변경 사항을 추적하고 AWS Config를 사용하여 API 호출을 기록합니다.  
B. AWS Config를 사용하여 구성 변경 사항을 추적하고 AWS CloudTrail을 사용하여 API 호출을 기록합니다.  
C. AWS Config를 사용하여 구성 변경 사항을 추적하고 Amazon CloudWatch를 사용하여 API 호출을 기록합니다.  
D. AWS CloudTrail을 사용하여 구성 변경 사항을 추적하고 Amazon CloudWatch를 사용하여 API 호출을 기록합니다.

---

A company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and security, the company must track configuration changes on its AWS resources and record a history of API calls made to these resources.  
What should a solutions architect do to meet these requirements?

- A. Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.
- B. Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.
- C. Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.
- D. Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls.
```

정답 : `B`

- AWS Config: AWS 리소스의 구성 상태를 추적하고 변경 이력을 기록하며, 규정 준수 감사에 필요한 리소스 변경 기록 제공 -> 리소스 설정 변경을 감사 및 추적할 때 적합
- AWS CloudTrail: AWS 계정 내에서 수행된 모든 API 호출을 기록, 이벤트 히스토리를 제공해 보안, 감사 운영 및 문제 해결에 사용

오답 이유

- **A. CloudTrail → 구성 변경, Config → API 호출**
    - CloudTrail은 리소스 상태 변경 자체를 추적하는 것이 아니라 API 호출 이벤트를 기록합니다.
    - Config가 API 호출 기록 기능을 제공하지 않음.
    - 따라서 요구사항과 정확히 일치하지 않습니다.
    
- **C. Config → 구성 변경, CloudWatch → API 호출**
    - CloudWatch는 주로 모니터링 및 로그 수집(애플리케이션 로그, 메트릭)에 사용됩니다.
    - AWS API 호출 이벤트 기록용으로는 CloudTrail이 필요합니다.
    - 따라서 API 호출 기록 요구사항 충족 불가.
    
- **D. CloudTrail → 구성 변경, CloudWatch → API 호출**
    - CloudTrail은 구성 변경 추적이 아니라 **API 호출 기록**에 적합합니다.
    - CloudWatch는 API 호출 기록용으로 적합하지 않음.
    - 따라서 요구사항을 만족하지 않습니다.

## #35
```
한 회사가 AWS 클라우드에서 공개 웹 애플리케이션을 출시할 준비를 하고 있습니다.  
아키텍처는 VPC 내 Amazon EC2 인스턴스와 그 뒤의 Elastic Load Balancer(ELB)로 구성되어 있습니다.  
DNS는 서드파티 서비스를 사용합니다.  
회사의 솔루션 설계자는 대규모 DDoS 공격을 감지하고 방어할 수 있는 솔루션을 권장해야 합니다.  

어떤 솔루션이 이러한 요구사항을 충족합니까?

A. 계정에서 Amazon GuardDuty를 활성화합니다.  
B. EC2 인스턴스에서 Amazon Inspector를 활성화합니다.  
C. AWS Shield를 활성화하고 Amazon Route 53에 연결합니다.  
D. AWS Shield Advanced를 활성화하고 ELB에 연결합니다.

---

A company is preparing to launch a public-facing web application in the AWS Cloud. The architecture consists of Amazon EC2 instances within a VPC behind an Elastic Load Balancer (ELB). A third-party service is used for the DNS. The company's solutions architect must recommend a solution to detect and protect against large-scale DDoS attacks.  
Which solution meets these requirements?

- A. Enable Amazon GuardDuty on the account.
- B. Enable Amazon Inspector on the EC2 instances.
- C. Enable AWS Shield and assign Amazon Route 53 to it.
- D. Enable AWS Shield Advanced and assign the ELB to it.
```

정답 : `D`

- 요구사항 : 대규모 DDoS 공격 감지 및 방어
- AWS Shield Standard는 모든 AWS 고객에게 기본 제공되지만, 규모가 큰 공격에 대한 고급 보호, 대응 및 알림 기능이 필요한 경우 Shield Advanced를 사용
- Shield Advanced는 ELB, CloudFront, Global Accelerator 등에 연결해 공격 감지, 완화, 금융 보호 및 24/7 DDoS 대응 팀(DRT) 지원 제공
- 현재 아키텍처에서는 ELB가 인터넷 트래픽의 진입점이므로 ELB에 Shield Advanced를 연결해야 효과적으로 보호 가능

오답 이유

- **A. Amazon GuardDuty**
    - GuardDuty는 악성 활동, 계정 침해, 의심스러운 API 호출 등을 탐지하는 **위협 탐지 서비스**입니다.        
    - DDoS 공격 방어 기능은 제공하지 않음. → 요구사항 불충족.

- **B. Amazon Inspector**
    - Inspector는 EC2 및 컨테이너의 **취약점 평가 및 보안 검사 서비스**입니다.
    - 실시간 DDoS 방어와는 관련 없음. → 요구사항 불충족.
    
- **C. AWS Shield + Route 53**
    - Shield Standard는 기본 보호만 제공, Advanced 수준의 **대규모 공격 완화, 알림, 비용 보호**는 제공되지 않습니다.
    - 또한, DNS는 서드파티를 사용하고 있으므로 Route 53 연결이 불필요하며, 보호 대상이 ELB여야 함. → 요구사항 불충족.


## #36
```
한 회사가 AWS 클라우드에서 애플리케이션을 구축하고 있습니다.  
이 애플리케이션은 두 개의 AWS 리전의 Amazon S3 버킷에 데이터를 저장할 예정입니다.  
회사는 모든 S3 버킷에 저장되는 데이터를 암호화하기 위해 **AWS Key Management Service(AWS KMS) 고객 관리 키(CMK)**를 사용해야 합니다.  
두 S3 버킷의 데이터는 **같은 KMS 키**로 암호화 및 복호화되어야 합니다.  
데이터와 키는 두 리전 각각에 저장되어야 합니다.  

운영 부담을 최소화하면서 이 요구사항을 충족할 수 있는 솔루션은 무엇입니까?

A. 각 리전에 S3 버킷을 생성합니다. S3 버킷을 **Amazon S3 관리 암호화 키(SSE-S3)**로 서버 측 암호화를 구성합니다. S3 버킷 간 복제를 구성합니다.  
B. **멀티 리전 고객 관리 KMS 키**를 생성합니다. 각 리전에 S3 버킷을 생성합니다. S3 버킷 간 복제를 구성합니다. 애플리케이션이 **클라이언트 측 암호화**로 KMS 키를 사용하도록 구성합니다.  
C. 고객 관리 KMS 키를 생성하고 각 리전에 S3 버킷을 생성합니다. S3 버킷을 **Amazon S3 관리 암호화 키(SSE-S3)**로 서버 측 암호화를 구성합니다. S3 버킷 간 복제를 구성합니다.  
D. 고객 관리 KMS 키를 생성하고 각 리전에 S3 버킷을 생성합니다. S3 버킷을 **AWS KMS 키(SSE-KMS)**로 서버 측 암호화를 구성합니다. S3 버킷 간 복제를 구성합니다.

---

A company is building an application in the AWS Cloud. The application will store data in Amazon S3 buckets in two AWS Regions. The company must use an AWS Key Management Service (AWS KMS) customer managed key to encrypt all data that is stored in the S3 buckets. The data in both S3 buckets must be encrypted and decrypted with the same KMS key. The data and the key must be stored in each of the two Regions.  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure replication between the S3 buckets.
- B. Create a customer managed multi-Region KMS key. Create an S3 bucket in each Region. Configure replication between the S3 buckets. Configure the application to use the KMS key with client-side encryption.
- C. Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure replication between the S3 buckets.
- D. Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with AWS KMS keys (SSE-KMS). Configure replication between the S3 buckets.
```

정답 : `B`

- 요구사항
	- 두 리전의 S3 버킷에 저장되는 모든 데이터는 같은 KMS 키로 암호화/복호화해야함
	- 데이터와 키는 두 리전에 모두 존재해야 함
	- 운영 부담 최소화
- 멀티 리전 KMS 키를 사용하면 단일 키 ID로 여러 리전에서 동일한 키 참조 가능
- 리전별 KMS 키 동기화 필요 없음
- S3 버킷 간 복제 시 키 재매핑이나 추가 작업 필요 없음
- 애플리케이션에서 클라이언트 측 암호화(CSE)를 사용하면 동일한 멀티 리전 키로 데이터를 암호화하고 복호화 가능

오답 이유

- **A. SSE-S3 사용**
    - SSE-S3는 AWS가 관리하는 키를 사용하므로 **고객 관리 KMS 키 요구사항을 충족하지 못함**.
    - 요구사항: 고객 관리 KMS 키 사용 → 불충분.
    
- **C. SSE-S3 + 고객 KMS 키**
    - 설명상 모순: SSE-S3는 S3 관리 키 사용, 고객 KMS 키는 미사용.
    - 또한, 같은 키로 두 리전을 암호화하려면 멀티 리전 CMK가 필요함.
    - 요구사항 충족 불가.
    
- **D. SSE-KMS + 단일 리전 KMS 키**
    - SSE-KMS는 고객 관리 키 사용 가능하지만, **단일 리전 KMS 키**는 다른 리전 S3 버킷 복제 시 사용 불가.
    - 리전별 키 동기화 필요 → 운영 부담 증가.


## #37
```
한 회사가 최근 AWS 계정에서 Amazon EC2 인스턴스에 다양한 신규 워크로드를 출시했습니다.  
회사는 인스턴스에 원격으로 안전하게 접근하고 관리할 수 있는 전략을 수립해야 합니다.  
또한, **AWS 네이티브 서비스**를 사용하고 **AWS Well-Architected Framework**를 준수하며 반복 가능한 프로세스를 구현해야 합니다.  

이 요구사항을 최소한의 운영 부담으로 충족할 수 있는 솔루션은 무엇입니까?

A. EC2 직렬 콘솔을 사용하여 각 인스턴스의 터미널 인터페이스에 직접 접근합니다.  
B. 각 기존 인스턴스와 신규 인스턴스에 적절한 IAM 역할을 연결합니다. AWS Systems Manager **Session Manager**를 사용하여 원격 SSH 세션을 설정합니다.  
C. 관리용 SSH 키 페어를 생성합니다. 각 EC2 인스턴스에 공개 키를 로드합니다. 퍼블릭 서브넷에 배스천 호스트를 배포하여 각 인스턴스의 관리용 터널을 제공합니다.  
D. AWS Site-to-Site VPN 연결을 설정합니다. 관리자는 온프레미스 로컬 머신에서 VPN 터널을 통해 SSH 키를 사용하여 인스턴스에 직접 연결하도록 안내합니다.

---

A company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The company needs to create a strategy to access and administer the instances remotely and securely. The company needs to implement a repeatable process that works with native AWS services and follows the AWS Well-Architected Framework.  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Use the EC2 serial console to directly access the terminal interface of each instance for administration.
- B. Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager Session Manager to establish a remote SSH session.
- C. Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy a bastion host in a public subnet to provide a tunnel for administration of each instance.
- D. Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local on-premises machines to connect directly to the instances by using SSH keys across the VPN tunnel.
```

정답 : `B`

- 요구사항
	- 원격으로 안전하게 접근 및 관리
	- 반복 가능한 프로세스
	- AWS 네이티브 서비스 사용
	- 최소 운영 부담
- AWS System Manager (SSM) Session Manager
	- SSH 키 없이 안전한게 EC2에 접근 가능
	- IAM 역할 기반 권한 제어 -> 계정별 관리 가능
	- CloudTrail 및 CloudWatch 통합 -> 감사 가능
	- 인터넷 접근이 필요 없으므로 보안 강화
	- 새로운 인스턴스에도 동일한 IAM 역할을 붙이면 반복 가능

오답 이유

- **A. EC2 직렬 콘솔**
    - 직렬 콘솔은 제한적인 상황에서만 사용 가능하며, 모든 운영 요구사항을 반복적으로 처리하기 어렵습니다.
    - SSH나 원격 관리 자동화에 적합하지 않음. → 운영 부담 증가.
    
- **C. SSH 키 + 배스천 호스트**
    - 전통적인 접근 방식, 배스천 호스트 운영 필요 → 유지관리 부담 높음.
    - 키 관리, 포트 관리, 보안 업데이트 등 반복 작업 필요.        
    - AWS 네이티브 서비스 기반 자동화가 아님.

- **D. Site-to-Site VPN + SSH 키**
    - VPN 구축과 관리 부담 존재, 온프레미스 의존.
    - 모든 관리자가 로컬 머신에서 접속해야 하며 반복적인 자동화 어렵고, Well-Architected Framework에서 권장하는 클라우드 네이티브 접근 아님.


## #38
```
한 회사가 Amazon S3에서 정적 웹사이트를 호스팅하고 있으며, Amazon Route 53을 DNS로 사용하고 있습니다.  
웹사이트에 전 세계적으로 증가하는 트래픽이 발생하고 있습니다.  
회사는 웹사이트에 접근하는 사용자에 대한 **지연 시간(latency)을 줄여야** 합니다.  

어떤 솔루션이 이러한 요구사항을 **가장 비용 효율적으로** 충족합니까?

A. 웹사이트가 저장된 S3 버킷을 모든 AWS 리전으로 복제합니다. Route 53 지리적 위치(Geolocation) 라우팅 항목을 추가합니다.  
B. AWS Global Accelerator에서 가속기를 프로비저닝합니다. 제공된 IP 주소를 S3 버킷과 연결합니다. Route 53 항목을 가속기의 IP 주소로 변경합니다.  
C. Amazon CloudFront 배포를 S3 버킷 앞에 추가합니다. Route 53 항목을 CloudFront 배포로 변경합니다.  
D. S3 Transfer Acceleration을 버킷에서 활성화합니다. Route 53 항목을 새로운 엔드포인트로 변경합니다.

---

A company is hosting a static website on Amazon S3 and is using Amazon Route 53 for DNS. The website is experiencing increased demand from around the world. The company must decrease latency for users who access the website.  
Which solution meets these requirements MOST cost-effectively?

- A. Replicate the S3 bucket that contains the website to all AWS Regions. Add Route 53 geolocation routing entries.
- B. Provision accelerators in AWS Global Accelerator. Associate the supplied IP addresses with the S3 bucket. Edit the Route 53 entries to point to the IP addresses of the accelerators.
- C. Add an Amazon CloudFront distribution in front of the S3 bucket. Edit the Route 53 entries to point to the CloudFront distribution.
- D. Enable S3 Transfer Acceleration on the bucket. Edit the Route 53 entries to point to the new endpoint.
```

정답 : `C`

- 요구사항
	- 전 세계 사용자에 대한 지연 시간 감소
	- 비용 효율적 솔루션
- CloudFront
	- 전 세계에 배포된 엣지 로케이션을 통해 콘텐츠 캐싱
	- 사용자가 가장 가까운 엣지 위치에서 콘텐츠를 제공 -> 지연 시간 최소화
	- S3 정적 웹사이트와 쉽게 통합 가능
	- 다른 옵션(Global Accelerator, S3 Transfer Acceleration, 리전 복제)에 비해 비용 효율적

오답 이유

- **A. S3 버킷을 모든 리전으로 복제 + Route 53 지리적 라우팅**
    - 리전별 복제 비용 및 관리 부담 발생.
    - 운영 복잡성 높고 비용 증가. → 비용 효율적 아님.
    
- **B. AWS Global Accelerator + IP 연결**
    - Global Accelerator는 주로 TCP/UDP 애플리케이션 가속용, 정적 웹사이트를 위한 최적화 아님.
    - 비용이 높고, S3 웹사이트와 직접 연결하려면 복잡한 설정 필요. → 비용 효율성 낮음.
    
- **D. S3 Transfer Acceleration**
    - S3 엔드포인트와 사용자의 네트워크 간 업로드 가속에는 적합.
    - 정적 웹사이트 다운로드/지연 시간 감소에는 CloudFront만큼 효율적이지 않음. → 비용 대비 성능 낮음.


## #39
```
한 회사가 웹사이트에서 항목을 검색할 수 있는 저장소를 운영하고 있습니다.  
데이터는 Amazon RDS for MySQL 데이터베이스 테이블에 저장되어 있으며, 1천만 개 이상의 행이 있습니다.  
데이터베이스는 2TB의 General Purpose SSD 스토리지를 사용하고 있습니다.  
회사의 웹사이트를 통해 매일 수백만 건의 데이터 업데이트가 발생합니다.  

회사는 일부 삽입(insert) 작업이 10초 이상 걸리는 것을 확인했습니다.  
분석 결과, **데이터베이스 스토리지 성능**이 문제임을 확인했습니다.  

이 성능 문제를 해결할 수 있는 솔루션은 무엇입니까?

A. 스토리지 유형을 **Provisioned IOPS SSD**로 변경합니다.  
B. DB 인스턴스를 **메모리 최적화 인스턴스 클래스**로 변경합니다.  
C. DB 인스턴스를 **버스트 가능한 성능(Burstable) 인스턴스 클래스**로 변경합니다.  
D. MySQL 네이티브 비동기 복제를 사용하여 **Multi-AZ RDS 읽기 전용 리플리카**를 활성화합니다.

---

A company maintains a searchable repository of items on its website. The data is stored in an Amazon RDS for MySQL database table that contains more than 10 million rows. The database has 2 TB of General Purpose SSD storage. There are millions of updates against this data every day through the company's website.  
The company has noticed that some insert operations are taking 10 seconds or longer. The company has determined that the database storage performance is the problem.  
Which solution addresses this performance issue?

- A. Change the storage type to Provisioned IOPS SSD.
- B. Change the DB instance to a memory optimized instance class.
- C. Change the DB instance to a burstable performance instance class.
- D. Enable Multi-AZ RDS read replicas with MySQL native asynchronous replication.
```

정답 : `A`

- 문제의 원인 : 스토리지 성능
- General Purpose SSD(gp2)는 기본 IOPS 성능이 용량에 따라 제한되며 대규모 업데이트가 많은 워크로드에서는 I/O 병목 발생 가능
- Provisioned IOPS SSD(io1/io2)는 원하는 IOPS를 직접 설정할 수 있어 일관된 고성능 I/O 제공
- 따라서 삽입/갱신 성능이 개선되고 수백만 건의 트랜잭션 처리에 적합
- 

오답 이유

- **B. 메모리 최적화 인스턴스로 변경**
    - 메모리 최적화 인스턴스는 캐시 활용 및 쿼리 성능에 유리하지만, 문제는 **스토리지 I/O 병목**입니다.
    - 메모리 확장만으로 삽입 성능 문제 해결 불가.
    
- **C. 버스트 가능한 성능 인스턴스로 변경**
    - Burstable 인스턴스(T3/T4)는 CPU 성능에 제한적.
    - 스토리지 I/O 문제에는 도움 되지 않음.
    
- **D. Multi-AZ 읽기 전용 리플리카 활성화**
    - 읽기 성능 향상에 적합하지만, **삽입/업데이트는 주 인스턴스에서 처리**됩니다.
    - 따라서 삽입 성능 문제는 해결되지 않음.


## #40
```
한 회사는 수천 개의 엣지 디바이스를 운영하며, 이 디바이스들이 매일 1TB의 상태 알림(status alerts)을 생성합니다.  
각 알림의 크기는 약 2KB입니다.  

솔루션 설계자는 알림을 수집하고 향후 분석을 위해 저장할 수 있는 솔루션을 구현해야 합니다.  

요구사항:  
- 고가용성 솔루션  
- 비용 최소화  
- 추가 인프라 관리 불필요  
- 데이터는 **14일간 즉시 분석 가능**  
- 14일이 지난 데이터는 **아카이브**  

이 요구사항을 가장 운영 효율적으로 충족하는 솔루션은 무엇입니까?

A. Amazon Kinesis Data Firehose 배포 스트림을 만들어 알림을 수집합니다. Kinesis Data Firehose 스트림이 알림을 Amazon S3 버킷으로 전달하도록 구성합니다. S3 수명주기(Lifecycle) 구성을 설정하여 데이터를 14일 후 Amazon S3 Glacier로 이전합니다.  

B. 두 개의 가용 영역(AZ)에 Amazon EC2 인스턴스를 배포하고, Elastic Load Balancer 뒤에 배치하여 알림을 수집합니다. EC2 인스턴스에서 알림을 Amazon S3 버킷에 저장하는 스크립트를 작성합니다. S3 수명주기 구성을 설정하여 14일 후 데이터를 Amazon S3 Glacier로 이전합니다.  

C. Amazon Kinesis Data Firehose 배포 스트림을 만들어 알림을 수집합니다. Kinesis Data Firehose 스트림이 알림을 Amazon OpenSearch Service(이전 Amazon Elasticsearch Service) 클러스터로 전달하도록 구성합니다. OpenSearch Service 클러스터에서 매일 수동 스냅샷을 생성하고 14일 이상 된 데이터를 삭제하도록 설정합니다.  

D. Amazon SQS 표준 큐를 만들어 알림을 수집하고, 메시지 보존 기간을 14일로 설정합니다. 소비자가 SQS 큐를 폴링하고 메시지의 나이를 확인하며 필요한 경우 메시지 데이터를 분석합니다. 메시지가 14일 이상이면 소비자가 메시지를 Amazon S3 버킷으로 복사하고 SQS 큐에서 삭제합니다.

---

A company has thousands of edge devices that collectively generate 1 TB of status alerts each day. Each alert is approximately 2 KB in size. A solutions architect needs to implement a solution to ingest and store the alerts for future analysis.  
The company wants a highly available solution. However, the company needs to minimize costs and does not want to manage additional infrastructure. Additionally, the company wants to keep 14 days of data available for immediate analysis and archive any data older than 14 days.  
What is the MOST operationally efficient solution that meets these requirements?

- A. Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.
- B. Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load Balancer to ingest the alerts. Create a script on the EC2 instances that will store the alerts in an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.
- C. Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. Set up the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster to take manual snapshots every day and delete data from the cluster that is older than 14 days.
- D. Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts, and set the message retention period to 14 days. Configure consumers to poll the SQS queue, check the age of the message, and analyze the message data as needed. If the message is 14 days old, the consumer should copy the message to an Amazon S3 bucket and delete the message from the SQS queue.
```

정답 : `A`

- 요구사항: 고가용성 / 운영 부담 최소화(추가 인프라 관리 불필요) / 비용 효율적 / 데이터 14일간 즉시 분석 가능 / 14일 이후 데이터 아카이브
- Kinesis Data Firehose + S3
	- 완전 관리형 서버리스 서비스 -> 인프라 관리 불필요
	- S3에 데이터 저장, 라이프사이클 정책으로 14일 후 Glacier로 자동 아카이브 가능
	- Firehose 데이터 버퍼링 및 배치 전송으로 높은 데이터 처리량 지원(1TB/일 가능)
	- 비용 효율적이며 고가용성 확보 가능

오답 이유

- **B. EC2 인스턴스 + ELB + 스크립트**
    - 인프라 관리 필요 (OS, 패치, 모니터링, Auto Scaling)
    - 운영 부담 높음 → 요구사항 불충족
    
- **C. Firehose → OpenSearch Service**
    - OpenSearch는 데이터 검색에 적합하지만, **1TB/일** 수준의 대량 데이터 장기 저장 시 비용 높음
    - 수동 스냅샷 관리 필요 → 운영 부담 증가
    
- **D. SQS 큐 + 소비자 스크립트**
    - 메시지 14일 보관 후 아카이브를 자동화하려면 **소비자 스크립트 필요**
    - 메시지 1TB/일 수준이면 SQS 메시지 수 제한 및 비용 문제 발생
    - 운영 효율성이 떨어짐