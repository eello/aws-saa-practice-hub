---
created: 2025-10-03 14:51:36
last_modified: 2025-10-04 10:57:41
---
## #341
한 회사는 AWS Lake Formation으로 거버넌스되는 Amazon S3 데이터 레이크를 보유하고 있습니다. 회사는 Amazon QuickSight에서 시각화를 생성하려고 하며, 데이터 레이크의 데이터와 Amazon Aurora MySQL 데이터베이스에 저장된 운영 데이터를 조인하려고 합니다. 회사는 마케팅 팀이 데이터베이스의 열 하위 집합에만 접근할 수 있도록 열 수준 권한 부여를 강제하려고 합니다.

최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. Amazon EMR을 사용하여 데이터베이스에서 QuickSight SPICE 엔진으로 데이터를 직접 수집합니다. 필요한 열만 포함합니다.
B. AWS Glue Studio를 사용하여 데이터베이스에서 S3 데이터 레이크로 데이터를 수집합니다. 열 수준 액세스 제어를 강제하기 위해 QuickSight 사용자에게 IAM 정책을 연결합니다. QuickSight에서 데이터 소스로 Amazon S3를 사용합니다.
C. AWS Glue Elastic Views를 사용하여 Amazon S3에 대한 데이터베이스의 물리화 뷰를 생성합니다. QuickSight 사용자에 대한 열 수준 액세스 제어를 강제하기 위해 S3 버킷 정책을 생성합니다. QuickSight에서 데이터 소스로 Amazon S3를 사용합니다.
D. Lake Formation 블루프린트를 사용하여 데이터베이스에서 S3 데이터 레이크로 데이터를 수집합니다. QuickSight 사용자에 대한 열 수준 액세스 제어는 Lake Formation을 사용해 강제합니다. QuickSight의 데이터 소스로 Amazon Athena를 사용합니다.

```
A company has an Amazon S3 data lake that is governed by AWS Lake Formation. The company wants to create a visualization in Amazon QuickSight by joining the data in the data lake with operational data that is stored in an Amazon Aurora MySQL database. The company wants to enforce column-level authorization so that the company’s marketing team can access only a subset of columns in the database.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Use Amazon EMR to ingest the data directly from the database to the QuickSight SPICE engine. Include only the required columns.
- B. Use AWS Glue Studio to ingest the data from the database to the S3 data lake. Attach an IAM policy to the QuickSight users to enforce column-level access control. Use Amazon S3 as the data source in QuickSight.
- C. Use AWS Glue Elastic Views to create a materialized view for the database in Amazon S3. Create an S3 bucket policy to enforce column-level access control for the QuickSight users. Use Amazon S3 as the data source in QuickSight.
- D. Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use Lake Formation to enforce column-level access control for the QuickSight users. Use Amazon Athena as the data source in QuickSight.
```

정답 : `D`

- 이미 Lake Formation으로 거버넌스되는 S3 데이터 레이크가 있으므로, 운영 데이터(Aurora MySQL)를 Lake Formation 블루프린트/Glue ETL로 데이터 레이크에 적재
	- Lake Formation: 데이터 레이크용 중앙 거버넌스(데이터 카탈로그 연동). 테이블/열/행 수준 권한을 정의하고 Athena/Redshift Spectrum/EMR 등에서 이를 강제
- Lake Formation 권한(테이블/열 수준 권한)으로 접근을 통제하는 것이 가장 자연스럽고 운영이 간단
- QuickSight는 Athena를 데이터 소스로 사용할 때 Lake Formation 권한을 존중하며, 사용자/그룹별로 열 수준 권한 적용 가능
- 별도 커스텀 보안 계층이나 데이터 사본 관리 없이 정책 기반 거버넌스로 최소 운영 오버헤드 달성

오답 이유

- **A. EMR → SPICE 직수집**
    - SPICE로 필요한 열만 적재하는 것은 **정적 필터링**에 불과하며, **사용자 그룹별 동적 열 수준 권한 제어**가 어렵습니다. Lake Formation 거버넌스를 우회하게 되어 운영 복잡성 증가.
    
- **B. S3로 적재 후 IAM 정책으로 열 수준 제어**
    - **IAM/S3 버킷 정책은 객체 단위 권한**은 가능하지만 **열 수준(column-level)** 권한 제어는 제공하지 않습니다. 요구사항 미충족.
    
- **C. Glue Elastic Views + S3 버킷 정책**
    - S3 버킷 정책으로는 **열 수준 제어 불가**. 또한 Elastic Views는 현재 일반적 선택이 아니며 운영 오버헤드를 줄이지 못합니다.


## #342
한 거래 처리 회사는 Amazon EC2 인스턴스에서 주간 스크립트 배치 작업을 실행합니다. EC2 인스턴스는 Auto Scaling 그룹에 있습니다. 트랜잭션 수는 변할 수 있지만, 각 실행에서 기록되는 기준선 CPU 사용률은 최소 60%입니다. 회사는 작업이 실행되기 30분 전에 용량을 프로비저닝해야 합니다.

현재 엔지니어들은 Auto Scaling 그룹 매개변수를 수동으로 수정하여 이 작업을 완료합니다. 회사에는 Auto Scaling 그룹 개수에 필요한 용량 추세를 분석할 리소스가 없습니다. 회사는 Auto Scaling 그룹의 원하는 용량(desired capacity)을 자동으로 수정하는 방법이 필요합니다.

다음 중 운영 오버헤드가 가장 적은 솔루션은 무엇입니까?

A. Auto Scaling 그룹에 대해 동적 스케일링 정책을 생성합니다. CPU 사용률 지표를 기준으로 스케일링하도록 정책을 구성합니다. 해당 지표의 목표 값을 60%로 설정합니다.
B. Auto Scaling 그룹에 대해 예약 스케일링 정책을 생성합니다. 적절한 원하는 용량, 최소 용량, 최대 용량을 설정합니다. 반복을 매주로 설정합니다. 시작 시간을 배치 작업이 실행되기 30분 전으로 설정합니다.
C. Auto Scaling 그룹에 대해 예측 스케일링 정책을 생성합니다. 예측(forecast)을 기준으로 스케일링하도록 정책을 구성합니다. 스케일링 지표를 CPU 사용률로 설정합니다. 해당 지표의 목표 값을 60%로 설정합니다. 정책에서 작업이 실행되기 30분 전에 인스턴스를 사전 실행(pre-launch)하도록 설정합니다.
D. Auto Scaling 그룹의 CPU 사용률 지표 값이 60%에 도달할 때 AWS Lambda 함수를 호출하도록 Amazon EventBridge 이벤트를 생성합니다. Lambda 함수가 Auto Scaling 그룹의 원하는 용량과 최대 용량을 20% 증가시키도록 구성합니다.

```
A transaction processing company has weekly scripted batch jobs that run on Amazon EC2 instances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, but the baseline CPU utilization that is noted on each run is at least 60%. The company needs to provision the capacity 30 minutes before the jobs run.  
  
Currently, engineers complete this task by manually modifying the Auto Scaling group parameters. The company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts. The company needs an automated way to modify the Auto Scaling group’s desired capacity.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Create a dynamic scaling policy for the Auto Scaling group. Configure the policy to scale based on the CPU utilization metric. Set the target value for the metric to 60%.
- B. Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired capacity, minimum capacity, and maximum capacity. Set the recurrence to weekly. Set the start time to 30 minutes before the batch jobs run.
- C. Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based on forecast. Set the scaling metric to CPU utilization. Set the target value for the metric to 60%. In the policy, set the instances to pre-launch 30 minutes before the jobs run.
- D. Create an Amazon EventBridge event to invoke an AWS Lambda function when the CPU utilization metric value for the Auto Scaling group reaches 60%. Configure the Lambda function to increase the Auto Scaling group’s desired capacity and maximum capacity by 20%.
```

정답 : `C`

- 워크로드는 주간 패턴을 가지며, 실행 시점이 예측 가능 
- Predictive Scaling(예측 스케일링)은 과거 지표(예: CPU/요청 수)를 바탕으로 수요를 자동 예측하고, 설정한 리드 타임(최대 60분)으로 미리 인스턴스 기동
- 이를 통해 배치 시작 시점에 이미 충분한 용량이 확보 → CPU 급상승으로 인한 지연/중단을 방지하고, 회사가 직접 용량 추세를 분석할 필요가 없음

오답 이유

- **A. 동적(Target Tracking) 스케일링**
    - CPU가 **올라간 뒤** 반응해 확장하는 **사후적** 방식이라, “작업 30분 전 사전 프로비저닝” 요구를 충족하지 못합니다. 스파이크 직전에 용량이 준비되지 않아 지연이 발생할 수 있음.
    
- **B. 예약 스케일링**
    - 시각은 맞출 수 있지만 **필요 용량 수치를 직접 산정/유지**해야 합니다. 문제에서 회사는 **용량 추세 분석 리소스가 없다**고 했으므로 운영 부담이 남습니다. 또한 트랜잭션 수 변동에 자동 적응하지 못함.
    
- **D. EventBridge + Lambda 임계치 기반 증가**
    - CPU가 60%에 **도달한 후** 증설되므로 선제적 확장이 아닙니다. 증설 폭(20%)도 고정이라 실제 부하 변화에 비탄력적이며, Lambda/이벤트 운용까지 필요해 오버헤드가 큼.


## #343
한 솔루션스 아키텍트가 회사의 재해 복구(DR) 아키텍처를 설계하고 있습니다. 회사에는 예약 백업이 구성된 Amazon EC2 인스턴스의 프라이빗 서브넷에서 실행되는 MySQL 데이터베이스가 있습니다. DR 설계에는 여러 AWS 리전이 포함되어야 합니다.

다음 중 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. MySQL 데이터베이스를 여러 EC2 인스턴스로 마이그레이션합니다. DR 리전에 대기(standby) EC2 인스턴스를 구성합니다. 복제를 활성화합니다.
B. MySQL 데이터베이스를 Amazon RDS로 마이그레이션합니다. 멀티 AZ 배포를 사용합니다. 기본 DB 인스턴스에 대해 서로 다른 가용 영역에 읽기 복제를 활성화합니다.
C. MySQL 데이터베이스를 Amazon Aurora 글로벌 데이터베이스로 마이그레이션합니다. 기본 DB 클러스터는 기본 리전에 호스팅합니다. 보조 DB 클러스터는 DR 리전에 호스팅합니다.
D. MySQL 데이터베이스의 예약 백업을 S3 교차 리전 복제(CRR)가 구성된 Amazon S3 버킷에 저장합니다. DR 리전에서 데이터 백업을 사용하여 데이터베이스를 복원합니다.

```
A solutions architect is designing a company’s disaster recovery (DR) architecture. The company has a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled backup. The DR design needs to include multiple AWS Regions.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR Region. Turn on replication.
- B. Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read replication for the primary DB instance in the different Availability Zones.
- C. Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster in the primary Region. Host the secondary DB cluster in the DR Region.
- D. Store the scheduled backup of the MySQL database in an Amazon S3 bucket that is configured for S3 Cross-Region Replication (CRR). Use the data backup to restore the database in the DR Region.
```

정답 : `C`

- Aurora Global Database는 스토리지 레벨의 비동기 리전 간 복제를 제공해 초 단위 지연으로 데이터를 DR 리전에 복제
- 장애 시 수 분 내 프로모션으로 DR 리전의 보조 클러스터를 주로 승격 가능하며 완전관리형이어서 운영 오버헤드가 최소화
- 기존 EC2 자체 운영 MySQL 대비 패치/백업/모니터링/장애조치 자동화 측면에서 훨씬 간편

오답 이유

- **A. EC2 자체 운영 + 대기 인스턴스 + 복제**
    - MySQL 설치/패치/백업/복구/페일오버 자동화 등 **운영 부담이 큼**. 리전 간 복제/장애조치 스크립트 관리도 필요.
    
- **B. RDS 멀티 AZ + 읽기 복제(가용 영역 간)**
    - 멀티 AZ는 **동일 리전 내 고가용성**입니다. 보기 설명의 읽기 복제도 “서로 다른 **가용 영역**”으로 표기되어 **다중 리전 DR 요구를 충족하지 못함**(Cross-Region Read Replica가 아니라는 점).
    
- **D. S3 CRR로 백업 전송 후 복원**
    - 다중 리전 데이터 사본은 확보되지만, **복구(RTO)가 길고 수동 절차**가 많습니다. DR 시 즉시 가동보다는 **복원 작업**이 필요하여 **운영 오버헤드와 다운타임**이 큼.


## #344
한 회사는 Amazon Simple Queue Service(Amazon SQS)를 사용하여 메시지를 파싱하는 Java 애플리케이션을 보유하고 있습니다. 애플리케이션은 크기가 256 KB를 초과하는 메시지를 파싱할 수 없습니다. 회사는 최대 50 MB 크기의 메시지를 파싱할 수 있는 기능을 애플리케이션에 구현하려고 합니다.

가장 적은 코드 변경으로 이러한 요구 사항을 충족할 수 있는 솔루션은 무엇입니까?

A. Amazon SQS Extended Client Library for Java를 사용하여 256 KB를 초과하는 메시지를 Amazon S3에 저장합니다.
B. Amazon SQS 대신 Amazon EventBridge를 사용하여 애플리케이션에서 대용량 메시지를 게시합니다.
C. Amazon SQS에서 제한을 변경하여 256 KB보다 큰 메시지를 처리할 수 있도록 합니다.
D. 256 KB보다 큰 메시지를 Amazon Elastic File System(Amazon EFS)에 저장합니다. Amazon SQS가 메시지에서 이 위치를 참조하도록 구성합니다.

```
A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to parse messages. The application cannot parse messages that are larger than 256 KB in size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB.  
  
Which solution will meet these requirements with the FEWEST changes to the code?

- A. Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.
- B. Use Amazon EventBridge to post large messages from the application instead of Amazon SQS.
- C. Change the limit in Amazon SQS to handle messages that are larger than 256 KB.
- D. Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS). Configure Amazon SQS to reference this location in the messages.
```

정답 : `A`

- Amazon SQS 메시지 본문 크기 제한은 256KB로 고정되어 변경 불가
- SQS Extended Client Library for Java는 이 한계를 우회하기 위해 설계된 공식 라이브러리
	- 큰 메시지(최대 2GB)를 Amazon S3에 저장하고, 메시지 본문에는 S3 객체 포인터를 포함
- 기존 Java 애플리케이션에서 라이브러리만 추가하면 되므로 코드 변경이 최소화

오답 이유

- **B. EventBridge 사용**
    - EventBridge는 이벤트 버스 서비스로, 메시지 크기 제한이 **256 KB**로 동일합니다. 대용량 메시지 지원 불가.
    
- **C. SQS 제한 변경**
    - **SQS 메시지 크기 제한은 서비스 차원에서 고정(최대 256 KB)** 되어 있으며, 설정 변경 불가.
    
- **D. EFS에 저장 후 SQS 참조**
    - SQS는 메시지 본문 내에서 EFS를 참조하는 기능을 제공하지 않습니다. 커스텀 구현 필요 → 코드 변경 및 운영 오버헤드 증가.


## #345
한 회사는 주요 웹 애플리케이션 중 하나의 콘텐츠 접근을 제한하고, AWS에서 제공되는 인가(authorization) 기법을 사용하여 콘텐츠를 보호하려고 합니다. 회사는 서버리스 아키텍처와 100명 미만 사용자를 위한 인증 솔루션을 구현하고자 합니다. 솔루션은 주요 웹 애플리케이션과 통합되어야 하며, 전 세계적으로 웹 콘텐츠를 제공해야 합니다. 또한 회사의 사용자 기반이 성장함에 따라 확장 가능해야 하고, 가능한 가장 낮은 로그인 지연 시간을 제공해야 합니다.

다음 중 가장 비용 효율적으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 인증은 Amazon Cognito를 사용합니다. 인가는 Lambda@Edge를 사용합니다. 전 세계적으로 웹 애플리케이션 제공은 Amazon CloudFront를 사용합니다.
B. 인증은 AWS Directory Service for Microsoft Active Directory를 사용합니다. 인가는 AWS Lambda를 사용합니다. 전 세계적으로 웹 애플리케이션 제공은 Application Load Balancer를 사용합니다.
C. 인증은 Amazon Cognito를 사용합니다. 인가는 AWS Lambda를 사용합니다. 전 세계적으로 웹 애플리케이션 제공은 Amazon S3 Transfer Acceleration을 사용합니다.
D. 인증은 AWS Directory Service for Microsoft Active Directory를 사용합니다. 인가는 Lambda@Edge를 사용합니다. 전 세계적으로 웹 애플리케이션 제공은 AWS Elastic Beanstalk를 사용합니다.

```
A company wants to restrict access to the content of one of its main web applications and to protect the content by using authorization techniques available on AWS. The company wants to implement a serverless architecture and an authentication solution for fewer than 100 users. The solution needs to integrate with the main web application and serve web content globally. The solution must also scale as the company's user base grows while providing the lowest login latency possible.  
  
Which solution will meet these requirements MOST cost-effectively?

- A. Use Amazon Cognito for authentication. Use Lambda@Edge for authorization. Use Amazon CloudFront to serve the web application globally.
- B. Use AWS Directory Service for Microsoft Active Directory for authentication. Use AWS Lambda for authorization. Use an Application Load Balancer to serve the web application globally.
- C. Use Amazon Cognito for authentication. Use AWS Lambda for authorization. Use Amazon S3 Transfer Acceleration to serve the web application globally.
- D. Use AWS Directory Service for Microsoft Active Directory for authentication. Use Lambda@Edge for authorization. Use AWS Elastic Beanstalk to serve the web application globally.
```

정답 : `A`

- Cognito(서버리스 IdP) + Lambda@Edge(서버리스) + CloudFront(관리형 CDN) 조합은 완전 서버리스
- 100명 미만 사용자에 대해 Cognito는 매우 비용 효율적, 사용량 증가 시 자동 확장
- CloudFront 엣지에서 콘텐츠를 제공하고 Lambda@Edge에서 토큰 검사/인가를 수행하면 사용자 근처 엣지 로케이션에서 인증 후 라우팅되 레이턴시 최소화
- Cognito 호스티드 UI/OIDC/OAuth 2.0/JWT 기반으로 프론트엔드･백엔드 쉽게 연동 가능
	- Lambda@Edge에서 JWT 검증/쿠키 검사로 세밀한 인가 정책 적용 가능

오답 이유

- **B. Directory Service + Lambda + ALB**
    - **Directory Service**는 관리형 AD로 비용이 상대적으로 높고(특히 소규모 사용자에 비효율), 서버리스 아키텍처 요구에 부적합.
    - **ALB**는 리전 서비스이며 글로벌 엣지 제공이 아님 → 전 세계 최소 지연 제공에 불리.
    - 인가를 리전에 있는 **Lambda**로 처리하면 엣지에서의 최소 지연 장점을 살리지 못함.
    
- **C. Cognito + Lambda + S3 Transfer Acceleration**
    - **S3 Transfer Acceleration**은 객체 업·다운로드 가속용이며 **웹 콘텐츠 글로벌 제공**에는 **CloudFront**가 정답.
    - 인가를 리전 **Lambda**로 처리 → 엣지에서의 저지연 인가 불가.
    
- **D. Directory Service + Lambda@Edge + Elastic Beanstalk**
    - **Elastic Beanstalk**는 관리형이지만 **서버리스가 아님**. 인프라 관리 부담 및 비용 증가.
    - Directory Service 역시 소규모 사용자·비용 효율 요구와 상충.


## #346
한 회사는 데이터 센터에 오래된 NAS(Network-Attached Storage) 어레이를 보유하고 있습니다. 이 NAS 어레이는 SMB 공유 및 NFS 공유를 클라이언트 워크스테이션에 제공합니다. 회사는 새로운 NAS 어레이를 구매하고 싶지 않으며, NAS 어레이의 지원 계약을 갱신하는 비용도 들이고 싶지 않습니다. 일부 데이터는 자주 액세스되지만, 많은 데이터는 비활성 상태입니다.

솔루션스 아키텍트는 데이터를 Amazon S3로 마이그레이션하고, S3 수명 주기(Lifecycle) 정책을 사용하며, 클라이언트 워크스테이션에 동일한 "룩 앤 필(look and feel)"을 유지하는 솔루션을 구현해야 합니다. 솔루션스 아키텍트는 이 솔루션의 일부로 AWS Storage Gateway를 선택했습니다.

이 요구 사항을 충족하기 위해 어떤 유형의 스토리지 게이트웨이를 프로비저닝해야 합니까?

A. Volume Gateway  
B. Tape Gateway  
C. Amazon FSx File Gateway  
D. Amazon S3 File Gateway  

```
A company has an aging network-attached storage (NAS) array in its data center. The NAS array presents SMB shares and NFS shares to client workstations. The company does not want to purchase a new NAS array. The company also does not want to incur the cost of renewing the NAS array’s support contract. Some of the data is accessed frequently, but much of the data is inactive.  
  
A solutions architect needs to implement a solution that migrates the data to Amazon S3, uses S3 Lifecycle policies, and maintains the same look and feel for the client workstations. The solutions architect has identified AWS Storage Gateway as part of the solution.  
  
Which type of storage gateway should the solutions architect provision to meet these requirements?

- A. Volume Gateway
- B. Tape Gateway
- C. Amazon FSx File Gateway
- D. Amazon S3 File Gateway
```

정답 : `D`

- 요구사항
	- 기존 NAS와 동일한 SMB/NFS 공유 방식 유지
	- 데이터를 Amazon S3로 마이그레이션
	- S3 라이프사이클 정책 사용 → 비활성 데이터 자동 계층화
- S3 File Gateway는 로컬에서 SMB 또는 NFS 공유를 제공하며, 실제 데이터는 Amazon S3에 저장
- 자주 사용하는 데이터는 로컬 캐시에 오래된 데이터는 S3에 두어 비용 최적화 가능
- 클라이언트 입장에서는 NAS와 동일한 접근 방식으로 접근 가능하므로 "look and feel" 유지 요구 충족

오답 이유

- **A. Volume Gateway**
    - iSCSI 블록 스토리지 제공. NAS 공유(SMB/NFS) 기능이 없어 **워크스테이션에 동일한 경험 제공 불가**.
    
- **B. Tape Gateway**
    - 가상 테이프 라이브러리(VTL) 제공. **백업/아카이브 시나리오 전용**으로 NAS 대체 목적과 맞지 않음.
    
- **C. Amazon FSx File Gateway**
    - FSx 파일 시스템(Native Windows File System/ONTAP) 캐싱용. 데이터는 S3에 저장되지 않고, **FSx 서비스 기반**이므로 문제의 요구(데이터를 S3에 저장 및 수명주기 정책 활용)와 불일치.


## #347
한 회사에 Amazon EC2 인스턴스에서 실행되는 애플리케이션이 있습니다. 솔루션스 아키텍트는 회사의 현재 요구 사항을 기반으로 특정 인스턴스 패밀리와 다양한 인스턴스 크기로 표준화했습니다.

회사는 향후 3년 동안 애플리케이션 비용 절감을 최대화하고자 합니다. 또한 애플리케이션의 인기와 사용량에 따라 향후 6개월 내에 인스턴스 패밀리와 크기를 변경할 수 있어야 합니다.

다음 중 가장 비용 효율적으로 이러한 요구 사항을 충족할 수 있는 솔루션은 무엇입니까?

A. Compute Savings Plan
B. EC2 Instance Savings Plan
C. Zonal Reserved Instances
D. Standard Reserved Instances

```
A company has an application that is running on Amazon EC2 instances. A solutions architect has standardized the company on a particular instance family and various instance sizes based on the current needs of the company.  
  
The company wants to maximize cost savings for the application over the next 3 years. The company needs to be able to change the instance family and sizes in the next 6 months based on application popularity and usage.  
  
Which solution will meet these requirements MOST cost-effectively?

- A. Compute Savings Plan
- B. EC2 Instance Savings Plan
- C. Zonal Reserved Instances
- D. Standard Reserved Instances
```

정답 : `A`

- Compute Savings Plan은 약정한 사용 금액(USD/시간) 내에서 인스턴스 패밀리, 크기, 리전, OS/테넌시 변경은 물론 Fargate/Lambda까지 폭넓게 적용되는 가장 유연한 약정 형태
- 3년 약정으로 설정하면 Savings Plan 중에서도 최대 수준의 할인(유연성 고려 시 최적)을 확보하면서, 향후 워크로드 변화에 맞게 자유롭게 전환 가능

오답 이유

- **B. EC2 Instance Savings Plan**
    - **리전 내 특정 인스턴스 패밀리**에 묶입니다. 사이즈/OS/테넌시는 바꿀 수 있지만 **패밀리 변경은 불가**하므로, 6개월 내 패밀리 변경 요구와 충돌합니다.
    
- **C. Zonal Reserved Instances (RI)**
    - 특정 **가용 영역(AZ) 고정**으로 용량 예약 효과가 있으나, 패밀리/리전/플랫폼 유연성이 매우 제한적입니다. 워크로드 변화 대응에 부적합.
    
- **D. Standard Reserved Instances**
    - 특정 **리전·패밀리·플랫폼** 등에 더 강하게 묶이며, 변경 유연성이 낮습니다(특히 패밀리 변경 어려움). 요구된 민첩성에 맞지 않습니다.


## #348
한 회사가 웨어러블 기기를 사용하는 많은 참가자들로부터 데이터를 수집합니다. 회사는 데이터를 Amazon DynamoDB 테이블에 저장하고 애플리케이션으로 데이터를 분석합니다. 데이터 워크로드는 일정하고 예측 가능합니다. 회사는 DynamoDB에 대해 예측된 예산 이하를 유지하고자 합니다.

가장 비용 효율적으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 프로비저닝 모드와 DynamoDB Standard-Infrequent Access(DynamoDB Standard-IA)를 사용합니다. 예측된 워크로드에 대해 용량을 예약합니다.
B. 프로비저닝 모드를 사용합니다. 읽기 용량 단위(RCU)와 쓰기 용량 단위(WCU)를 지정합니다.
C. 온디맨드 모드를 사용합니다. 워크로드 변화에 대응할 수 있도록 RCU와 WCU를 충분히 높게 설정합니다.
D. 온디맨드 모드를 사용합니다. 예약 용량과 함께 RCU와 WCU를 지정합니다.

```
A company collects data from a large number of participants who use wearable devices. The company stores the data in an Amazon DynamoDB table and uses applications to analyze the data. The data workload is constant and predictable. The company wants to stay at or below its forecasted budget for DynamoDB.  
  
Which solution will meet these requirements MOST cost-effectively?

- A. Use provisioned mode and DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA). Reserve capacity for the forecasted workload.
- B. Use provisioned mode. Specify the read capacity units (RCUs) and write capacity units (WCUs).
- C. Use on-demand mode. Set the read capacity units (RCUs) and write capacity units (WCUs) high enough to accommodate changes in the workload.
- D. Use on-demand mode. Specify the read capacity units (RCUs) and write capacity units (WCUs) with reserved capacity.
```

정답 : `B`

- 워크로드가 일정하고 예측 가능할 때는 프로비저닝 모드가 가장 비용 효율적
- 필요한 RCU/WCU를 정확히 설정하면 비용이 선형･예측 가능하게 유지되어 예산 이하로 운영하기 쉬움
- 온디맨드 모드는 변동이 크거나 예측이 어려운 워크로드에 유리하지만, 지속적인 트래픽에서는 비용이 더 높아질 수 있음

오답 이유

- **A. 프로비저닝 + Standard-IA + 예약 용량**
    - **DynamoDB Standard-IA** 테이블 클래스는 **드물게 액세스되는** 데이터에 적합합니다. 본 시나리오는 “애플리케이션이 데이터를 분석”하며 **일정한 액세스**가 예상되므로 Standard-IA는 **읽기/쓰기 비용 증가**로 오히려 **비효율**일 수 있습니다. (예약 용량 자체는 프로비저닝에 유리하지만, Standard-IA 선택이 부적절)
    
- **C. 온디맨드 모드**
    - 설정할 RCU/WCU가 없습니다(온디맨드는 자동). 그리고 **지속적·예측 가능한** 워크로드에서는 온디맨드가 **비용 우위가 낮습니다**.
    
- **D. 온디맨드 + 예약 용량**
    - **예약 용량은 프로비저닝 모드에만 적용**되며, 온디맨드에는 적용되지 않습니다. 또한 온디맨드에서 RCU/WCU를 지정할 수도 없습니다. **개념적으로 잘못된 선택**입니다.


## #349
한 회사는 ap-southeast-3 리전에 있는 Amazon Aurora PostgreSQL 데이터베이스에 기밀 데이터를 저장합니다. 데이터베이스는 AWS Key Management Service(AWS KMS) 고객 관리형 키로 암호화되어 있습니다. 최근 이 회사는 인수되었으며, ap-southeast-3에서 인수하는 회사의 AWS 계정과 데이터베이스 백업을 안전하게 공유해야 합니다.

이 요구 사항을 충족하려면 솔루션스 아키텍트는 무엇을 해야 합니까?

A. 데이터베이스 스냅샷을 생성합니다. 스냅샷을 새 비암호화 스냅샷으로 복사합니다. 새 스냅샷을 인수 회사의 AWS 계정과 공유합니다.
B. 데이터베이스 스냅샷을 생성합니다. KMS 키 정책에 인수 회사의 AWS 계정을 추가합니다. 스냅샷을 인수 회사의 AWS 계정과 공유합니다.
C. 다른 AWS 관리형 KMS 키를 사용하는 데이터베이스 스냅샷을 생성합니다. KMS 키 별칭에 인수 회사의 AWS 계정을 추가합니다. 스냅샷을 인수 회사의 AWS 계정과 공유합니다.
D. 데이터베이스 스냅샷을 생성합니다. 데이터베이스 스냅샷을 다운로드합니다. 데이터베이스 스냅샷을 Amazon S3 버킷에 업로드합니다. S3 버킷 정책을 업데이트하여 인수 회사의 AWS 계정에서 액세스할 수 있도록 합니다.

```
A company stores confidential data in an Amazon Aurora PostgreSQL database in the ap-southeast-3 Region. The database is encrypted with an AWS Key Management Service (AWS KMS) customer managed key. The company was recently acquired and must securely share a backup of the database with the acquiring company’s AWS account in ap-southeast-3.  
  
What should a solutions architect do to meet these requirements?

- A. Create a database snapshot. Copy the snapshot to a new unencrypted snapshot. Share the new snapshot with the acquiring company’s AWS account.
- B. Create a database snapshot. Add the acquiring company’s AWS account to the KMS key policy. Share the snapshot with the acquiring company’s AWS account.
- C. Create a database snapshot that uses a different AWS managed KMS key. Add the acquiring company’s AWS account to the KMS key alias. Share the snapshot with the acquiring company's AWS account.
- D. Create a database snapshot. Download the database snapshot. Upload the database snapshot to an Amazon S3 bucket. Update the S3 bucket policy to allow access from the acquiring company’s AWS account.
```

정답 : `B`

- 암호화된 RDS/Aurora 스냅샷을 다른 AWS 계정과 공유하려면, 스냅샷 공유와 함께 해당 스냅샷의 KMS 키 사용 권한을 그 계정에 부여해야 함
- KMS 키 정책에 상대 계정을 추가해야 상대 계정이 스냅샷을 복사/복원 가능

오답 이유

- **A:** 암호화된 스냅샷을 **비암호화로 변환할 수 없습니다.** 또한 암호화 해제를 목적으론 보안 요구에도 부합하지 않습니다.
    
- **C:** **AWS 관리형 KMS 키는 키 정책을 고객이 편집할 수 없고** 교차 계정 공유 제어에 부적합합니다. 또한 “키 별칭에 계정을 추가”는 권한 부여가 아닙니다. 교차 계정 접근은 **키 정책/그랜트**로 해야 합니다.
    
- **D:** RDS/Aurora **스냅샷은 다운로드할 수 없습니다.** (스냅샷을 S3에 직접 업로드하는 방식 불가) Export to S3 기능은 Parquet 등으로 **데이터 내보내기**일 뿐 스냅샷 공유와는 다릅니다.


## #350
한 회사가 고객 거래를 저장하기 위해 us-east-1 리전에서 100 GB Amazon RDS for Microsoft SQL Server 단일 AZ(Single-AZ) DB 인스턴스를 사용하고 있습니다. 회사는 이 DB 인스턴스에 대해 고가용성과 자동 복구가 필요합니다.

또한 회사는 1년에 여러 번 RDS 데이터베이스에서 보고서를 실행해야 합니다. 보고서 처리 과정 때문에 고객 계정에 거래가 반영되는 데 평소보다 더 오래 걸립니다. 회사는 보고서 처리 성능을 개선하는 솔루션이 필요합니다.

이 요구 사항을 충족하기 위해 어떤 단계 조합을 수행해야 합니까? (두 가지 선택)

A. DB 인스턴스를 Single-AZ에서 Multi-AZ 배포로 수정합니다.
B. 현재 DB 인스턴스의 스냅샷을 생성합니다. 해당 스냅샷을 복원하여 다른 가용 영역의 새 RDS 배포로 만듭니다.
C. 다른 가용 영역에 DB 인스턴스의 읽기 전용 복제본을 생성합니다. 보고서에 대한 모든 요청을 읽기 전용 복제본으로 전달합니다.
D. 데이터베이스를 RDS Custom으로 마이그레이션합니다.
E. RDS Proxy를 사용하여 보고 요청을 유지 관리(메인터넌스) 시간으로 제한합니다.

```
A company uses a 100 GB Amazon RDS for Microsoft SQL Server Single-AZ DB instance in the us-east-1 Region to store customer transactions. The company needs high availability and automatic recovery for the DB instance.  
  
The company must also run reports on the RDS database several times a year. The report process causes transactions to take longer than usual to post to the customers’ accounts. The company needs a solution that will improve the performance of the report process.  
  
Which combination of steps will meet these requirements? (Choose two.)

- A. Modify the DB instance from a Single-AZ DB instance to a Multi-AZ deployment.
- B. Take a snapshot of the current DB instance. Restore the snapshot to a new RDS deployment in another Availability Zone.
- C. Create a read replica of the DB instance in a different Availability Zone. Point all requests for reports to the read replica.
- D. Migrate the database to RDS Custom.
- E. Use RDS Proxy to limit reporting requests to the maintenance window.
```

정답 : `A, C`

- RDS for SQL Server에서 멀티-AZ는 동기식 스탠바이를 유지하여 고가용성(HA)과 자동 장애 복구를 제공
- 보고 쿼리를 읽기 전용 복제본으로 분리하면 운영 트랜잭션과 리포팅 워크로드의 경합을 줄여 성능을 개선
	- 복제본을 다른 AZ에 두면 내결함성도 향상

오답 이유

- **B. 스냅샷 복원본으로 보고 수행**
    - 스냅샷은 **정적 시점 사본**으로, 복원·가동·폐기를 매번 수동으로 해야 하므로 운영 오버헤드가 큽니다. 또한 최신 데이터로 보고하기 어렵고 자동 장애 복구 요건도 충족하지 못합니다.
    
- **D. RDS Custom으로 마이그레이션**
    - OS/DB에 대한 더 많은 제어가 필요한 특수 요구가 아니면 불필요하게 복잡합니다. HA와 리포팅 성능 요구를 만족하는 더 단순한 관리형 옵션(A, C)이 존재합니다.
    
- **E. RDS Proxy로 유지보수 시간에만 보고 제한**
    - RDS Proxy는 **연결 풀링/가용성 향상**에 좋지만, 보고 워크로드를 성능적으로 분리하지 못합니다. 또한 “보고를 특정 시간으로 제한”은 비즈니스 요구를 만족하지 못하고 사용성도 떨어집니다.


## #351
한 회사가 데이터 관리 애플리케이션을 AWS로 이전하고 있습니다. 회사는 이벤트 기반 아키텍처로 전환하기를 원합니다. 이 아키텍처는 더 분산되어야 하며, 워크플로의 다양한 단계를 수행하면서 서버리스 개념을 사용해야 합니다. 또한 회사는 운영 오버헤드를 최소화하고 싶습니다.

다음 중 어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. 워크플로를 AWS Glue에서 구축합니다. AWS Glue를 사용하여 AWS Lambda 함수를 호출해 워크플로 단계를 처리합니다.  
B. 워크플로를 AWS Step Functions에서 구축합니다. 애플리케이션을 Amazon EC2 인스턴스에 배포합니다. Step Functions를 사용하여 EC2 인스턴스에서 워크플로 단계를 호출합니다.  
C. 워크플로를 Amazon EventBridge에서 구축합니다. EventBridge를 사용하여 AWS Lambda 함수를 일정에 따라 호출해 워크플로 단계를 처리합니다.  
D. 워크플로를 AWS Step Functions에서 구축합니다. Step Functions로 상태 머신을 생성합니다. 상태 머신을 사용하여 AWS Lambda 함수를 호출해 워크플로 단계를 처리합니다.

```
A company is moving its data management application to AWS. The company wants to transition to an event-driven architecture. The architecture needs to be more distributed and to use serverless concepts while performing the different aspects of the workflow. The company also wants to minimize operational overhead.  
  
Which solution will meet these requirements?

- A. Build out the workflow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process the workflow steps.
- B. Build out the workflow in AWS Step Functions. Deploy the application on Amazon EC2 instances. Use Step Functions to invoke the workflow steps on the EC2 instances.
- C. Build out the workflow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda functions on a schedule to process the workflow steps.
- D. Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workflow steps.
```

정답 : `D`

- 이벤트 기반 + 서버리스 + 최소 운영 오버헤드 요구사항을 충족하려면 Step Functions(워크플로 관리) + 람다(서버리스 처리)
- Step Functions는 상태 머신 기반으로 분산된 워크플로를 정의하고, 각 단계를 람다 함수로 연결해 서버 관리 없이 처리 가능
- 자동 오류 처리, 재시도, 분기 로직, 상태 추적 기능이 있어 이벤트 기반 분산 아키텍처를 구현하는 데 적합

오답 이유

- **A. AWS Glue + Lambda**
    - Glue는 **ETL(데이터 변환)** 중심 서비스이며 일반 워크플로 오케스트레이션 용도 아님. 이벤트 기반 아키텍처 설계와는 거리가 있음.
    
- **B. Step Functions + EC2**
    - EC2 인스턴스는 **서버 관리 필요 → 운영 오버헤드 증가**. “서버리스”와 “운영 부담 최소화” 요구에 위배됨.
    
- **C. EventBridge + Lambda (스케줄 기반)**
    - EventBridge는 이벤트 라우팅에 최적화되어 있으나, **복잡한 워크플로 오케스트레이션(상태 추적, 분기, 오류 처리)** 기능은 없음. 단순 트리거에는 적합하지만, 문제의 “워크플로 단계 관리” 요구와는 불일치.


## #352
한 회사가 온라인 멀티플레이어 게임을 위한 네트워크를 설계하고 있습니다. 이 게임은 UDP 네트워킹 프로토콜을 사용하며 8개의 AWS 리전에 배포될 예정입니다. 네트워크 아키텍처는 지연(latency)과 패킷 손실을 최소화하여 최종 사용자에게 높은 품질의 게임 경험을 제공해야 합니다.

다음 중 어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. 각 리전에 트랜짓 게이트웨이를 설정합니다. 각 트랜짓 게이트웨이 간에 리전 간 피어링 연결을 생성합니다.
B. UDP 리스너와 각 리전에 엔드포인트 그룹을 구성하여 AWS Global Accelerator를 설정합니다.
C. UDP를 켠 Amazon CloudFront를 설정합니다. 각 리전에 오리진을 구성합니다.
D. 각 리전 간에 VPC 피어링 메시를 설정합니다. 각 VPC에서 UDP를 켭니다.

```
A company is designing the network for an online multi-player game. The game uses the UDP networking protocol and will be deployed in eight AWS Regions. The network architecture needs to minimize latency and packet loss to give end users a high-quality gaming experience.  
  
Which solution will meet these requirements?

- A. Setup a transit gateway in each Region. Create inter-Region peering attachments between each transit gateway.
- B. Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.
- C. Set up Amazon CloudFront with UDP turned on. Configure an origin in each Region.
- D. Set up a VPC peering mesh between each Region. Turn on UDP for each VPC.
```

정답 : `B`

- Global Accelerator는 Anycast 정적 IP를 통해 사용자 트래픽을 가까운 엣지 로케이션으로 온보딩
- 이후 AWS 글로벌 백본망으로 각 리전의 엔드포인트(ALB/NLB/EC2 등)까지 트래픽 전송
- UDP 리스너를 지원하며 인터넷 구간의 변동성을 줄여 지연 및 패킷 손실을 최소화
- 다중 리전 엔드포인트로 자동 헬스체크 및 장애 시 빠른 페일오버 제공
- 멀티플레이어 게임에 필요한 저지연･고가용 UDP 전송에 적합

오답 이유

- **A. Transit Gateway + 리전 간 피어링**
    - TGW는 **VPC 간 라우팅/사설 연결**을 위한 서비스입니다. **최종 사용자 → 게임 엔드포인트**의 퍼블릭 경로 최적화나 가속을 제공하지 않습니다. 운영 복잡도도 큼.
    
- **C. CloudFront(UDP 켜기)**
    - CloudFront는 **HTTP/HTTPS** 기반의 CDN으로 **UDP를 지원하지 않습니다.** 게임용 UDP 트래픽 가속에 부적합.
    
- **D. VPC 피어링 메시**
    - VPC 간 **사설 통신**을 위한 것이며, 클라이언트에서 리전 엔드포인트까지의 인터넷 구간 지연/손실을 줄여주지 못합니다. 또한 “UDP를 켠다”는 설정 개념도 존재하지 않습니다.


## #353
한 회사가 단일 가용 영역(Availability Zone)의 Amazon EC2 인스턴스에서 3계층 웹 애플리케이션을 호스팅하고 있습니다. 웹 애플리케이션은 EC2 인스턴스에서 호스팅되는 셀프 매니지드(self-managed) MySQL 데이터베이스를 사용하며, Amazon Elastic Block Store(Amazon EBS) 볼륨에 데이터를 저장합니다. 현재 MySQL 데이터베이스는 1TB 프로비저닝된 IOPS SSD(io2) EBS 볼륨을 사용합니다. 회사는 피크 트래픽에서 읽기와 쓰기 각각 1,000 IOPS의 트래픽을 예상합니다.

회사는 중단을 최소화하고, 성능을 안정화하며, 비용을 절감하면서도 IOPS 용량을 두 배로 유지하기를 원합니다. 또한 데이터베이스 계층을 고가용성(HA) 및 내결함성을 갖춘 완전관리형 솔루션으로 이전하고자 합니다.

다음 중 가장 비용 효율적으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. io2 Block Express EBS 볼륨을 사용하는 Amazon RDS for MySQL DB 인스턴스의 Multi-AZ 배포를 사용합니다.
B. 범용 SSD(gp2) EBS 볼륨을 사용하는 Amazon RDS for MySQL DB 인스턴스의 Multi-AZ 배포를 사용합니다.
C. Amazon S3 Intelligent-Tiering 액세스 계층을 사용합니다.
D. 두 개의 대형 EC2 인스턴스를 사용하여 액티브-패시브 모드로 데이터베이스를 호스팅합니다.

```
A company hosts a three-tier web application on Amazon EC2 instances in a single Availability Zone. The web application uses a self-managed MySQL database that is hosted on an EC2 instance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL database currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects traffic of 1,000 IOPS for both reads and writes at peak traffic.  
  
The company wants to minimize any disruptions, stabilize performance, and reduce costs while retaining the capacity for double the IOPS. The company wants to move the database tier to a fully managed solution that is highly available and fault tolerant.  
  
Which solution will meet these requirements MOST cost-effectively?

- A. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block Express EBS volume.
- B. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.
- C. Use Amazon S3 Intelligent-Tiering access tiers.
- D. Use two large EC2 instances to host the database in active-passive mode.
```

정답 : `B`

- RDS for MySQL Multi-AZ는 동기식 스탠바이(자동 장애조치)를 제공해 중단 최소화와 운영 오버헤드 감소
- gp2는 GB당 3 IOPS의 베이스라인을 제공. 1TB(=1024GB)면 약 3,072 IOPS의 지속 성능을 제공
	- 요구한 2,000 IOPS(현 피크의 2배 용량)를 버스트 없이 안정적으로 충족
- io2/Block Express 대비 gp2가 훨씬 저렴하며, 요구 성능을 이미 충족하므로 가장 비용 효율적
	- io2/Block Express는 높은 IOPS/일관성을 제공하지만 고비용, 고성능 OLTP나 수만~수십만 IOPS 필요 시 적합
- RDS 관리형 백업/모니터링/자동화와 멀티-AZ 복제 구조로 성능을 일관되게 유지 가능

오답 이유

- **A. RDS + io2 Block Express**
    - 성능은 우수하지만, **요구한 IOPS(≈2,000)** 대비 **과도한 스펙**으로 **비용 상승**. 비용 최적화 목표에 부적합.
    
- **C. S3 Intelligent-Tiering**
    - **객체 스토리지** 계층 정책으로, **RDBMS 블록 스토리지/IOPS 요구**와 무관. 요구사항 미충족.
    
- **D. EC2 액티브-패시브**
    - 여전히 **셀프 매니지드**로 패치/백업/모니터링/페일오버 자동화 부담이 큼. 또한 단일 AZ 구성에서 HA 보장이 어려우며, **완전관리형/비용 절감** 요구에 부적합.


## #354
한 회사가 AWS에서 서버리스 애플리케이션을 호스팅하고 있습니다. 애플리케이션은 Amazon API Gateway, AWS Lambda, 그리고 Amazon RDS for PostgreSQL 데이터베이스를 사용합니다. 회사는 피크 트래픽이나 예측 불가능한 트래픽 시간 동안 데이터베이스 연결 타임아웃으로 인한 애플리케이션 오류가 증가하는 것을 발견했습니다. 회사는 코드 변경을 최소화하면서 애플리케이션 실패를 줄일 수 있는 솔루션이 필요합니다.

이 요구 사항을 충족하려면 솔루션스 아키텍트는 무엇을 해야 합니까?

A. Lambda 동시성 비율을 줄입니다.
B. RDS DB 인스턴스에서 RDS Proxy를 활성화합니다.
C. 더 많은 연결을 수용하도록 RDS DB 인스턴스 클래스를 리사이즈합니다.
D. 온디맨드 스케일링이 가능한 Amazon DynamoDB로 데이터베이스를 마이그레이션합니다.

```
A company hosts a serverless application on AWS. The application uses Amazon API Gateway, AWS Lambda, and an Amazon RDS for PostgreSQL database. The company notices an increase in application errors that result from database connection timeouts during times of peak traffic or unpredictable traffic. The company needs a solution that reduces the application failures with the least amount of change to the code.  
  
What should a solutions architect do to meet these requirements?

- A. Reduce the Lambda concurrency rate.
- B. Enable RDS Proxy on the RDS DB instance.
- C. Resize the RDS DB instance class to accept more connections.
- D. Migrate the database to Amazon DynamoDB with on-demand scaling.
```

정답 : `B`

- 람다는 트래픽 급증 시 대량의 동시 실행을 유발 → 각 실행이 DB에 직접 연결하면 연결 폭증으로 포스트그레스 연결 한도/메모리 소진으로 타임아웃과 오류 발생
- Amazon RDS Proxy는 커넥션 풀링/멀티플렉싱으로 람다의 폭발적 동시성을 흡수하고, 대기/재시도/자동 페일오버 연계로 안정성 높임
- 애플리케이션 측 변경은 DB 엔드포인트를 프록시 엔드포인트로 교체하는 수준으로 코드 변경을 최소화

오답 이유

- **A. Lambda 동시성 축소**
    - 오류는 줄 수 있으나 **처리량이 감소**하고 지연이 증가합니다. 근본 원인(연결 관리 부재)을 해결하지 못하며 비즈니스 요구를 저해할 수 있음.
    
- **C. RDS 인스턴스 리사이즈**
    - 일시적 완화는 가능하나 **연결 폭증 문제는 구조적**입니다. 과대 프로비저닝 비용이 발생하고, 여전히 스파이크 시 한계에 부딪칠 수 있습니다.
    
- **D. DynamoDB 마이그레이션**
    - 대규모 **애플리케이션/데이터 모델 변경**이 필요합니다. “코드 변경 최소화” 요구와 상충하며, 관계형 워크로드 적합성도 불확실.


## #355
한 회사가 오래된 애플리케이션을 AWS로 마이그레이션하고 있습니다. 이 애플리케이션은 매시간 배치 작업을 실행하며 CPU 집약적입니다. 온프레미스 서버에서 배치 작업은 평균 15분이 걸립니다. 서버 사양은 64 vCPU와 512 GiB 메모리입니다.

다음 중 운영 오버헤드를 최소화하면서 15분 이내에 배치 작업을 실행할 수 있는 솔루션은 무엇입니까?

A. 기능적 스케일링이 가능한 AWS Lambda를 사용합니다.
B. AWS Fargate를 사용하는 Amazon Elastic Container Service(Amazon ECS)를 사용합니다.
C. AWS Auto Scaling과 함께 Amazon Lightsail을 사용합니다.
D. Amazon EC2에서 AWS Batch를 사용합니다.

```
A company is migrating an old application to AWS. The application runs a batch job every hour and is CPU intensive. The batch job takes 15 minutes on average with an on-premises server. The server has 64 virtual CPU (vCPU) and 512 GiB of memory.  
  
Which solution will run the batch job within 15 minutes with the LEAST operational overhead?

- A. Use AWS Lambda with functional scaling.
- B. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.
- C. Use Amazon Lightsail with AWS Auto Scaling.
- D. Use AWS Batch on Amazon EC2.
```

정답 : `D`

- 요구 리소스를 단일 작업에 그대로 제공할 수 있는 인스턴스를 AWS Batch의 EC2 관리형 컴퓨팅 환경으로 손쉽게 확보 가능
- Batch는 작업 큐･스케줄링･자동확장/축소･재시도를 관리해 운영 오버헤드를 최소화하고, 시간당 실행(매시간)도 간단히 설정 가능
- 기존과 동일한 리소스급을 할당하면 15분 내 완료 가능성을 가장 높이면서 관리형 오케스트레이션을 얻을 수 있음

오답 이유

- **A. AWS Lambda**
    - Lambda는 최대 실행 시간이 15분이고, vCPU/메모리가 **64 vCPU/512 GiB 수준에 미달**합니다(단일 함수 최대 vCPU 수 제한). 단일 작업을 그대로 옮기기 어렵습니다.
    
- **B. ECS on Fargate**
    - Fargate 단일 태스크의 상한(최대 vCPU/메모리)이 요구 사양에 못 미칩니다. 작업을 쪼개 병렬화해야 하므로 **코드/아키텍처 변경**이 커집니다.
    
- **C. Lightsail + Auto Scaling**
    - Lightsail은 단순 워크로드용으로 인스턴스 스펙과 기능이 제한적입니다. **요구 리소스 충족과 HA/스케줄링** 측면에서 부적합합니다.


## #356
한 회사가 데이터를 Amazon S3 Standard 스토리지에 저장하고 있습니다. 솔루션스 아키텍트는 데이터의 75%가 30일 이후 거의 액세스되지 않는다는 사실을 발견했습니다. 회사는 모든 데이터를 동일한 높은 가용성과 내구성을 유지하면서 즉시 액세스할 수 있어야 하지만, 스토리지 비용은 최소화하고자 합니다.

이 요구 사항을 충족하는 스토리지 솔루션은 무엇입니까?

A. 30일 이후 데이터를 S3 Glacier Deep Archive로 이동합니다.  
B. 30일 이후 데이터를 S3 Standard-Infrequent Access(S3 Standard-IA)로 이동합니다.  
C. 30일 이후 데이터를 S3 One Zone-Infrequent Access(S3 One Zone-IA)로 이동합니다.  
D. 데이터를 즉시 S3 One Zone-Infrequent Access(S3 One Zone-IA)로 이동합니다.  

```
A company stores its data objects in Amazon S3 Standard storage. A solutions architect has found that 75% of the data is rarely accessed after 30 days. The company needs all the data to remain immediately accessible with the same high availability and resiliency, but the company wants to minimize storage costs.  
  
Which storage solution will meet these requirements?

- A. Move the data objects to S3 Glacier Deep Archive after 30 days.
- B. Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.
- C. Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.
- D. Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately.
```

정답 : `B`

- 요구사항
	- 즉시 액세스 가능해야함: Glacier 계열은 복원 지연 존재 → 부적합
	- 높은 가용성과 내구성 유지: One Zone-IA는 단일 AZ에만 저장 → 내구성은 높지만 가용성/내결함성 낮음
	- 비용 최적화: S3 Standard보다 저렴하면서 동일한 멀티 AZ 내구성 제공
- S3 Standard-IA는 S3 Standard와 동일한 가용성과 내구성을 제공하면서, 드물게 액세스하는 데이터를 저장할 떄 저렴

오답 이유

- **A. S3 Glacier Deep Archive**
    - 가장 저렴하지만, **복원 지연(분 단위~시간 단위)** 존재. “즉시 액세스” 요구 조건 불충족.
    
- **C. S3 One Zone-IA (30일 이후 이동)**
    - 단일 AZ에 저장하므로, **AZ 장애 시 데이터 접근 불가 위험**. “높은 가용성과 내결함성” 요구 불충족.
    
- **D. S3 One Zone-IA (즉시 이동)**
    - 마찬가지로 AZ 장애 위험 존재. 또한 처음 30일 동안은 데이터 접근 빈도가 높으므로, IA 적용 시 **비용이 오히려 증가**할 수 있음.


## #357
한 게임 회사가 공개 점수판을 데이터 센터에서 AWS 클라우드로 이전하고 있습니다. 회사는 동적 애플리케이션을 호스팅하기 위해 Application Load Balancer 뒤에 Amazon EC2 Windows Server 인스턴스를 사용합니다. 회사는 애플리케이션을 위한 고가용성 스토리지 솔루션이 필요합니다. 애플리케이션은 정적 파일과 동적 서버 측 코드로 구성됩니다.

이 요구 사항을 충족하기 위해 솔루션스 아키텍트는 어떤 단계 조합을 수행해야 합니까? (두 가지 선택)

A. 정적 파일을 Amazon S3에 저장합니다. Amazon CloudFront를 사용하여 객체를 엣지에서 캐시합니다.
B. 정적 파일을 Amazon S3에 저장합니다. Amazon ElastiCache를 사용하여 객체를 엣지에서 캐시합니다.
C. 서버 측 코드를 Amazon Elastic File System(Amazon EFS)에 저장합니다. 각 EC2 인스턴스에 EFS 볼륨을 마운트하여 파일을 공유합니다.
D. 서버 측 코드를 Amazon FSx for Windows File Server에 저장합니다. 각 EC2 인스턴스에 FSx for Windows File Server 볼륨을 마운트하여 파일을 공유합니다.
E. 서버 측 코드를 범용 SSD(gp2) Amazon Elastic Block Store(Amazon EBS) 볼륨에 저장합니다. 각 EC2 인스턴스에 EBS 볼륨을 마운트하여 파일을 공유합니다.

```
A gaming company is moving its public scoreboard from a data center to the AWS Cloud. The company uses Amazon EC2 Windows Server instances behind an Application Load Balancer to host its dynamic application. The company needs a highly available storage solution for the application. The application consists of static files and dynamic server-side code.  
  
Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.
- B. Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.
- C. Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS volume on each EC2 instance to share the files.
- D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the files.
- E. Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on each EC2 instance to share the files.
```

정답 : `A, D`

- 정적 콘텐츠는 S3 + CloudFront 조합으로 전 세계 엣지에서 고가용성･저지연 제공 가능하며 비용 효율적
- 서버 측 코드(공유 파일 시스템 필요)는 FSx for Windows File Server(SMB)가 Windows 워크로드에 최적의 완전관리형 고가용성 파일 서비스

오답 이유

- **B (S3 + ElastiCache)**: ElastiCache는 VPC 내부 인메모리 캐시이며 **엣지 캐시가 아님**. 전 세계 엣지 전송 최적화는 CloudFront가 담당.
    
- **C (EFS)**: EFS는 **NFS(Linux용)** 기반이며 Windows의 표준 SMB 워크로드에 권장되지 않습니다. Windows에는 **FSx for Windows** 가 맞습니다.
    
- **E (EBS 공유)**: EBS는 단일 인스턴스 전용(특수 Multi-Attach는 io1/io2, 같은 AZ 제한, Windows 파일시스템 공유 용도 아님). **여러 인스턴스 간 파일 공유에 부적합**합니다.


## #358
한 소셜 미디어 회사가 Application Load Balancer(ALB) 뒤의 Amazon EC2 인스턴스에서 애플리케이션을 실행합니다. ALB는 Amazon CloudFront 배포의 오리진입니다. 이 애플리케이션은 Amazon S3 버킷에 10억 개가 넘는 이미지를 저장하고 있으며, 매초 수천 개의 이미지를 처리합니다. 회사는 이미지를 동적으로 리사이즈하고 클라이언트에 적절한 포맷을 제공하고자 합니다.

다음 중 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 외부 이미지 관리 라이브러리를 EC2 인스턴스에 설치합니다. 이미지 관리 라이브러리를 사용하여 이미지를 처리합니다.
B. CloudFront 오리진 요청 정책을 생성합니다. 이 정책을 사용하여 이미지를 자동으로 리사이즈하고 요청의 User-Agent HTTP 헤더에 따라 적절한 포맷을 제공합니다.
C. 외부 이미지 관리 라이브러리를 사용하는 Lambda@Edge 함수를 사용합니다. 해당 Lambda@Edge 함수를 이미지를 제공하는 CloudFront 동작에 연결합니다.
D. CloudFront 응답 헤더 정책을 생성합니다. 이 정책을 사용하여 이미지를 자동으로 리사이즈하고 요청의 User-Agent HTTP 헤더에 따라 적절한 포맷을 제공합니다.

```
A social media company runs its application on Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. The application has more than a billion images stored in an Amazon S3 bucket and processes thousands of images each second. The company wants to resize the images dynamically and serve appropriate formats to clients.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Install an external image management library on an EC2 instance. Use the image management library to process the images.
- B. Create a CloudFront origin request policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.
- C. Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the CloudFront behaviors that serve the images.
- D. Create a CloudFront response headers policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.
```

정답 : `C`

- Lambda@Edge는 CloudFront의 엣지 로케이션에서 코드가 실행되어 요청/응답 경로에서 이미지 변환(리사이즈, 포맷 전환)을 수행 가능
- S3에 저장된 원본 이미지를 Sharp 같은 네이티브 이미지 라이브러리로 처리해 디바이스･브라우저에 맞게 WebP/AVIF/JPEG 등 포맷을 동적 제공 가능
- 엣지에서 처리/캐싱되므로 지연을 낮추고 오리진 부하를 줄이며, 서버를 직접 운영･스케일링 할 필요가 없어 운영 오버헤드 최소화
- AWS의 레퍼런스인 Serverless Image Handler 아키텍처도 CloudFront + Lambda(or Lambda@Edge) + S3 조합 사용

오답 이유

- **A. EC2에 라이브러리 설치**
    - 오토스케일, 패치, 용량 계획 등 **서버 운영 부담**이 큽니다. 초당 수천 건 처리 규모에서 관리 오버헤드가 증가.
    
- **B. CloudFront 오리진 요청 정책**
    - 헤더/쿠키/쿼리 전달을 제어하는 **메타데이터 정책**일 뿐, **이미지 변환 기능이 없음**.
    
- **D. CloudFront 응답 헤더 정책**
    - 응답 헤더를 추가/수정하는 정책일 뿐, **이미지 리사이즈/포맷 변환 불가**.


## #359
한 병원이 Amazon S3 버킷에 환자 기록을 저장해야 합니다. 병원의 컴플라이언스 팀은 모든 PHI(보호 건강 정보)가 전송 중과 저장 시 암호화되어야 함을 보장해야 합니다. 컴플라이언스 팀은 저장 데이터에 대한 암호화 키를 관리해야 합니다.

이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. AWS Certificate Manager(ACM)에서 공개 SSL/TLS 인증서를 생성합니다. 인증서를 Amazon S3에 연결합니다. 각 S3 버킷의 기본 암호화를 AWS KMS 키를 사용하는 서버 측 암호화(SSE-KMS)로 구성합니다. KMS 키 관리를 컴플라이언스 팀에 할당합니다.
B. S3 버킷 정책에서 aws:SecureTransport 조건을 사용하여 HTTPS(TLS)를 통한 암호화된 연결만 허용합니다. 각 S3 버킷의 기본 암호화를 S3 관리형 암호화 키(SSE-S3)를 사용하는 서버 측 암호화로 구성합니다. SSE-S3 키 관리를 컴플라이언스 팀에 할당합니다.
C. S3 버킷 정책에서 aws:SecureTransport 조건을 사용하여 HTTPS(TLS)를 통한 암호화된 연결만 허용합니다. 각 S3 버킷의 기본 암호화를 AWS KMS 키(SSE-KMS)를 사용하는 서버 측 암호화로 구성합니다. KMS 키 관리를 컴플라이언스 팀에 할당합니다.
D. S3 버킷 정책에서 aws:SecureTransport 조건을 사용하여 HTTPS(TLS)를 통한 암호화된 연결만 허용합니다. Amazon Macie를 사용하여 Amazon S3에 저장된 민감 데이터를 보호합니다. Macie 관리를 컴플라이언스 팀에 할당합니다.

```
A hospital needs to store patient records in an Amazon S3 bucket. The hospital’s compliance team must ensure that all protected health information (PHI) is encrypted in transit and at rest. The compliance team must administer the encryption key for data at rest.  
  
Which solution will meet these requirements?

- A. Create a public SSL/TLS certificate in AWS Certificate Manager (ACM). Associate the certificate with Amazon S3. Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.
- B. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with S3 managed encryption keys (SSE-S3). Assign the compliance team to manage the SSE-S3 keys.
- C. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.
- D. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Use Amazon Macie to protect the sensitive data that is stored in Amazon S3. Assign the compliance team to manage Macie.
```

정답 : `C`

- S3 버킷 정책의 aws:SecureTransport 조건으로 HTTPS(TLS) 요청만 허용해 전송 중 암호화를 강제
- SSE-KMS를 기본 암호화로 사용하면 KMS 고객 관리형 키(CMK)에 대해 키 정책/그랜트/로케이션 등을 통해 컴플라이언스 팀이 직접 키를 관리 가능
	- PHI 같은 규제 데이터에 적합

오답 이유

- **A**: ACM 인증서를 **S3에 직접 연결할 수 없습니다**(CloudFront 등 프런트 도메인에 연결). 또한 전송 중 암호화를 강제하는 가장 적절한 방식은 **버킷 정책의 SecureTransport** 조건입니다.
    
- **B**: 전송 중 암호화는 충족하지만 **SSE-S3는 S3 관리형 키**이며, 고객이 키를 직접 관리할 수 없습니다. “컴플라이언스 팀이 키를 관리” 요건 불충족.
    
- **D**: Macie는 **민감 데이터 식별/분류(DLP)** 서비스이며 **암호화 강제나 키 관리** 수단이 아닙니다. 요구사항과 불일치.


## #360
한 회사가 동일한 VPC에서 두 개의 REST API로 프라이빗 API Gateway를 운영하고 있습니다. BuyStock RESTful 웹 서비스는 주식을 구매하기 전에 충분한 자금이 있는지 확인하기 위해 CheckFunds RESTful 웹 서비스를 호출합니다. 회사는 VPC 플로우 로그에서 BuyStock RESTful 웹 서비스가 VPC를 통해서가 아니라 인터넷을 통해 CheckFunds RESTful 웹 서비스를 호출하고 있음을 발견했습니다. 솔루션스 아키텍트는 API들이 VPC를 통해 통신하도록 하는 솔루션을 구현해야 합니다.

가장 적은 코드 변경으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 권한 부여를 위해 HTTP 헤더에 X-API-Key 헤더를 추가합니다.
B. 인터페이스 엔드포인트를 사용합니다.
C. 게이트웨이 엔드포인트를 사용합니다.
D. 두 REST API 사이에 Amazon Simple Queue Service(Amazon SQS) 큐를 추가합니다.

```
A company uses Amazon API Gateway to run a private gateway with two REST APIs in the same VPC. The BuyStock RESTful web service calls the CheckFunds RESTful web service to ensure that enough funds are available before a stock can be purchased. The company has noticed in the VPC flow logs that the BuyStock RESTful web service calls the CheckFunds RESTful web service over the internet instead of through the VPC. A solutions architect must implement a solution so that the APIs communicate through the VPC.  
  
Which solution will meet these requirements with the FEWEST changes to the code?

- A. Add an X-API-Key header in the HTTP header for authorization.
- B. Use an interface endpoint.
- C. Use a gateway endpoint.
- D. Add an Amazon Simple Queue Service (Amazon SQS) queue between the two REST APIs.
```

정답 : `B`

- API Gateway의 프라이빗 REST API는 VPC 내부에서만 접근 가능하며, 이를 위해 Interface VPC Endpoint(AWS PrivateLink)를 사용
- VPC에 Interface Endpoint + Private DNS 사용 설정 후, 프라이빗 API 호출을 해당 엔드포인트로 라우팅하면 트래픽이 인터넷 대신 VPC 내부로 흐름

오답 이유

- **A. X-API-Key 헤더 추가**
    - 이는 **인증/권한 부여** 수단일 뿐, **트래픽 경로(인터넷 vs VPC)** 를 바꾸지 못합니다.
    
- **C. 게이트웨이 엔드포인트 사용**
    - 게이트웨이 엔드포인트는 **S3, DynamoDB 전용**입니다. API Gateway 프라이빗 API에는 적용되지 않습니다.
    
- **D. SQS 큐 추가**
    - 아키텍처와 코드 변경이 커지며, **동기 REST 호출 패턴**을 **비동기 메시징**으로 바꾸는 것은 요구사항 범위를 벗어납니다.