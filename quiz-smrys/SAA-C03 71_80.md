---
created: 2025-09-26 09:29:34
last_modified: 2025-09-26 18:43:15
---
## #71
한 회사는 고객 정보를 저장하기 위해 Amazon DynamoDB를 사용하는 쇼핑 애플리케이션을 운영합니다.  
데이터 손상(corruption) 발생 시, 솔루션 설계자는 다음 목표치를 만족하는 복구 설계를 해야 합니다:
- RPO(복구 시점 목표): 15분  
- RTO(복구 시간 목표): 1시간  

이 요구사항을 충족하려면 솔루션 설계자는 무엇을 권장해야 합니까?

A. DynamoDB 글로벌 테이블을 구성합니다. RPO 복구 시 애플리케이션을 다른 AWS 리전으로 포인팅합니다.  
B. DynamoDB 시점 복구(Point-in-Time Recovery, PITR)를 구성합니다. RPO 복구 시 원하는 시점으로 복원합니다.  
C. DynamoDB 데이터를 매일 Amazon S3 Glacier로 내보냅니다. RPO 복구 시 S3 Glacier에서 DynamoDB로 데이터를 가져옵니다.  
D. DynamoDB 테이블에 대해 15분마다 Amazon EBS 스냅샷을 예약합니다. RPO 복구 시 EBS 스냅샷을 사용하여 DynamoDB 테이블을 복원합니다.

```
A company runs a shopping application that uses Amazon DynamoDB to store customer information. In case of data corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour.  
What should the solutions architect recommend to meet these requirements?

- A. Configure DynamoDB global tables. For RPO recovery, point the application to a different AWS Region.
- B. Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.
- C. Export the DynamoDB data to Amazon S3 Glacier on a daily basis. For RPO recovery, import the data from S3 Glacier to DynamoDB.
- D. Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every 15 minutes. For RPO recovery, restore the DynamoDB table by using the EBS snapshot.
```

정답 : `B`

- RPO 15분 요구는 손상 발생 시 그 시점에서 15분 이내의 상태로 복수할 수 있어야 한다는 뜻
- DynamoDB의 PITR(Point-in-Time Recovery)은 마지막 35일 범위 내에서 초 단위로 원하는 시점으로 복원할 수 있음
- 복원 절차는 기존(손상된) 테이블을 덮어쓰지 않고 새 테이블로 복원한 뒤 애플리케이션의 연결을 새 테이블로 전환(컷오버)하는 방식으로 진행 -> 데이터 손실을 최소화하고 안전하게 복구 가능
- RTO 1시간도 현실적으로 달성 가능
	- 실제 복원 시간은 테이블 크기와 항목 수에 따라 다름
	- 복원 시간을 사전 테스트하여 1시간 이내 복구 가능한지 검증 필요
	- 일반적으로 PITR 복원은 대규모 테이블에서도 수십 분에서 한 시간 이내에 안료되는 경우가 많음
- PITR은 관리형 기능으로 활성화만 하면 지속적으로 데이터 변경을 백업하므로 가장 적은 운영 노력 -> 운영 부담이 적음

오답 이유

- **A. DynamoDB 글로벌 테이블 — 오답**
    - 글로벌 테이블은 여러 리전 간에 쓰기/읽기 복제를 제공하므로 리전 장애에 대비할 수 있으나, **데이터 손상(corruption)이 한 리전에서 발생하면 그 변경이 다른 리전으로도 복제되어 전역으로 전파될 수 있음**. 즉, 손상 시점을 롤백하는 기능을 제공하지 않으므로 RPO(15분) 요구를 보장하지 못합니다.
    - 글로벌 테이블만으로는 “특정 시점으로 되돌리기”가 불가능하며, 손상된 데이터를 수동으로 조치해야 할 수 있어 운영 부담이 큽니다.
    
- **C. S3 Glacier로 일일 익스포트 — 오답**
    - 하루 단위(매일) 백업은 **RPO가 최대 24시간**이 되어 15분 목표를 크게 초과합니다. 또한 Glacier에서 복원하는 데 시간(수 분 ~ 시간)이 더 걸리므로 RTO 1시간 보장도 어렵습니다.
    
- **D. EBS 스냅샷 사용 — 오답**
    - DynamoDB는 관리형 NoSQL 서비스로서 **EBS 스냅샷을 통한 백업/복원 방식이 적용되지 않음(구현 불가)**. EBS 스냅샷은 EC2 인스턴스의 블록 스토리지에만 해당하므로 이 옵션은 기술적으로 부적절합니다.


## #72
한 회사는 동일한 AWS 리전에 위치한 Amazon S3 버킷에 사진을 자주 업로드하고 다운로드해야 하는 사진 처리 애플리케이션을 실행합니다.  
솔루션 설계자는 데이터 전송 요금의 증가를 발견했으며 이러한 비용을 절감할 수 있는 솔루션을 구현해야 합니다.  
솔루션 설계자는 이 요구사항을 어떻게 충족할 수 있습니까?

- A. Amazon API Gateway를 퍼블릭 서브넷에 배포하고 라우트 테이블을 조정하여 S3 호출을 이를 통해 라우팅합니다.  
- B. 퍼블릭 서브넷에 NAT 게이트웨이를 배포하고 S3 버킷에 대한 액세스를 허용하는 엔드포인트 정책을 연결합니다.  
- C. 애플리케이션을 퍼블릭 서브넷에 배포하고 인터넷 게이트웨이를 통해 S3 버킷에 접근하도록 허용합니다.  
- D. VPC에 S3 VPC 게이트웨이 엔드포인트를 배포하고 S3 버킷에 대한 액세스를 허용하는 엔드포인트 정책을 연결합니다.

```
A company runs a photo processing application that needs to frequently upload and download pictures from Amazon S3 buckets that are located in the same AWS Region. A solutions architect has noticed an increased cost in data transfer fees and needs to implement a solution to reduce these costs.  
How can the solutions architect meet this requirement?

- A. Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls through it.
- B. Deploy a NAT gateway into a public subnet and attach an endpoint policy that allows access to the S3 buckets.
- C. Deploy the application into a public subnet and allow it to route through an internet gateway to access the S3 buckets.
- D. Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to the S3 buckets.
```

정답 : `D`

- 현재 데이터 전송 비용이 증가한 주된 원인은 인스턴스가 S3에 접근할 때 인터넷/NAT 경로를 통해 나가면서 발생하는 egress 또는 NAT 처리 요금일 가능성이 높음
- S3 VPC 게이트웨이 엔드포인트를 사용하면 VPC 내부에서 S3로 가는 트래픽이 퍼블릭 인터넷, 인터넷 게이트웨이(IGW), 또는 NAT 게이트웨이를 통하지 않고 AWS 네트워크 내부에서 직접 처리
- 게이트웨이 엔드포인트를 사용하면 이러한 경로를 우회하여 NAT/IGW 관련 데이터 전송비(및 NAT 처리비)를 피할 수 있음
- 엔드포인트 정책을 통해 특정 버킷만 허용하도록 제한할 수 있어 보안도 강화, 구현이 간단하고 운영 오버헤드가 거의 없음

오답 이유

- **A. API Gateway를 퍼블릭 서브넷에 배포하고 S3 호출을 라우팅** — 오답
    - API Gateway는 HTTP API/REST API를 위한 관리형 서비스이며 S3 트래픽을 중계하도록 설계된 방법이 아닙니다. API Gateway를 통해 S3로 라우팅하면 추가 요청/데이터 처리 비용 및 복잡성만 증가합니다. 데이터 전송 비용 절감과는 무관하거나 오히려 비용이 증가할 수 있습니다.
    
- **B. NAT 게이트웨이 배포 + 엔드포인트 정책 연결** — 오답
    - NAT 게이트웨이를 거치면 프라이빗 인스턴스의 아웃바운드 트래픽이 인터넷 경로(또는 퍼블릭 서비스 경로)를 통해 전달되므로 **NAT 게이트웨이 시간당/GB당 요금**이 발생합니다. NAT을 사용하면 오히려 비용이 더 늘어날 가능성이 높습니다. (또한 NAT 게이트웨이에는 엔드포인트 정책 같은 개념이 직접 적용되지 않습니다.)
    
- **C. 애플리케이션을 퍼블릭 서브넷에 두고 IGW 경유 접근** — 오답
    - 인터넷 게이트웨이를 통한 접근은 인터넷 egress 식으로 처리되어 **데이터 전송 비용이 발생**합니다. 퍼블릭으로 두면 보안 노출 위험도 커지고 비용 절감 목적에 반합니다.

## #73
한 회사는 최근 VPC의 퍼블릭 서브넷에 Linux 기반의 배스천(중간) 호스트를 Amazon EC2 인스턴스로 실행하고, 프라이빗 서브넷에 Linux 기반 애플리케이션 인스턴스를 Amazon EC2에 실행했습니다.  
솔루션 설계자는 온프레미스 네트워크에서 회사의 인터넷 연결을 통해 배스천 호스트로 접속하고, 배스천 호스트를 통해 애플리케이션 서버에 접속해야 합니다. 솔루션 설계자는 모든 EC2 인스턴스의 보안 그룹이 해당 접근을 허용하도록 보장해야 합니다.  
이 요구사항을 충족하려면 어떤 조치 조합을 취해야 합니까? (두 가지 선택)

A. 배스천 호스트의 현재 보안 그룹을 애플리케이션 인스턴스에서만 인바운드 액세스를 허용하는 보안 그룹으로 교체합니다.  
B. 배스천 호스트의 현재 보안 그룹을 회사의 내부 IP 범위에서만 인바운드 액세스를 허용하는 보안 그룹으로 교체합니다.  
C. 배스천 호스트의 현재 보안 그룹을 회사의 외부(퍼블릭) IP 범위에서만 인바운드 액세스를 허용하는 보안 그룹으로 교체합니다.  
D. 애플리케이션 인스턴스의 현재 보안 그룹을 배스천 호스트의 프라이빗 IP 주소에서만 SSH 인바운드 액세스를 허용하는 보안 그룹으로 교체합니다.  
E. 애플리케이션 인스턴스의 현재 보안 그룹을 배스천 호스트의 퍼블릭 IP 주소에서만 SSH 인바운드 액세스를 허용하는 보안 그룹으로 교체합니다.

```
A company recently launched Linux-based application instances on Amazon EC2 in a private subnet and launched a Linux-based bastion host on an Amazon EC2 instance in a public subnet of a VPC. A solutions architect needs to connect from the on-premises network, through the company's internet connection, to the bastion host, and to the application servers. The solutions architect must make sure that the security groups of all the EC2 instances will allow that access.  
Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)

- A. Replace the current security group of the bastion host with one that only allows inbound access from the application instances.
- B. Replace the current security group of the bastion host with one that only allows inbound access from the internal IP range for the company.
- C. Replace the current security group of the bastion host with one that only allows inbound access from the external IP range for the company.
- D. Replace the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host.
- E. Replace the current security group of the application instances with one that allows inbound SSH access from only the public IP address of the bastion host.
```

정답 : `C, D`

- 트래픽 흐름은 온프레미스 -> 회사 인터넷을 통해 퍼블릭 IP(또는 공인 주소)를 사용한 배스천 호스트 -> VPC 내부의 애플리케이션(프라이빗 IP)
- 퍼블릭에서 배스천으로 접속을 허용하려면 배스천의 보안 그룹이 회사(온프레미스)의 외부(공인) IP 범위만 인바운드로 허용하도록 설정
- 배스천에서 프라이빗 서브넷의 애플리케이션으로 SSH 연결할 때는 소스가 배스천의 프라이빗 IP(또는 배스천의 보안 그룹)으로 나타나므로 애플리케이션 인스턴스의 보안 그룹은 배스천의 프라이빗 IP에서만 SSH 허용하도록 제한


오답 이유

- **A. 배스천 보안 그룹을 애플리케이션 인스턴스에서만 허용하도록 교체 — 오답**
    - 배스천은 외부(온프레미스)에서 접근해야 하는 퍼블릭 호스트입니다. 배스천의 보안 그룹을 애플리케이션 인스턴스에서만 접근 허용하도록 바꾸면 외부에서 배스천으로 SSH할 수 없게 되어 요구사항을 충족하지 않습니다.
    
- **B. 배스천 보안 그룹을 회사의 내부 IP 범위(사설 주소)만 허용하도록 교체 — 오답**
    - 온프레미스에서 접속할 때는 공인 IP(또는 회사의 인터넷 출발지 공인 주소)를 사용하여 인터넷을 통해 접속합니다. 배스천이 내부(사설) IP 대역만 허용하면 외부 인터넷에서 접근 불가합니다. (내부 IP 범위는 VPC 내/온프레 내부 네트워크용이며, 공인 인터넷 트래픽 출처와 다릅니다.)
    
- **E. 애플리케이션 보안 그룹을 배스천의 퍼블릭 IP에서만 SSH 허용하도록 교체 — 오답**
    - 배스천이 내부 대상으로 애플리케이션에 접속할 때 소스 IP는 퍼블릭 IP가 아니라 **사설 IP**(VPC 내부 주소)이므로, 퍼블릭 IP를 허용하면 접속이 차단됩니다. 또한 퍼블릭 IP 허용은 보안상 바람직하지 않음(패킷 출발지를 신뢰할 수 없음).


## #74
솔루션 설계자는 2계층 웹 애플리케이션을 설계하고 있습니다. 애플리케이션은 퍼블릭 서브넷의 Amazon EC2에서 호스트되는 퍼블릭용 웹 계층으로 구성됩니다. 데이터베이스 계층은 프라이빗 서브넷에서 Amazon EC2에서 실행되는 Microsoft SQL Server로 구성됩니다. 회사는 보안을 매우 중요하게 생각합니다.
이 상황에서 보안 그룹을 어떻게 구성해야 합니까? (두 가지 선택)

A. 웹 계층의 보안 그룹을 구성하여 포트 443의 인바운드 트래픽을 0.0.0.0/0에서 허용합니다.  
B. 웹 계층의 보안 그룹을 구성하여 포트 443의 아웃바운드 트래픽을 0.0.0.0/0에서 허용합니다.  
C. 데이터베이스 계층의 보안 그룹을 구성하여 포트 1433의 인바운드 트래픽을 웹 계층의 보안 그룹에서 허용합니다.  
D. 데이터베이스 계층의 보안 그룹을 구성하여 포트 443 및 1433의 아웃바운드 트래픽을 웹 계층의 보안 그룹으로 허용합니다.  
E. 데이터베이스 계층의 보안 그룹을 구성하여 포트 443 및 1433의 인바운드 트래픽을 웹 계층의 보안 그룹에서 허용합니다.

```
A solutions architect is designing a two-tier web application. The application consists of a public-facing web tier hosted on Amazon EC2 in public subnets. The database tier consists of Microsoft SQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the company.  
How should security groups be configured in this situation? (Choose two.)

- A. Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.
- B. Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0.
- C. Configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier.
- D. Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group for the web tier.
- E. Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from the security group for the web tier.
```

정답 : `A, C`

- 웹 계층은 외부 사용자로부터 HTTPS로 접근받아야 하므로 웹 계층 보안 그룹에서 인바운드 포트 443을 0.0.0.0/0으로 허용
- 데이터베이스는 프라이빗 서브넷에 있고 외부에서 직접 노출되서는 안됨 -> 웹 계층에서만 DB에 접속하도록 제한하려면 DB 보안 그룹의 인바운드 규칙에 웹 계층의 보안 그룹을 소스로 하여 포트 1443(MSSQL)을 허용
- 보안 그룹을 소스로 지정하면 웹 계층 인스턴스들의 동적 IP 변경에도 규칙이 유효

오답 이유

- **B. 웹 계층의 보안 그룹을 포트 443 아웃바운드로 0.0.0.0/0 허용 — 오답**
    - 보안 그룹은 상태 저장(stateful)이므로, 웹 계층에서 클라이언트로의 응답 트래픽은 별도 아웃바운드 규칙 없이도 허용됩니다(요청이 들어오면 응답이 자동 허용). 보통 웹 서버는 아웃바운드 모든 트래픽을 허용해도 무방하지만, 문제의 목적(웹에서 들어오는 HTTPS 허용)을 달성하려면 인바운드 규칙이 핵심입니다. 따라서 B는 필수적 규칙이 아니며 정답으로 적절치 않습니다.
    
- **D. DB 보안 그룹에서 포트 443 및 1433의 아웃바운드 트래픽을 웹 계층 SG로 허용 — 오답**
    - 데이터베이스는 일반적으로 클라이언트(웹 서버)로 연결을 받는 역할이며, DB에서 웹 서버로 아웃바운드(특히 HTTPS 포트 443)로 연결할 필요가 없습니다. 또한 보안 그룹 규칙은 주로 인바운드 측에서 출발지(SG)를 제한하는 방식으로 설계합니다. DB가 웹으로 연결을 시작할 필요가 없다면 이 규칙은 불필요하며 보안 원칙에도 맞지 않습니다.
    
- **E. DB 보안 그룹에서 포트 443 및 1433의 인바운드를 웹 계층 SG에서 허용 — 오답**
    - 포트 1433은 적절하지만 포트 443(HTTPS)은 DB에 대해 일반적으로 필요하지 않습니다. DB에 443을 허용하는 것은 불필요한 노출을 초래하므로 최소 권한 원칙에 위배됩니다. 따라서 E는 과하게 열려 있는 규칙을 포함하고 있어 부적절합니다.


### #75
한 회사는 애플리케이션의 성능을 개선하기 위해 온프레미스에서 AWS 클라우드로 다계층(Multi-tier) 애플리케이션을 이전하려고 합니다.  
애플리케이션은 서로 RESTful 서비스로 통신하는 애플리케이션 계층으로 구성됩니다. 한 계층이 과부하되면 트랜잭션이 손실(drop)됩니다. 솔루션 설계자는 이러한 문제를 해결하고 애플리케이션을 모던화해야 합니다.  
어떤 솔루션이 이러한 요구사항을 충족하고 가장 운영적으로 효율적입니까?

A. Amazon API Gateway를 사용하고 트랜잭션을 애플리케이션 계층으로 AWS Lambda 함수로 직접 라우팅합니다. 애플리케이션 서비스 간 통신 계층으로 Amazon Simple Queue Service(Amazon SQS)를 사용합니다.  
B. Amazon CloudWatch 지표를 사용하여 성능 실패 시 서버의 피크 사용률을 분석합니다. 애플리케이션 서버의 Amazon EC2 인스턴스 크기를 피크 요구사항을 충족하도록 증가시킵니다.  
C. Amazon Simple Notification Service(Amazon SNS)를 사용하여 Auto Scaling 그룹에서 실행되는 Amazon EC2의 애플리케이션 서버 간 메시징을 처리합니다. Amazon CloudWatch를 사용하여 SNS 대기열 길이를 모니터링하고 필요에 따라 스케일업/다운합니다.  
D. Amazon Simple Queue Service(Amazon SQS)를 사용하여 Auto Scaling 그룹에서 실행되는 Amazon EC2의 애플리케이션 서버 간 메시징을 처리합니다. Amazon CloudWatch를 사용하여 SQS 대기열 길이를 모니터링하고 통신 실패가 감지되면 스케일 업합니다.

```
A company wants to move a multi-tiered application from on premises to the AWS Cloud to improve the application's performance. The application consists of application tiers that communicate with each other by way of RESTful services. Transactions are dropped when one tier becomes overloaded. A solutions architect must design a solution that resolves these issues and modernizes the application.  
Which solution meets these requirements and is the MOST operationally efficient?

- A. Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the application layer. Use Amazon Simple Queue Service (Amazon SQS) as the communication layer between application services.
- B. Use Amazon CloudWatch metrics to analyze the application performance history to determine the servers' peak utilization during the performance failures. Increase the size of the application server's Amazon EC2 instances to meet the peak requirements.
- C. Use Amazon Simple Notification Service (Amazon SNS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the SNS queue length and scale up and down as required.
- D. Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the SQS queue length and scale up when communication failures are detected.
```


정답 : `A`

- 핵심 문제 : 계층 간 동기 호출에서 한 계층이 과부하되면 트랜잭션 손실 -> 계층 간 비동기적 완충(buffering)과 자동 확장이 가능한 처리 구조 필요
- API Gateway -> Lambda로 요청을 서버리스로 처리해 운영 부담을 줄임
- 서비스 간의 통신은 SQS로 비동기화하여 버퍼링과 내구성 확보
- SQS는 메시지를 안정적으로 보관하고 소비자(람다)가 준비되면 처리하게 하므로 과부하 시에도 트랜잭션 유실 방지
- 람다와 SQS 결합은 자동 확장과 pay-per-use 요금 모델을 제공해 운영 효율과 비용 효율성에서 우수
- SQS는 DLQ, 가시성 타임아웃, 메시지 재시도 정책, FIFO 등 생산성 신뢰성 기능을 제공해 모던 아키텍처를 구성할 수 있음

오답 이유

- **B. CloudWatch로 과거 피크를 분석하고 EC2 크기를 늘림 — 오답**
    - 수직 확장(인스턴스 크기 증가)은 일시적인 완화책일 뿐이며, 트래픽 패턴이 변화하거나 갑작스런 스파이크가 발생하면 여전히 과부하와 트랜잭션 손실이 발생할 수 있습니다.
    - 또한 수직 확장은 **운영·비용 부담**이 크고 모던화(서버리스/마이크로서비스로의 전환) 요구를 충족하지 못합니다.
    - 근본적 해결책(비동기 완충 + 자동 확장) 대신 부하에 맞춘 크기 증설은 비효율적입니다.
    
- **C. SNS로 메시징하고 EC2 Auto Scaling 사용 — 오답**
    - SNS는 **pub/sub**(브로드캐스트) 모델로, 다수 구독자에게 동시 전달이 필요할 때 유리합니다. 그러나 **과부하 시 메시지 보유/버퍼링 기능이 SQS만큼 적합하지 않으며**, 메시지 순서 보장·재시도 제어·DLQ 등을 SNS만으로 구현하기 어렵습니다.
    - 또한 “SNS 대기열 길이”를 모니터링한다는 서술 자체가 부정확합니다(실제로는 SQS에 해당하는 개념). SNS는 큐 길이 지표를 제공하지 않으며, EC2 인스턴스 기반 처리 시 운영 부담이 높습니다.
    
- **D. SQS + EC2 Auto Scaling (Queue-based scaling) — 부분적으로 타당하나 오답**
    - 이 옵션은 **디커플링(비동기화)**을 제공하므로 트랜잭션 유실을 막는다는 관점에서는 타당합니다. SQS 큐 길이를 기반으로 Auto Scaling을 트리거하면 과부하에 대응할 수 있습니다.
    - 그러나 **운영 효율성 측면(AWS 관리형 서버리스 대비)**에서 보면 EC2 인스턴스와 Auto Scaling을 계속 관리해야 하므로 A보다 운영 부담이 큽니다.
    - 문제는 “가장 운영적으로 효율적”을 요구하므로 **관리 부담이 적은 서버리스(Lambda)** 를 사용하는 A가 더 우수합니다.


## #76
한 회사는 공장 한 곳에 위치한 여러 장비로부터 매일 10TB의 계측(instrumentation) 데이터를 수신합니다. 데이터는 공장 내 온프레미스 데이터 센터의 SAN에 JSON 파일로 저장됩니다. 회사는 이 데이터를 Amazon S3로 전송하여 여러 추가 시스템이 실시간에 가깝게 분석할 수 있도록 하려 합니다. 데이터는 민감하므로 전송 보안이 중요합니다.
어떤 솔루션이 가장 신뢰성 높은 데이터 전송을 제공합니까?

A. 공용 인터넷을 통한 AWS DataSync  
B. AWS Direct Connect를 통한 AWS DataSync  
C. 공용 인터넷을 통한 AWS Database Migration Service(AWS DMS)  
D. AWS Database Migration Service(AWS DMS)를 AWS Direct Connect를 통해

```
A company receives 10 TB of instrumentation data each day from several machines located at a single factory. The data consists of JSON files stored on a storage area network (SAN) in an on-premises data center located within the factory. The company wants to send this data to Amazon S3 where it can be accessed by several additional systems that provide critical near-real-time analytics. A secure transfer is important because the data is considered sensitive.  
Which solution offers the MOST reliable data transfer?

- A. AWS DataSync over public internet
- B. AWS DataSync over AWS Direct Connect
- C. AWS Database Migration Service (AWS DMS) over public internet
- D. AWS Database Migration Service (AWS DMS) over AWS Direct Connect
```

정답 : `B`

- 요구사항
	- 매일 10TB
	- 파일(JSON) 기반 전송
	- 민감 데이터이므로 보안/신뢰성 우선
	- 거의 실시간 분석
- AWS DataSync는 파일(SAN/NFS/SMB 등)에서 아마존 S3로 대량의 파일을 안정적으로, 고속으로 전송하도록 설계된 관리형 서비스
	- 병령 전송, 자동 재시도, 데이터 유효성 검사(무결성 검증), 소스/대상 간의 차이점만 전송하는 증분 전송 등 신뢰성과 성능 제공
- Direct Connect는 온프레미스와 AWS 간의 전용 네트워크 회선으로 공용 인터넷을 통과하지 않으므로 대역폭이 크고 지연과 변동성이 낮음. 보안과 일관성 면에서도 우수
	- 하루 10TB에 달하는 지속적 고용량을 전송하려면 공용 인터넷보다 Direct Connect가 더 안정적이고 비용 측면에서도 유리

오답 이유

- **A. AWS DataSync over public internet — 오답(부분적 타당성 있으나 최선 아님)**
    - DataSync 자체는 적합하지만 **공용 인터넷 경로**는 대규모 지속 전송(10TB/일)에서 지연·변동성·보안(인터넷 경로 노출) 리스크가 커지고, 전송 실패/성능 저하 가능성이 큽니다. 튼튼한 암호화(TLS)는 제공되나, 고객이 “보안·신뢰성 우선”을 명시했으므로 Direct Connect가 더 적합합니다.
    
- **C. AWS DMS over public internet — 오답**
    - DMS는 **데이터베이스(테이블/레코드) 복제/마이그레이션**에 특화된 서비스이며, 파일 시스템(SAN에 있는 JSON 파일) 전송용으로 적절치 않습니다. 또한 공용 인터넷을 경유하면 위와 같은 네트워크 신뢰성 문제가 있습니다.
    
- **D. AWS DMS over Direct Connect — 오답**
    - Direct Connect는 적합하지만 **DMS는 파일(오브젝트) 전송 목적이 아님**. 온프레 파일 시스템 → S3 로 대용량 파일을 옮기는 시나리오에는 DataSync가 올바른 선택입니다.


## #77
한 회사는 애플리케이션을 위해 실시간 데이터 수집(ingestion) 아키텍처를 구성해야 합니다. 회사는 데이터가 스트리밍되는 동안 데이터를 변환하는 프로세스를 위한 API, 그리고 데이터를 저장할 솔루션이 필요합니다.
어떤 솔루션이 최소한의 운영 오버헤드로 이러한 요구사항을 충족할 수 있습니까?

A. Amazon EC2 인스턴스를 배포하여 데이터를 Amazon Kinesis 데이터 스트림으로 전송하는 API를 호스팅합니다. Kinesis 데이터 스트림을 데이터 소스로 사용하는 Amazon Kinesis Data Firehose 전달 스트림을 생성합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다. Kinesis Data Firehose 전달 스트림을 사용하여 데이터를 Amazon S3로 전송합니다.

B. Amazon EC2 인스턴스를 배포하여 데이터를 AWS Glue로 전송하는 API를 호스팅합니다. EC2 인스턴스에서 소스/대상 검사를 중지합니다. AWS Glue를 사용하여 데이터를 변환하고 Amazon S3로 전송합니다.

C. Amazon API Gateway API를 구성하여 데이터를 Amazon Kinesis 데이터 스트림으로 전송합니다. Kinesis 데이터 스트림을 데이터 소스로 사용하는 Amazon Kinesis Data Firehose 전달 스트림을 생성합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다. Kinesis Data Firehose 전달 스트림을 사용하여 데이터를 Amazon S3로 전송합니다.

D. Amazon API Gateway API를 구성하여 데이터를 AWS Glue로 전송합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다. AWS Glue를 사용하여 데이터를 Amazon S3로 전송합니다.

```
A company needs to configure a real-time data ingestion architecture for its application. The company needs an API, a process that transforms data as the data is streamed, and a storage solution for the data.  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.
- B. Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Stop source/destination checking on the EC2 instance. Use AWS Glue to transform the data and to send the data to Amazon S3.
- C. Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.
- D. Configure an Amazon API Gateway API to send data to AWS Glue. Use AWS Lambda functions to transform the data. Use AWS Glue to send the data to Amazon S3.
```


정답 : `C`

- 요구사항
	- 실시간 데이터 수집(ingestion)
	- 데이터 변환 및 저장
	- 운영 오버헤드 최소화
- API Gateway는 서버리스 API 엔드포인트를 제공하여 EC2를 직접 운영할 필요가 없으므로 운영 오버헤드가 낮음
- Amazon Kinesis Data Streams + Kinesis Data Firehose 조합은 실시간 데이터 스트리밍과 변환 및 S3로의 안정적인 저장을 제공
- 람다를 사용하면 Firehose 내에서 실시간으로 데이터 변환 가능 -> 서버를 직접 관리하지 않아도 됨

오답 이유

- **A. EC2를 호스팅하여 API를 운영**
    - EC2를 직접 운영하면 서버 관리, 패치, 확장 등 운영 오버헤드가 발생합니다. 문제에서 최소 운영 오버헤드를 요구하므로 부적합합니다.
    
- **B. EC2 + AWS Glue 직접 연동**
    - Glue는 ETL(batch) 중심 서비스로 실시간 스트리밍 데이터 처리에 최적화되어 있지 않습니다. 또한 EC2 API 호스팅으로 운영 오버헤드가 발생합니다.
    
- **D. API Gateway → AWS Glue + Lambda transform → S3**
    - Glue는 서버리스지만 스트리밍 데이터 처리에 Glue Streaming을 사용하는 경우 설정이 복잡하고 Firehose 대비 운영 및 지연 측면에서 불리합니다. Firehose는 실시간 S3 적재에 최적화되어 있습니다.

## #78
한 회사는 사용자 거래 데이터를 Amazon DynamoDB 테이블에 보관해야 합니다. 회사는 데이터를 7년 동안 보존해야 합니다.
이 요구사항을 가장 운영 효율적으로 충족하는 솔루션은 무엇입니까?

A. DynamoDB 포인트 인 타임 복구(Point-in-Time Recovery)를 사용하여 테이블을 지속적으로 백업합니다.

B. AWS Backup을 사용하여 테이블에 대한 백업 일정과 보존 정책을 생성합니다.

C. DynamoDB 콘솔을 사용하여 테이블의 온디맨드 백업을 생성합니다. 백업을 Amazon S3 버킷에 저장합니다. S3 버킷에 대해 S3 수명 주기(Lifecycle) 구성을 설정합니다.

D. Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성하여 AWS Lambda 함수를 호출합니다. Lambda 함수를 구성하여 테이블을 백업하고 Amazon S3 버킷에 백업을 저장하도록 합니다. S3 버킷에 대해 S3 수명 주기(Lifecycle) 구성을 설정합니다.

```
A company needs to keep user transaction data in an Amazon DynamoDB table. The company must retain the data for 7 years.  
What is the MOST operationally efficient solution that meets these requirements?

- A. Use DynamoDB point-in-time recovery to back up the table continuously.
- B. Use AWS Backup to create backup schedules and retention policies for the table.
- C. Create an on-demand backup of the table by using the DynamoDB console. Store the backup in an Amazon S3 bucket. Set an S3 Lifecycle configuration for the S3 bucket.
- D. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to invoke an AWS Lambda function. Configure the Lambda function to back up the table and to store the backup in an Amazon S3 bucket. Set an S3 Lifecycle configuration for the S3 bucket.
```

정답 : `B`

- 요구사항
	- DynamoDB 테이블 데이터를 7년 동안 보존
	- 운영 효율성(자동화, 최소 수동 작업)
- AWS Backup은 DynamoDB를 포함한 여러 AWS 서비스에 대해 자동 백업 일정과 장기 보존 정책을 구성 가능

오답 이유

- **A. DynamoDB 포인트 인 타임 복구(PITR)**
    - PITR은 **35일 동안만** 데이터를 복원 가능하며, 7년 장기 보존 요구사항을 충족하지 못합니다.
    - 따라서 장기 보존에는 적합하지 않습니다.
    
- **C. DynamoDB 온디맨드 백업 + S3 저장 + 수명주기**
    - 온디맨드 백업은 수동으로 생성해야 하므로 장기 보존 시 운영 오버헤드가 높습니다.
    - 7년 동안 반복적으로 수동 백업을 관리해야 하므로 자동화 요구사항에 부적합합니다.
    
- **D. EventBridge + Lambda + S3 수명주기**
    - 가능은 하지만, Lambda 스케줄링, 에러 처리, 모니터링 등 추가 관리가 필요하므로 운영 효율성이 낮습니다.

## #79
한 회사는 데이터 저장을 위해 Amazon DynamoDB 테이블을 사용할 계획입니다. 회사는 비용 최적화에 대해 우려하고 있습니다. 테이블은 대부분의 아침 시간에는 사용되지 않을 예정입니다. 저녁에는 읽기 및 쓰기 트래픽이 종종 예측할 수 없습니다. 트래픽이 급증할 경우 매우 빠르게 발생합니다.
솔루션 설계자는 무엇을 권장해야 합니까?

A. 온디맨드 용량 모드로 DynamoDB 테이블을 생성합니다.

B. 글로벌 보조 인덱스를 가진 DynamoDB 테이블을 생성합니다.

C. 프로비저닝된 용량과 자동 스케일링이 적용된 DynamoDB 테이블을 생성합니다.

D. 프로비저닝된 용량 모드로 DynamoDB 테이블을 생성하고, 글로벌 테이블로 구성합니다.

```
A company is planning to use an Amazon DynamoDB table for data storage. The company is concerned about cost optimization. The table will not be used on most mornings. In the evenings, the read and write traffic will often be unpredictable. When traffic spikes occur, they will happen very quickly.  
What should a solutions architect recommend?

- A. Create a DynamoDB table in on-demand capacity mode.
- B. Create a DynamoDB table with a global secondary index.
- C. Create a DynamoDB table with provisioned capacity and auto scaling.
- D. Create a DynamoDB table in provisioned capacity mode, and configure it as a global table.
```

정답 : `A`

- 요구사항
	- 아침 시간에는 테이블이 사용되지 않음 -> 비용 최적화 필요
	- 저녁에는 급격하고 예측 불가하게 증가 -> 유연한 용량 필요
- DynamoDB 온디맨드 모드는 사용량 기반으로 과금되며 트래픽이 없을 때 비용이 발생하지 않고 급격한 트래픽 증가도 자동 처리 가능

오답 이유

- **B. 글로벌 보조 인덱스(GSI)**
    - GSI는 쿼리 성능 향상용이며 비용 최적화나 자동 스케일링과 직접 관련이 없음.
    - 트래픽 급증 문제를 해결하지 못함.
    
- **C. 프로비저닝된 용량 + 자동 스케일링**
    - 자동 스케일링은 트래픽 변동을 다루지만, 초기 용량을 항상 프로비저닝해야 하므로 아침 사용량이 없는 시간에도 비용이 일부 발생할 수 있음.
    - 급격한 스파이크에 즉시 대응하지 못할 수도 있음(스케일링 조정 지연).
    
- **D. 프로비저닝된 용량 + 글로벌 테이블**
    - 글로벌 테이블은 다중 리전 데이터 복제를 위한 것이며, 트래픽 급증 및 비용 최적화 요구와는 직접적인 관련이 없음.
    - 글로벌 테이블은 추가 비용 발생.

## #80
한 회사는 최근 애플리케이션 마이그레이션 이니셔티브를 지원하기 위해 AWS 관리 서비스 제공업체(MSP) 파트너와 계약을 체결했습니다. 솔루션 설계자는 기존 AWS 계정에서 MSP 파트너의 AWS 계정으로 Amazon Machine Image(AMI)를 공유해야 합니다. AMI는 Amazon Elastic Block Store(Amazon EBS)에 의해 백업되며, AWS Key Management Service(AWS KMS) 고객 관리 키를 사용하여 EBS 볼륨 스냅샷을 암호화합니다.  
솔루션 설계자가 MSP 파트너의 AWS 계정과 AMI를 공유하기 위한 가장 안전한 방법은 무엇입니까?

A. 암호화된 AMI와 스냅샷을 공개적으로 사용 가능하게 합니다. 키 정책을 수정하여 MSP 파트너의 AWS 계정이 키를 사용할 수 있도록 허용합니다.

B. AMI의 launchPermission 속성을 수정합니다. AMI를 MSP 파트너의 AWS 계정에만 공유합니다. 키 정책을 수정하여 MSP 파트너의 AWS 계정이 키를 사용할 수 있도록 허용합니다.

C. AMI의 launchPermission 속성을 수정합니다. AMI를 MSP 파트너의 AWS 계정에만 공유합니다. 키 정책을 수정하여 MSP 파트너가 소유한 새로운 KMS 키를 신뢰하도록 구성합니다.

D. AMI를 소스 계정에서 MSP 파트너의 AWS 계정에 있는 Amazon S3 버킷으로 내보냅니다. S3 버킷을 MSP 파트너가 소유한 새 KMS 키로 암호화합니다. MSP 파트너의 AWS 계정에서 AMI를 복사하고 실행합니다.

```
A company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for help with an application migration initiative. A solutions architect needs ta share an Amazon Machine Image (AMI) from an existing AWS account with the MSP Partner's AWS account. The AMI is backed by Amazon Elastic Block Store (Amazon EBS) and uses an AWS Key Management Service (AWS KMS) customer managed key to encrypt EBS volume snapshots.  
What is the MOST secure way for the solutions architect to share the AMI with the MSP Partner's AWS account?

- A. Make the encrypted AMI and snapshots publicly available. Modify the key policy to allow the MSP Partner's AWS account to use the key.
- B. Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to allow the MSP Partner's AWS account to use the key.
- C. Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to trust a new KMS key that is owned by the MSP Partner for encryption.
- D. Export the AMI from the source account to an Amazon S3 bucket in the MSP Partner's AWS account, Encrypt the S3 bucket with a new KMS key that is owned by the MSP Partner. Copy and launch the AMI in the MSP Partner's AWS account.
```

정답 : `B`

- 목표는 특정 MSP 파트너 계정과만 안전하게 AMI를 공유하고 암호화된 EBS 스냅샷 접근 권한도 안전하게 제공
- AMI launchPermission 속성 수정 -> 특정 AWS 계정과만 공유
- KMS 키 정책 수정 -> MSP 파트너 계정이 해당 KMS 키를 사용해 스냅샷을 복호화하고 AMI를 사용할 수 있도록 허용

오답 이유

- **A. 암호화된 AMI와 스냅샷을 공개적으로 공유**
    - 공개 공유는 모든 AWS 계정이 AMI와 스냅샷에 접근 가능 → 보안 위험
    - 고객 관리 KMS 키의 권한만으로 안전성을 보장할 수 없음.
    
- **C. MSP 파트너의 KMS 키를 새로 사용하도록 변경**
    - AMI 및 스냅샷은 원래 KMS 키로 암호화되어 있음
    - 새 키를 신뢰하도록 변경하면 스냅샷 재암호화 필요 → 불필요한 운영 오버헤드
    
- **D. S3로 내보내고 MSP KMS 키로 암호화 후 복사**
    - 가능하지만 절차가 복잡하고 추가 단계 발생
    - 운영 오버헤드가 많고, 기존 KMS 키 공유 방법보다 불필요하게 복잡