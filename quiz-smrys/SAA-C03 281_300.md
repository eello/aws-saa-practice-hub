---
created: 2025-10-02 14:14:10
last_modified: 2025-10-03 09:53:48
---
## #281
한 회사는 Amazon RDS for PostgreSQL DB 인스턴스를 사용하여 웹 서버 플릿을 운영하고 있습니다. 정기적인 컴플라이언스 점검 후, 회사는 모든 프로덕션 데이터베이스에 대해 복구 시점 목표(RPO)가 1초 미만이어야 한다는 표준을 설정했습니다.

이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. DB 인스턴스에 대해 Multi-AZ 배포를 활성화합니다.
B. 하나의 가용 영역(AZ)에서 DB 인스턴스에 대해 자동 확장을 활성화합니다.
C. 하나의 가용 영역(AZ)에 DB 인스턴스를 구성하고, 별도의 가용 영역에 다수의 읽기 전용 복제본을 생성합니다.
D. 하나의 가용 영역(AZ)에 DB 인스턴스를 구성하고, AWS Database Migration Service(AWS DMS) 변경 데이터 캡처(CDC) 작업을 구성합니다.

```
A company runs a fleet of web servers using an Amazon RDS for PostgreSQL DB instance. After a routine compliance check, the company sets a standard that requires a recovery point objective (RPO) of less than 1 second for all its production databases.  
  
Which solution meets these requirements?

- A. Enable a Multi-AZ deployment for the DB instance.
- B. Enable auto scaling for the DB instance in one Availability Zone.
- C. Configure the DB instance in one Availability Zone, and create multiple read replicas in a separate Availability Zone.
- D. Configure the DB instance in one Availability Zone, and configure AWS Database Migration Service (AWS DMS) change data capture (CDC) tasks.
```

정답 : `A`

- RPO(Recovery Point Objective)가 1초 미만이라는 것은 장애 시 데이터 손실이 사실상 없어야 함
	- RDS의 Multi-AZ 배포는 동기식 복제를 통해 데이터를 다른 AZ의 스탠바이 인스턴스로 실시간에 가깝게 복제
	- 장애가 발생하면 자동으로 스탠바이로 페일오버 되어 데이터 손실 없이 RPO≈0 달성 가능
	- 다른 옵션들은 비동기 복제 기반이거나 복제와 무관하기 때문에 RPO < 1초를 보장 못함

오답 이유

- **B. Auto Scaling 활성화**
    - RDS 인스턴스에는 EC2 Auto Scaling 같은 개념이 적용되지 않음. 확장성과 관계된 기능이지, RPO와 무관.
    
- **C. Read Replicas**
    - Read Replica는 **비동기 복제**이므로 네트워크/지연에 따라 초 단위 이상 데이터 손실 발생 가능. RPO < 1초 불가능.
    
- **D. DMS CDC 작업**
    - DMS는 데이터 마이그레이션/ETL 목적이며, 실시간에 가깝게 스트리밍할 수 있어도 RDS Multi-AZ와 같은 동기식 복제를 보장하지 않음. RPO < 1초 보장 불가.


## #282
한 회사는 웹 애플리케이션을 운영 중이며, 이 애플리케이션은 VPC의 프라이빗 서브넷에 배포된 Amazon EC2 인스턴스에서 실행됩니다. 퍼블릭 서브넷에 걸쳐 있는 Application Load Balancer (ALB)가 EC2 인스턴스로 웹 트래픽을 전달합니다. 회사는 새로운 보안 조치를 구현하여 ALB에서 EC2 인스턴스로 들어오는 인바운드 트래픽은 허용하되, EC2 인스턴스의 프라이빗 서브넷 내부 또는 외부의 다른 어떤 소스에서도 접근하지 못하도록 하고자 합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. 라우트 테이블에 인터넷에서 EC2 인스턴스의 프라이빗 IP 주소로 트래픽을 전달하는 경로를 구성합니다.  
B. EC2 인스턴스의 보안 그룹을 ALB의 보안 그룹에서 오는 트래픽만 허용하도록 구성합니다.  
C. EC2 인스턴스를 퍼블릭 서브넷으로 이동시키고, EC2 인스턴스에 Elastic IP 주소를 할당합니다.  
D. ALB의 보안 그룹을 모든 포트에서 모든 TCP 트래픽을 허용하도록 구성합니다.  

```
A company runs a web application that is deployed on Amazon EC2 instances in the private subnet of a VPC. An Application Load Balancer (ALB) that extends across the public subnets directs web traffic to the EC2 instances. The company wants to implement new security measures to restrict inbound traffic from the ALB to the EC2 instances while preventing access from any other source inside or outside the private subnet of the EC2 instances.  
  
Which solution will meet these requirements?

- A. Configure a route in a route table to direct traffic from the internet to the private IP addresses of the EC2 instances.
- B. Configure the security group for the EC2 instances to only allow traffic that comes from the security group for the ALB.
- C. Move the EC2 instances into the public subnet. Give the EC2 instances a set of Elastic IP addresses.
- D. Configure the security group for the ALB to allow any TCP traffic on any port.
```

정답 : `B`

- AWS 보안 그룹은 다른 보안 그룹을 소스로 지정가능
- EC2 인스턴스의 보안 그룹에서 ALB 보안 그룹을 소스로 명시하면, ALB를 통해 들어오는 요청만 허용
	- 다른 어떤 내부/외부 소스에서 접근 불가

오답 이유

- **A. 라우트 테이블에 인터넷에서 EC2 인스턴스의 프라이빗 IP로 경로 구성**
    - 잘못된 방법입니다. 라우트 테이블은 보안 정책이 아니라 네트워크 경로를 정의하는 것이며, EC2 프라이빗 IP를 대상으로 직접 인터넷 라우팅하는 것은 불가능합니다.
    
- **C. EC2 인스턴스를 퍼블릭 서브넷으로 이동하고 Elastic IP 할당**
    - 보안 요구사항에 위배됩니다. 퍼블릭 액세스를 가능하게 하는 방식으로, 내부 전용으로 유지해야 하는 프라이빗 서브넷 배치를 무력화합니다.
    
- **D. ALB 보안 그룹에서 모든 TCP 트래픽 허용**
    - 이는 불필요하게 열려 있는 설정으로 보안 위험이 커집니다. ALB → EC2만 허용해야지, ALB에서 모든 포트/프로토콜을 허용하는 것은 Least Privilege 원칙 위반입니다.



## #283
한 연구 회사는 시뮬레이션 애플리케이션과 시각화 애플리케이션으로 구동되는 실험을 수행합니다. 시뮬레이션 애플리케이션은 Linux에서 실행되며 5분마다 중간 데이터를 NFS 공유에 출력합니다. 시각화 애플리케이션은 Windows 데스크톱 애플리케이션으로 시뮬레이션 출력을 표시하며 SMB 파일 시스템이 필요합니다.

회사는 두 개의 동기화된 파일 시스템을 유지하고 있습니다. 이 전략은 데이터 중복 및 비효율적인 리소스 사용을 초래하고 있습니다. 회사는 두 애플리케이션 중 어느 것도 코드 변경 없이 AWS로 마이그레이션해야 합니다.

어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. 두 애플리케이션을 AWS Lambda로 마이그레이션합니다. 애플리케이션 간 데이터 교환을 위해 Amazon S3 버킷을 생성합니다.
B. 두 애플리케이션을 Amazon Elastic Container Service (Amazon ECS)로 마이그레이션합니다. 스토리지로 Amazon FSx File Gateway를 구성합니다.
C. 시뮬레이션 애플리케이션을 Linux Amazon EC2 인스턴스로 마이그레이션합니다. 시각화 애플리케이션을 Windows EC2 인스턴스로 마이그레이션합니다. 애플리케이션 간 데이터 교환을 위해 Amazon Simple Queue Service (Amazon SQS)를 구성합니다.
D. 시뮬레이션 애플리케이션을 Linux Amazon EC2 인스턴스로 마이그레이션합니다. 시각화 애플리케이션을 Windows EC2 인스턴스로 마이그레이션합니다. 스토리지로 Amazon FSx for NetApp ONTAP을 구성합니다.

```
A research company runs experiments that are powered by a simulation application and a visualization application. The simulation application runs on Linux and outputs intermediate data to an NFS share every 5 minutes. The visualization application is a Windows desktop application that displays the simulation output and requires an SMB file system.  
  
The company maintains two synchronized file systems. This strategy is causing data duplication and inefficient resource usage. The company needs to migrate the applications to AWS without making code changes to either application.  
  
Which solution will meet these requirements?

- A. Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data between the applications.
- B. Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure Amazon FSx File Gateway for storage.
- C. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon Simple Queue Service (Amazon SQS) to exchange data between the applications.
- D. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx for NetApp ONTAP for storage.
```

정답 : `D`

- FSx for NetApp ONTAP은 동일한 파일 시스템(또는 볼륨)에 대해 NFS와 SMB를 동시에 제공(멀티프로토콜) 할 수 있음
- Linux 애플리케이션은 NFS로, Windows 애플리케이션은 SMB로 같은 데이터에 접근 가능
- 따라서 코드 변경없이 두 애플리케이션은 EC2로 옮기고 단일 스토리지로 통합해 데이터 중복과 비효율 해소

오답 이유

- **A. Lambda + S3**
    - 파일 시스템(NFS/SMB) 대신 객체 스토리지(S3)를 사용하므로 **파일 시스템 I/O 기대**를 충족하지 못합니다. 또한 애플리케이션을 Lambda로 재작성하는 **코드 변경**이 필요할 가능성이 큼.
    
- **B. ECS + FSx File Gateway**
    - FSx File Gateway는 **SMB 전용** 접근(주로 온프레미스 ↔ FSx for Windows File Server) 시나리오에 적합합니다. **NFS가 필요**한 Linux 앱 요구를 충족하지 못합니다.
    
- **C. EC2 + SQS 교환**
    - SQS는 메시지 큐이지 **공유 파일 시스템 대체**가 아닙니다. 파일 기반 워크플로를 큐 기반으로 바꾸려면 **애플리케이션 코드 변경**이 필요합니다.


## #284
예산 계획의 일환으로, 경영진은 사용자별로 나열된 AWS 청구 항목 보고서를 원합니다. 이 데이터는 부서별 예산을 생성하는 데 사용될 것입니다. 솔루션스 아키텍트는 이 보고서 정보를 얻는 가장 효율적인 방법을 결정해야 합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. Amazon Athena로 쿼리를 실행하여 보고서를 생성합니다.
B. Cost Explorer에서 보고서를 생성하고 보고서를 다운로드합니다.
C. 청구 대시보드에서 청구 세부 정보를 액세스하고 청구서를 다운로드합니다.
D. AWS Budgets에서 비용 예산을 수정하여 Amazon Simple Email Service (Amazon SES)로 알림을 보냅니다.

```
As part of budget planning, management wants a report of AWS billed items listed by user. The data will be used to create department budgets. A solutions architect needs to determine the most efficient way to obtain this report information.  
  
Which solution meets these requirements?

- A. Run a query with Amazon Athena to generate the report.
- B. Create a report in Cost Explorer and download the report.
- C. Access the bill details from the billing dashboard and download the bill.
- D. Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES).
```

정답 : `B`

- 사용자별로 비용을 분류하려면 태깅 또는 IAM 사용자/역할 기반 비용 집계 필요
- Cost Explorer는 사용자/태그/서비스/리소스 기준으로 비용을 분석하고 시각화 가능

오답 이유

- **A. Athena 쿼리 실행**
    - Cost and Usage Report(CUR)를 S3에 저장하고 Athena로 쿼리하면 가능하지만, 설정 및 관리 오버헤드가 큼. “가장 효율적” 요구사항에는 적합하지 않음.
    
- **C. 청구 대시보드에서 청구서 다운로드**
    - 전체 계정별 비용 청구서는 제공하지만 사용자/태그 단위로 세분화된 분석은 불가능.
    
- **D. AWS Budgets + SES 알림**
    - 예산 초과 시 알림을 받을 수 있지만, 사용자별 상세 비용 보고서 생성 목적에는 맞지 않음.


## #285
한 회사가 Amazon S3를 사용하여 정적 웹사이트를 호스팅하고 있습니다. 회사는 웹페이지에 문의(Contact) 양식을 추가하고자 합니다. 이 문의 양식은 사용자가 이름, 이메일 주소, 전화번호, 사용자 메시지를 입력할 수 있는 동적 서버 측 컴포넌트를 포함합니다. 회사는 월 방문 수가 100회 미만일 것으로 예상합니다.

이 요구사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?

A. Amazon Elastic Container Service (Amazon ECS)에서 동적 문의 양식 페이지를 호스팅합니다. Amazon Simple Email Service (Amazon SES)를 설정하여 서드파티 이메일 제공자와 연결합니다.
B. Amazon API Gateway 엔드포인트를 생성하고, AWS Lambda 백엔드가 Amazon Simple Email Service (Amazon SES)를 호출하도록 구성합니다.
C. Amazon Lightsail을 배포하여 정적 웹페이지를 동적으로 변환합니다. 클라이언트 측 스크립팅으로 문의 양식을 구성합니다. 양식을 Amazon WorkMail과 통합합니다.
D. t2.micro Amazon EC2 인스턴스를 생성합니다. LAMP 스택을 배포해 웹페이지를 호스팅합니다. 클라이언트 측 스크립팅으로 문의 양식을 구성합니다. 양식을 Amazon WorkMail과 통합합니다.

```
A company hosts its static website by using Amazon S3. The company wants to add a contact form to its webpage. The contact form will have dynamic server-side components for users to input their name, email address, phone number, and user message. The company anticipates that there will be fewer than 100 site visits each month.  
  
Which solution will meet these requirements MOST cost-effectively?

- A. Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up Amazon Simple Email Service (Amazon SES) to connect to any third-party email provider.
- B. Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).
- C. Convert the static webpage to dynamic by deploying Amazon Lightsail. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail.
- D. Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL, PHP/Perl/Python) stack to host the webpage. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail.
```

정답 : `B`

- 트래픽이 매우 낮으므로 항상 켜진 서버나 컨테이너 불필요 → 서버리스(API Gateway/Lambda/SES)는 요청당 과금으로 유휴 비용이 거의 0
- S3 정적 사이트는 그대로 두고, 폼 제출만 API Gateway로 POST → Lambda에서 입력 검증/로깅 후 SES로 메일 발송으로 구현이 쉬움
- 완전관리형 서비스들로 운영 오버헤드 최소화. 인스턴스 패치/확장/고가용성 고민 불필요.

오답 이유

- **A. ECS**: 컨테이너 오케스트레이션은 이 트래픽 수준에 과도하며 클러스터/서비스 운영 비용과 복잡도가 증가.
    
- **C. Lightsail로 동적 전환**: 항상 켜진 VM 비용이 발생하고, 서버 관리가 필요. WorkMail은 메일 박스 서비스이지 폼 처리/발송에 최적이 아님(SES가 적합).
    
- **D. EC2 + LAMP**: 상시 인스턴스 비용과 보안/패치/확장 관리 필요. 월 100회 미만 트래픽에 비경제적.


## #286
한 회사는 Amazon S3 앞단에 Amazon CloudFront를 사용하여 정적 웹사이트를 호스팅하고 있습니다. 이 정적 웹사이트는 데이터베이스 백엔드를 사용합니다. 회사는 웹사이트가 Git 리포지토리에 반영된 업데이트를 표시하지 않는다는 것을 발견했습니다. 회사는 Git 리포지토리와 Amazon S3 간의 CI/CD 파이프라인을 확인했습니다. 회사는 웹훅이 제대로 구성되어 있고, CI/CD 파이프라인이 성공적인 배포를 나타내는 메시지를 전송하고 있음을 확인했습니다.

솔루션스 아키텍트는 웹사이트에 업데이트가 표시되도록 하는 솔루션을 구현해야 합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. Application Load Balancer를 추가합니다.  
B. 웹 애플리케이션의 데이터베이스 계층에 Amazon ElastiCache for Redis 또는 Memcached를 추가합니다.  
C. CloudFront 캐시를 무효화합니다.  
D. AWS Certificate Manager (ACM)을 사용하여 웹사이트의 SSL 인증서를 검증합니다.  

```
A company has a static website that is hosted on Amazon CloudFront in front of Amazon S3. The static website uses a database backend. The company notices that the website does not reflect updates that have been made in the website’s Git repository. The company checks the continuous integration and continuous delivery (CI/CD) pipeline between the Git repository and Amazon S3. The company verifies that the webhooks are configured properly and that the CI/CD pipeline is sending messages that indicate successful deployments.  
  
A solutions architect needs to implement a solution that displays the updates on the website.  
  
Which solution will meet these requirements?

- A. Add an Application Load Balancer.
- B. Add Amazon ElastiCache for Redis or Memcached to the database layer of the web application.
- C. Invalidate the CloudFront cache.
- D. Use AWS Certificate Manager (ACM) to validate the website’s SSL certificate.
```

정답 : `C`

- CloudFront는 S3 콘텐츠를 캐싱.
- S3 버킷에 새로운 배포가 완료되었더라도, CloudFront 엣지 로케이션에 캐싱된 객체가 만료되지 않았다면 사용자는 구 버전의 콘텐츠를 보게됨
- 따라서 CloudFront 컈시 무효화(invalidation) 를 수행해야 최신 콘텐츠가 엣지 로케이션에서 반영

오답 이유

- **A. Application Load Balancer 추가**
    - 정적 웹사이트 + S3 + CloudFront 아키텍처에서는 필요 없음. ALB는 동적 애플리케이션 로드 밸런싱용.
    
- **B. ElastiCache 추가**
    - 캐시 계층은 데이터베이스 성능 개선 용도이지, 정적 S3 객체 업데이트와는 무관.
    
- **D. ACM SSL 인증서 검증**
    - HTTPS 보안을 위한 SSL 인증서 검증이며, 웹사이트의 콘텐츠 업데이트 표시 문제와 무관.


## #287
한 회사가 온프레미스의 Windows 기반 애플리케이션을 AWS 클라우드로 마이그레이션하려고 합니다. 애플리케이션은 애플리케이션 계층, 비즈니스 계층, Microsoft SQL Server를 사용하는 데이터베이스 계층의 세 가지 계층으로 구성됩니다. 회사는 SQL Server의 네이티브 백업(native backups)과 Data Quality Services(DQS)와 같은 특정 기능을 사용하고자 합니다. 또한 계층 간 처리용 파일을 공유해야 합니다.

이 요구사항을 충족하도록 솔루션스 아키텍트는 아키텍처를 어떻게 설계해야 합니까?

A. 세 계층 모두를 Amazon EC2 인스턴스에서 호스팅합니다. 계층 간 파일 공유에는 Amazon FSx File Gateway를 사용합니다.
B. 세 계층 모두를 Amazon EC2 인스턴스에서 호스팅합니다. 계층 간 파일 공유에는 Amazon FSx for Windows File Server를 사용합니다.
C. 애플리케이션 계층과 비즈니스 계층은 Amazon EC2 인스턴스에서 호스팅합니다. 데이터베이스 계층은 Amazon RDS에서 호스팅합니다. 계층 간 파일 공유에는 Amazon Elastic File System(Amazon EFS)을 사용합니다.
D. 애플리케이션 계층과 비즈니스 계층은 Amazon EC2 인스턴스에서 호스팅합니다. 데이터베이스 계층은 Amazon RDS에서 호스팅합니다. 계층 간 파일 공유에는 프로비저닝된 IOPS SSD(io2) Amazon Elastic Block Store(Amazon EBS) 볼륨을 사용합니다.

```
A company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a database tier with Microsoft SQL Server. The company wants to use specific features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers.  
  
How should a solutions architect design the architecture to meet these requirements?

- A. Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.
- B. Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.
- C. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.
- D. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for file sharing between the tiers.
```

정답 : `B`

- SQL Server의 고유 기능(네이티브 백업, DQS 등)을 사용하려면 전체 SQL Server 기능에 대한 제어권 필요. 이는 EC2에서 SQL Server를 직접 운영할 때 보장
	- RDS for SQL Server는 여러 엔진 기능이 제한되며 DQS 같은 구성 요소는 지원 대상이 아님
- 계층 간 파일 공유는 Windows 워크로드에 최적화된 Amazon FSx for Windows File Server(SMB/NTFS, AD 통합)가 가장 적합

오답 이유

- **A. FSx File Gateway 사용**: FSx File Gateway는 **온프레미스에서** 클라우드의 FSx for Windows File Server에 접근하는 하이브리드 용도입니다. 이미 워크로드를 **AWS 내부(EC2)** 로 마이그레이션하므로 불필요하며, EC2는 **FSx for Windows를 직접 SMB로** 마운트하면 됩니다.
    
- **C. RDS + EFS**: RDS for SQL Server는 질문의 **특정 SQL Server 기능(DQS 등)** 을 보장하지 못합니다. 또한 **EFS는 NFS(리눅스용)** 이며 Windows SMB 공유 요구에 부적합합니다.
    
- **D. RDS + EBS 공유**: RDS 제약은 C와 동일. EBS 볼륨은 **단일 인스턴스에만 블록 디바이스로 연결**되며, 다수 인스턴스 간 일반적인 **동시 파일 공유 용도**가 아닙니다(한 인스턴스에서 파일 서버를 구성해 공유할 수는 있으나 고가용성/관리성 측면에서 FSx가 우수).


## #288
한 회사가 Linux 기반 웹 서버 그룹을 AWS로 마이그레이션하고 있습니다. 웹 서버들은 일부 콘텐츠를 위해 공유 파일 저장소의 파일에 접근해야 합니다. 회사는 애플리케이션에 어떠한 변경도 해서는 안 됩니다.

요구 사항을 충족하려면 솔루션스 아키텍트는 무엇을 해야 합니까?

A. 웹 서버에서 접근할 수 있도록 Amazon S3 Standard 버킷을 생성합니다.
B. Amazon S3 버킷을 오리진으로 사용하는 Amazon CloudFront 배포를 구성합니다.
C. Amazon Elastic File System(Amazon EFS) 파일 시스템을 생성합니다. 모든 웹 서버에 EFS 파일 시스템을 마운트합니다.
D. 범용 SSD(gp3) Amazon Elastic Block Store(Amazon EBS) 볼륨을 구성합니다. 모든 웹 서버에 EBS 볼륨을 마운트합니다.

```
A company is migrating a Linux-based web server group to AWS. The web servers must access files in a shared file store for some content. The company must not make any changes to the application.  
  
What should a solutions architect do to meet these requirements?

- A. Create an Amazon S3 Standard bucket with access to the web servers.
- B. Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.
- C. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers.
- D. Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume to all web servers.
```

정답 : `C`

- 요구사항
	- Linux 기반 다수의 EC2 인스턴스에서 동시에 파일을 읽고/쓰기 위한 공유 파일 스토리지 필요
	- 애플리케이션 변경 없이 기존과 동일한 POSIX 파일 시스템 인터페이스를 제공
- Amazon EFS는 표준 NFS 프로토콜(POSIX)을 제공하며 여러 인스턴스가 동시에 마운트 가능해 애플리케이션 수정 없이 요구사항 충족

오답 이유

- **A. S3 Standard 버킷**: 객체 스토리지로 **POSIX 파일 시스템이 아님**. NFS 마운트가 불가하고 open()/rename() 등 파일시스템 기대 동작을 그대로 제공하지 않아 **애플리케이션 수정**이 필요합니다.
    
- **B. CloudFront + S3 오리진**: CDN/정적 콘텐츠 캐싱 용도입니다. 서버 측에서 **공유 파일 시스템처럼 읽고/쓰기** 할 수 없으며, 역시 **애플리케이션 변경**이 필요합니다.
    
- **D. EBS gp3 볼륨**: EBS는 기본적으로 **단일 인스턴스에만 연결**됩니다(특수한 io1/io2 Multi-Attach도 제약이 많고 파일시스템/클러스터 락 설계 변경 필요). 여러 웹 서버가 동시에 마운트하는 **공유 파일 스토리지 용도에 부적합**합니다.



## #289
한 회사는 AWS Lambda 함수가 동일한 AWS 계정에 있는 Amazon S3 버킷에 대해 읽기 권한이 필요합니다.

이 요구사항을 가장 안전한 방식으로 충족하는 솔루션은 무엇입니까?

A. 읽기 권한을 부여하는 S3 버킷 정책을 S3 버킷에 적용합니다.  
B. Lambda 함수에 IAM 역할을 적용합니다. 역할에 S3 버킷에 대한 읽기 권한을 부여하는 IAM 정책을 적용합니다.  
C. Lambda 함수의 코드에 액세스 키와 비밀 키를 내장하여 S3 버킷 읽기 권한을 위한 IAM 권한을 부여합니다.  
D. Lambda 함수에 IAM 역할을 적용합니다. 역할에 계정 내 모든 S3 버킷에 대한 읽기 권한을 부여하는 IAM 정책을 적용합니다.  

```
A company has an AWS Lambda function that needs read access to an Amazon S3 bucket that is located in the same AWS account.  
  
Which solution will meet these requirements in the MOST secure manner?

- A. Apply an S3 bucket policy that grants read access to the S3 bucket.
- B. Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to the S3 bucket.
- C. Embed an access key and a secret key in the Lambda function’s code to grant the required IAM permissions for read access to the S3 bucket.
- D. Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to all S3 buckets in the account.
```

정답 : `B`

- Lambda는 실행 시 IAM 역할(Execution Role) 을 통해 AWS 리소스에 접근
- 가장 보안성이 높은 방식은 최소 권한 원칙을 따르는 것으로 람다 함수가 필요한 리소스(S3 특정 버킷)에만 접근 권한을 부여해야 함
- 따라서 람다에 IAM 역할을 연결하고, 역할에 S3 특정 버킷의 읽기 권한을 가진 정책을 부여

오답 이유

- **A. S3 버킷 정책만 사용**
    - 가능은 하지만 보안적으로 바람직하지 않음. 버킷 정책은 외부 계정 접근 제어가 주 목적이며, 동일 계정 내 리소스 권한은 IAM 역할을 사용하는 것이 모범 사례.
    
- **C. 코드에 액세스 키/비밀 키 내장**
    - **안전하지 않은 방식**으로, 키 유출 위험이 있으며 AWS 보안 모범 사례에 위배.
    
- **D. 모든 S3 버킷 읽기 권한 부여**
    - 불필요하게 광범위한 권한으로, 최소 권한 원칙 위배. 보안 취약점이 될 수 있음.


## #290
한 회사가 여러 Amazon EC2 인스턴스에서 웹 애플리케이션을 호스팅하고 있습니다. EC2 인스턴스는 사용자 수요에 따라 확장하는 Auto Scaling 그룹에 속해 있습니다. 회사는 장기 약정을 하지 않고 비용 절감을 최적화하고자 합니다.

이 요구사항을 충족하기 위해 솔루션스 아키텍트는 어떤 EC2 인스턴스 구매 옵션을 권장해야 합니까?

A. 전용 인스턴스(Dedicated Instances)만 사용  
B. 온디맨드 인스턴스(On-Demand Instances)만 사용  
C. 온디맨드 인스턴스와 스팟 인스턴스(Spot Instances)를 혼합 사용  
D. 온디맨드 인스턴스와 예약 인스턴스(Reserved Instances)를 혼합 사용  

```
A company hosts a web application on multiple Amazon EC2 instances. The EC2 instances are in an Auto Scaling group that scales in response to user demand. The company wants to optimize cost savings without making a long-term commitment.  
  
Which EC2 instance purchasing option should a solutions architect recommend to meet these requirements?

- A. Dedicated Instances only
- B. On-Demand Instances only
- C. A mix of On-Demand Instances and Spot Instances
- D. A mix of On-Demand Instances and Reserved Instances
```

정답 : `C`

- 장기 약정 없음 → Reserved Instances(RI)나 Savings Plans는 제외
- 비용 최적화 → 온디맨드만 쓰면 유연하긴 하지만 가장 비쌈
- 오토 스케일링 그룹 환경 → 스팟 인스턴스를 섞어 쓰면 유연 확장 시 비용 절감 극대화 가능

오답 이유

- **A. 전용 인스턴스(Dedicated Instances)**
    - 하드웨어 전용 환경으로 보안/컴플라이언스 용도. 비용 절감 목적과 무관하며 오히려 비쌈.
    
- **B. 온디맨드 인스턴스만 사용**
    - 유연하긴 하지만 비용 최적화 요구를 충족하지 못함.
    
- **D. 온디맨드 + 예약 인스턴스**
    - 비용 최적화는 가능하지만 **장기 약정**이 필요하므로 문제 조건에 위배.


## #291
한 미디어 회사는 Amazon CloudFront를 사용하여 공개 스트리밍 비디오 콘텐츠를 제공합니다.  
회사는 Amazon S3에 호스팅된 비디오 콘텐츠에 대한 액세스를 제어하여 보안을 강화하고자 합니다.  
일부 사용자는 쿠키를 지원하지 않는 사용자 정의 HTTP 클라이언트를 사용하고 있습니다.  
또한 일부 사용자는 액세스를 위해 사용 중인 하드코딩된 URL을 변경할 수 없습니다.  

다음 중 어떤 서비스 또는 방법이 사용자에게 가장 적은 영향을 주면서 이러한 요구사항을 충족할 수 있습니까? (두 개 선택)

A. 서명된 쿠키(Signed cookies)  
B. 서명된 URL(Signed URLs)  
C. AWS AppSync  
D. JSON Web Token (JWT)  
E. AWS Secrets Manager  

```
A media company uses Amazon CloudFront for its publicly available streaming video content. The company wants to secure the video content that is hosted in Amazon S3 by controlling who has access. Some of the company’s users are using a custom HTTP client that does not support cookies. Some of the company’s users are unable to change the hardcoded URLs that they are using for access.  
  
Which services or methods will meet these requirements with the LEAST impact to the users? (Choose two.)

- A. Signed cookies
- B. Signed URLs
- C. AWS AppSync
- D. JSON Web Token (JWT)
- E. AWS Secrets Manager
```

정답 : `A, B`

- Signed URLs
	- 특정 객체 단위 접근 제어. 쿠키 없는 환경에서 유용
	- 특정 객체에 대한 액세스를 제한된 시간 동안 허용할 수 있음
	- 쿠키를 지원하지 않는 클라이언트도 사용 가능
- Signed Cookies
	- 경로나 다수 객체 제어 가능. 기존 URL 변경 불필요
	- 여러 개의 객체나 경로에 대한 액세스를 제어할 수 있음
	- URL을 변경하지 않고도 제어 가능

오답 이유

- **C. AWS AppSync**
    - GraphQL API 서비스로, 콘텐츠 보호와는 직접적 관련이 없음.
    
- **D. JSON Web Token (JWT)**
    - 인증 및 권한 부여 용도이지만 CloudFront와 직접 연동해 S3 콘텐츠 보호를 위한 솔루션은 아님.
    
- **E. AWS Secrets Manager**
    - 비밀 키 관리 서비스이며 사용자 콘텐츠 접근 제어와는 무관.


## #292
한 회사가 여러 소스에서 유입되는 실시간 스트리밍 데이터를 수집하는 새로운 데이터 플랫폼을 준비하고 있습니다. 회사는 데이터를 Amazon S3에 기록하기 전에 변환해야 합니다. 또한 변환된 데이터를 SQL로 조회할 수 있어야 합니다.

다음 솔루션 중 어떤 것들이 이러한 요구 사항을 충족합니까? (두 개 선택)

A. Amazon Kinesis Data Streams로 데이터를 스트리밍합니다. Amazon Kinesis Data Analytics로 데이터를 변환합니다. Amazon Kinesis Data Firehose로 데이터를 Amazon S3에 기록합니다. Amazon Athena로 S3의 변환된 데이터를 조회합니다.
B. Amazon Managed Streaming for Apache Kafka (Amazon MSK)로 데이터를 스트리밍합니다. AWS Glue로 데이터를 변환하고 Amazon S3에 기록합니다. Amazon Athena로 S3의 변환된 데이터를 조회합니다.
C. AWS Database Migration Service (AWS DMS)로 데이터를 수집합니다. Amazon EMR로 데이터를 변환하고 Amazon S3에 기록합니다. Amazon Athena로 S3의 변환된 데이터를 조회합니다.
D. Amazon Managed Streaming for Apache Kafka (Amazon MSK)로 데이터를 스트리밍합니다. Amazon Kinesis Data Analytics로 데이터를 변환하고 Amazon S3에 기록합니다. Amazon RDS Query Editor로 S3의 변환된 데이터를 조회합니다.
E. Amazon Kinesis Data Streams로 데이터를 스트리밍합니다. AWS Glue로 데이터를 변환합니다. Amazon Kinesis Data Firehose로 데이터를 Amazon S3에 기록합니다. Amazon RDS Query Editor로 S3의 변환된 데이터를 조회합니다.

```
A company is preparing a new data platform that will ingest real-time streaming data from multiple sources. The company needs to transform the data before writing the data to Amazon S3. The company needs the ability to use SQL to query the transformed data.  
  
Which solutions will meet these requirements? (Choose two.)

- A. Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.
- B. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.
- C. Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.
- D. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use Amazon Kinesis Data Analytics to transform the data and to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.
- E. Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.
```

정답 : `A, B`

- A
	- Kinesis Data Streams(수집) → Kinesis Data Analytics(실시간 SQL/플링크 변환) → Firehose(S3 적재) → Athena(SQL 조회)
- B
	- MSK(수집) → AWS Glue 스트리밍 잡(Spark Structured Streaming)으로 변환 및 S3 적재 → Athena(SQL 조회)
	- MSK : 완전관리형 Kafka. Glue 스트리밍/Spark로 처리 가능

오답 이유

- **C**: DMS는 주로 **데이터베이스 복제/마이그레이션** 용도로 적합하며 “여러 소스의 실시간 스트리밍 수집” 일반 용도로는 부적합. EMR은 가능하지만 운영 오버헤드가 커 “가장 효율적” 관점에서 벗어남.
    
- **D, E**: **Amazon RDS Query Editor는 S3를 직접 쿼리할 수 없습니다.** S3의 데이터는 **Athena**(또는 Redshift Spectrum 등)로 조회해야 합니다.


## #293
한 회사는 온프레미스 볼륨 백업 솔루션이 수명을 다했습니다. 회사는 새 백업 솔루션의 일부로 AWS를 사용하고자 하며, AWS에 백업되는 동안 모든 데이터에 대해 로컬 액세스를 유지하고자 합니다. 또한 AWS에 백업되는 데이터가 자동으로, 그리고 안전하게 전송되도록 보장하고자 합니다.

이 요구사항을 충족하는 솔루션은 무엇입니까?

A. AWS Snowball을 사용하여 온프레미스 솔루션에서 Amazon S3로 데이터를 마이그레이션합니다. 온프레미스 시스템이 데이터에 로컬로 액세스할 수 있도록 Snowball S3 엔드포인트를 마운트하도록 구성합니다.
B. AWS Snowball Edge를 사용하여 온프레미스 솔루션에서 Amazon S3로 데이터를 마이그레이션합니다. Snowball Edge 파일 인터페이스를 사용하여 온프레미스 시스템에 로컬 액세스를 제공합니다.
C. AWS Storage Gateway를 사용하고 캐시드 볼륨 게이트웨이를 구성합니다. 온프레미스에서 Storage Gateway 소프트웨어 어플라이언스를 실행하고 일부 데이터를 로컬로 캐시하도록 구성합니다. 게이트웨이 스토리지 볼륨을 마운트하여 데이터에 로컬 액세스를 제공합니다.
D. AWS Storage Gateway를 사용하고 스토어드 볼륨 게이트웨이를 구성합니다. 온프레미스에서 Storage Gateway 소프트웨어 어플라이언스를 실행하고 게이트웨이 스토리지 볼륨을 온프레미스 스토리지에 매핑합니다. 게이트웨이 스토리지 볼륨을 마운트하여 데이터에 로컬 액세스를 제공합니다.

```
A company has an on-premises volume backup solution that has reached its end of life. The company wants to use AWS as part of a new backup solution and wants to maintain local access to all the data while it is backed up on AWS. The company wants to ensure that the data backed up on AWS is automatically and securely transferred.  
  
Which solution meets these requirements?

- A. Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on-premises systems to mount the Snowball S3 endpoint to provide local access to the data.
- B. Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the Snowball Edge file interface to provide on-premises systems with local access to the data.
- C. Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway software appliance on premises and configure a percentage of data to cache locally. Mount the gateway storage volumes to provide local access to the data.
- D. Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway software appliance on premises and map the gateway storage volumes to on-premises storage. Mount the gateway storage volumes to provide local access to the data.
```

정답 : `D`

- 로컬 액세스 유지: Stored Volumes는 원본 데이터를 온프레미스에 보관하고 저지연 로컬 액세스를 제공
- 자동･보안 전송: 변경 데이터는 자동으로, 암호화된 채널(TLS)을 통해 AWS로 전송되고 EBS 스냅샷 형태로 S3에 안전하게 저장
- 백업 목적 적합: 온프레미스가 소스 오브 트루스이고 AWS는 오프사이트 백업/복구 지점이 되는 전형적인 패턴

오답 이유

- **A. Snowball**: 오프라인/대용량 **일회성** 이전 장치입니다. **지속적 백업**이나 상시 로컬 액세스를 제공하지 않습니다. “Snowball S3 엔드포인트 마운트”도 불가합니다.
    
- **B. Snowball Edge**: 마찬가지로 **일시적 에지 디바이스**로 상시 백업/동기화 및 지속 로컬 액세스 요구에 맞지 않습니다.
    
- **C. Storage Gateway Cached Volumes**: **주 데이터는 S3에**, 자주 접근 데이터만 로컬 캐시—즉 **전체 데이터의 로컬 액세스 보장** 요구와 불일치합니다.


## #294
Amazon EC2 인스턴스에서 호스팅되는 애플리케이션이 Amazon S3 버킷에 액세스해야 합니다. 이때 트래픽은 인터넷을 통과해서는 안 됩니다.

이 요구사항을 충족하려면 솔루션스 아키텍트는 어떻게 액세스를 구성해야 합니까?

A. Amazon Route 53을 사용하여 프라이빗 호스티드 존을 생성합니다.
B. VPC에서 Amazon S3용 게이트웨이 VPC 엔드포인트를 설정합니다.
C. EC2 인스턴스가 S3 버킷에 액세스하도록 NAT 게이트웨이를 구성합니다.
D. VPC와 S3 버킷 간에 AWS Site-to-Site VPN 연결을 설정합니다.

```
An application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Traffic must not traverse the internet.  
  
How should a solutions architect configure access to meet these requirements?

- A. Create a private hosted zone by using Amazon Route 53.
- B. Set up a gateway VPC endpoint for Amazon S3 in the VPC.
- C. Configure the EC2 instances to use a NAT gateway to access the S3 bucket.
- D. Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket.
```

정답 : `B`

- VPC Gateway Endpoint : VPC 내부에서 Amazon S3 및 DynamoDB에 대한 프라이빗 연결 제공

오답 이유

- **A. Route 53 Private Hosted Zone**: 이는 내부 DNS 해석 용도로 사용되며, VPC에서 S3로의 비인터넷 통신을 보장하지 않습니다. 잘못된 접근입니다.
    
- **C. NAT Gateway**: NAT 게이트웨이는 퍼블릭 서브넷을 통해 인터넷으로 나가는 트래픽을 허용합니다. 따라서 여전히 인터넷을 경유하므로 요구사항(“인터넷 통과 금지”)에 맞지 않습니다.
    
- **D. Site-to-Site VPN**: VPN은 온프레미스 네트워크와 AWS VPC를 연결할 때 사용합니다. 여기서는 이미 EC2와 S3가 동일한 AWS 환경 내에 있으므로 불필요하며 비용과 복잡성만 증가합니다.



## #295
한 이커머스 회사가 AWS 클라우드에 테라바이트 규모의 고객 데이터를 저장하고 있습니다. 데이터에는 개인식별정보(PII)가 포함되어 있습니다. 회사는 이 데이터를 세 개의 애플리케이션에서 사용하려고 합니다. 이 중 한 개의 애플리케이션만 PII를 처리할 필요가 있습니다. 나머지 두 애플리케이션이 데이터를 처리하기 전에 PII는 제거되어야 합니다.

운영 오버헤드를 최소화하면서 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 데이터를 Amazon DynamoDB 테이블에 저장합니다. 각 애플리케이션이 요청하는 데이터를 가로채어 처리하는 프록시 애플리케이션 계층을 생성합니다.
B. 데이터를 Amazon S3 버킷에 저장합니다. S3 Object Lambda를 사용하여 데이터를 요청 애플리케이션에 반환하기 전에 데이터를 처리하고 변환합니다.
C. 데이터를 처리하여 변환된 데이터를 세 개의 개별 Amazon S3 버킷에 저장해 각 애플리케이션이 자체 커스텀 데이터셋을 갖도록 합니다. 각 애플리케이션을 해당 S3 버킷을 가리키도록 설정합니다.
D. 데이터를 처리하여 변환된 데이터를 세 개의 개별 Amazon DynamoDB 테이블에 저장해 각 애플리케이션이 자체 커스텀 데이터셋을 갖도록 합니다. 각 애플리케이션을 해당 DynamoDB 테이블을 가리키도록 설정합니다.

```
An ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains personally identifiable information (PII). The company wants to use the data in three applications. Only one of the applications needs to process the PII. The PII must be removed before the other two applications process the data.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and process the data that each application requests.
- B. Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.
- C. Process the data and store the transformed data in three separate Amazon S3 buckets so that each application has its own custom dataset. Point each application to its respective S3 bucket.
- D. Process the data and store the transformed data in three separate Amazon DynamoDB tables so that each application has its own custom dataset. Point each application to its respective DynamoDB table.
```

정답 : `B`

- 운영 오버헤드 최소화: 데이터를 한 번만 저장(S3)하고, 요청 시점에 S3 Object Lambda로 PII 마스킹/삭제 수행 → 별도의 파이프라인/복제본 관리 불필요
- 유연한 접근 분리
	- PII가 필요한 애플리케이션은 일반 S3/액세스 포인트로 원본 접근
	- PII가 불필요한 두 애플리케이션은 Object Lambda 액세스 포인트로 접근해 자동으로 정제된 데이터 수신
- 최소 권한 IAM과 액세스 포인트 정책으로 원본과 정제본 접근을 명확히 분리 가능

오답 이유

- **A (프록시 애플리케이션 계층)**: 중간 계층 개발/운영(확장성, 장애, 패치) 오버헤드 증가. S3 Object Lambda가 제공하는 기능을 재구현하게 됨.
    
- **C (S3에 세 개 버킷/사본 관리)**: 다중 사본 생성/동기화/수명주기 관리 필요 → 비용 및 운영 복잡도 증가.
    
- **D (DynamoDB에 세 개 테이블)**: 객체 데이터에 부적합하며, 여전히 사본을 여러 개 관리해야 하므로 오버헤드 큼.


## #296
개발팀이 Amazon EC2 인스턴스에서 호스팅되는 새 애플리케이션을 개발 VPC 내에 배포했습니다. 솔루션스 아키텍트는 동일한 계정에 새 VPC를 생성해야 합니다. 이 새 VPC는 개발 VPC와 피어링될 예정입니다. 개발 VPC의 CIDR 블록은 192.168.0.0/24입니다. 솔루션스 아키텍트는 새 VPC에 대해 CIDR 블록을 생성해야 합니다. 이 CIDR 블록은 개발 VPC와 VPC 피어링 연결에 유효해야 합니다.

이 요구 사항을 충족하는 가장 작은 CIDR 블록은 무엇입니까?

A. 10.0.1.0/32  
B. 192.168.0.0/24  
C. 192.168.1.0/32  
D. 10.0.1.0/24  

```
A development team has launched a new application that is hosted on Amazon EC2 instances inside a development VPC. A solutions architect needs to create a new VPC in the same account. The new VPC will be peered with the development VPC. The VPC CIDR block for the development VPC is 192.168.0.0/24. The solutions architect needs to create a CIDR block for the new VPC. The CIDR block must be valid for a VPC peering connection to the development VPC.  
  
What is the SMALLEST CIDR block that meets these requirements?

- A. 10.0.1.0/32
- B. 192.168.0.0/24
- C. 192.168.1.0/32
- D. 10.0.1.0/24
```

정답 : `D`

- VPC 피어링 조건: 두 VPC 간에 CIDR 블록이 겹치면 안 됨. 중복되는 주소 범위가 있으면 라우팅 충돌로 인해 피어링 연결 불가
- 현재 개발 VPC: 192.168.0.0/24 → 따라서 새 VPC는 192.168.0.0/24 범위와 겹치지 않는 별도의 CIDR
- CIDR 최소 크기: VPC에서 허용되는 최소 크기는 /28 (16 IP)이며 /32 (1 IP)는 유효하지 않음
- D(10.0.1.0/24) → 유효한 VPC CIDR, 기존 192.168.0.0/24와 겹치지 않음

오답 이유

- **A (10.0.1.0/32)** → /32는 유효한 VPC CIDR 아님 (1개 IP만 포함 → 불가능).
        
- **B (192.168.0.0/24)** → 기존 VPC와 동일 CIDR → 충돌 발생, 불가능.
        
- **C (192.168.1.0/32)** → /32는 유효하지 않음.


## #297
한 회사가 애플리케이션을 5개의 Amazon EC2 인스턴스에 배포했습니다. Application Load Balancer(ALB)가 타깃 그룹을 사용하여 인스턴스들로 트래픽을 분산합니다. 각 인스턴스의 평균 CPU 사용률은 대부분의 시간 동안 10% 미만이며, 가끔 65%까지 급증합니다.

솔루션스 아키텍트는 애플리케이션의 확장성을 자동화하는 솔루션을 구현해야 합니다. 이 솔루션은 아키텍처 비용을 최적화해야 하며, 급증이 발생할 때 애플리케이션이 충분한 CPU 리소스를 갖도록 해야 합니다.

다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. CPUUtilization 지표가 20% 미만일 때 ALARM 상태가 되도록 Amazon CloudWatch 알람을 생성합니다. CloudWatch 알람이 호출하는 AWS Lambda 함수를 생성하여 ALB 타깃 그룹의 EC2 인스턴스 중 하나를 종료합니다.
B. EC2 Auto Scaling 그룹을 생성합니다. 기존 ALB를 로드 밸런서로 선택하고 기존 타깃 그룹을 타깃 그룹으로 선택합니다. ASGAverageCPUUtilization 지표를 기반으로 하는 타깃 추적 스케일링 정책을 설정합니다. 최소 인스턴스를 2, 원하는 용량을 3, 최대 인스턴스를 6, 타깃 값을 50%로 설정합니다. EC2 인스턴스들을 Auto Scaling 그룹에 추가합니다.
C. EC2 Auto Scaling 그룹을 생성합니다. 기존 ALB를 로드 밸런서로 선택하고 기존 타깃 그룹을 타깃 그룹으로 선택합니다. 최소 인스턴스를 2, 원하는 용량을 3, 최대 인스턴스를 6으로 설정합니다. EC2 인스턴스들을 Auto Scaling 그룹에 추가합니다.
D. 두 개의 Amazon CloudWatch 알람을 생성합니다. 첫 번째 CloudWatch 알람은 평균 CPUUtilization 지표가 20% 미만일 때 ALARM 상태가 되도록 구성합니다. 두 번째 CloudWatch 알람은 평균 CPUUtilization 지표가 50%를 초과할 때 ALARM 상태가 되도록 구성합니다. 알람이 Amazon Simple Notification Service(Amazon SNS) 주제로 게시하여 이메일 메시지를 보내도록 구성합니다. 메시지를 받은 후 로그인하여 실행 중인 EC2 인스턴스 수를 줄이거나 늘립니다.

```
A company deploys an application on five Amazon EC2 instances. An Application Load Balancer (ALB) distributes traffic to the instances by using a target group. The average CPU usage on each of the instances is below 10% most of the time, with occasional surges to 65%.  
  
A solutions architect needs to implement a solution to automate the scalability of the application. The solution must optimize the cost of the architecture and must ensure that the application has enough CPU resources when surges occur.  
  
Which solution will meet these requirements?

- A. Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization metric is less than 20%. Create an AWS Lambda function that the CloudWatch alarm invokes to terminate one of the EC2 instances in the ALB target group.
- B. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.
- C. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set the minimum instances to 2, the desired capacity to 3, and the maximum instances to 6. Add the EC2 instances to the Auto Scaling group.
- D. Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the ALARM state when the average CPUUtilization metric is below 20%. Configure the second CloudWatch alarm to enter the ALARM state when the average CPUUtilization matric is above 50%. Configure the alarms to publish to an Amazon Simple Notification Service (Amazon SNS) topic to send an email message. After receiving the message, log in to decrease or increase the number of EC2 instances that are running.
```

정답 : `B`

- 자동 확장/축소: 타깃 추적 정책은 집계 CPU가 목표값(예: 50%) 근처가 되도록 인스턴스 수를 자동으로 증감.
	- 급증 시 최대(65%) 신속히 스케일 아웃해 CPU 여유 확보, 평시(10%) 미만에는 스케일 인으로 비용 최적화
- ALB 연동: 오토 스케일링 그룹을 ALB 타깃 그룹과 연동하면 새로 기동된 인스턴스가 자동 등록/헬스체크 되어 무중단으로 용량 조절
- 경계값 설정: 최소 2, 원하는 용량을 3으로 기본 가용성을 확보하고, 최대 6으로 상한을 지정해 예산 통제

오답 이유

- **A**: 낮은 CPU에서 인스턴스를 종료하는 **수작업성 높은 커스텀 로직**이며, 급증 시 자동 스케일 아웃이 없습니다. 비용·안정성·운영성 모두 미흡.
    
- **C**: Auto Scaling 그룹만 만들고 **스케일링 정책이 없음** → 자동 확장/축소가 일어나지 않아 요구(자동화) 불충족.
    
- **D**: 알람→이메일→사람이 로그인해 조정하는 **수동 운영**입니다. 자동/탄력 요구와 비용 최적화 목표에 부적합.

## #298
한 회사가 중요 비즈니스 애플리케이션을 Application Load Balancer 뒤의 Amazon EC2 인스턴스에서 실행하고 있습니다. EC2 인스턴스는 Auto Scaling 그룹에서 실행되며, Amazon RDS DB 인스턴스에 접근합니다.

현재 설계는 EC2 인스턴스와 DB 인스턴스가 모두 단일 가용 영역(Availability Zone)에 위치해 있어 운영 검토를 통과하지 못했습니다. 솔루션스 아키텍트는 두 번째 가용 영역을 사용하도록 설계를 업데이트해야 합니다.

애플리케이션을 고가용성으로 만들 수 있는 솔루션은 무엇입니까?

A. 각 가용 영역에 서브넷을 프로비저닝합니다. Auto Scaling 그룹이 두 가용 영역에 EC2 인스턴스를 분산하도록 구성합니다. DB 인스턴스를 각 네트워크에 대한 연결로 구성합니다.
B. 두 가용 영역 전체에 걸쳐 확장되는 두 개의 서브넷을 프로비저닝합니다. Auto Scaling 그룹이 두 가용 영역에 EC2 인스턴스를 분산하도록 구성합니다. DB 인스턴스를 각 네트워크에 대한 연결로 구성합니다.
C. 각 가용 영역에 서브넷을 프로비저닝합니다. Auto Scaling 그룹이 두 가용 영역에 EC2 인스턴스를 분산하도록 구성합니다. DB 인스턴스를 Multi-AZ 배포로 구성합니다.
D. 두 가용 영역 전체에 걸쳐 확장되는 서브넷을 프로비저닝합니다. Auto Scaling 그룹이 두 가용 영역에 EC2 인스턴스를 분산하도록 구성합니다. DB 인스턴스를 Multi-AZ 배포로 구성합니다.

```
A company is running a critical business application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances run in an Auto Scaling group and access an Amazon RDS DB instance.  
  
The design did not pass an operational review because the EC2 instances and the DB instance are all located in a single Availability Zone. A solutions architect must update the design to use a second Availability Zone.  
  
Which solution will make the application highly available?

- A. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.
- B. Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.
- C. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.
- D. Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.
```

정답 : `C`

- 서브넷은 AZ를 걸쳐서 확장될 수 없음. 각 AZ에 별도 서브넷을 만들어야 함
- 오토 스케일링 그룹(ASG) 을 두 AZ에 걸쳐 배치하면 한 AZ 장애 시 다른 AZ 인스턴스가 트래픽을 계속 처리 가능
- RDS Multi-AZ 배포는 동기식 스탠바이(다른 AZ)로 고가용성을 제공하며, AZ 장애 시 자동 페일오버로 데이터 손실 없이 빠르게 복구 가능

오답 이유

- **A**: “DB 인스턴스를 각 네트워크에 대한 연결로 구성”은 RDS에 해당 개념이 없습니다. 고가용성은 **Multi-AZ** 로 구성해야 합니다.
    
- **B**: “두 AZ 전체에 걸쳐 확장되는 서브넷”은 **불가능**합니다. 서브넷은 **단일 AZ에만** 존재합니다. 게다가 DB의 HA 설정이 없음.
    
- **D**: 마찬가지로 **서브넷은 AZ를 걸쳐서 확장**할 수 없습니다. RDS는 Multi-AZ로 옳지만, 서브넷 전제가 잘못되었습니다.


## #299
한 연구소는 약 8TB의 데이터를 처리해야 합니다. 연구소는 스토리지 서브시스템에 대해 서브 밀리초(latency)와 최소 6GBps의 처리량(throughput)을 요구합니다. 수백 대의 Amazon Linux를 실행하는 Amazon EC2 인스턴스가 데이터를 분산 처리할 예정입니다.

어떤 솔루션이 성능 요구 사항을 충족합니까?

A. Amazon FSx for NetApp ONTAP 파일 시스템을 생성합니다. 각 볼륨의 티어링 정책을 ALL로 설정합니다. 원시 데이터를 파일 시스템으로 가져옵니다. 파일 시스템을 EC2 인스턴스에 마운트합니다.
B. 원시 데이터를 저장할 Amazon S3 버킷을 생성합니다. 지속성 SSD 스토리지를 사용하는 Amazon FSx for Lustre 파일 시스템을 생성합니다. Amazon S3에서 데이터 가져오기 및 Amazon S3로 데이터 내보내기 옵션을 선택합니다. 파일 시스템을 EC2 인스턴스에 마운트합니다.
C. 원시 데이터를 저장할 Amazon S3 버킷을 생성합니다. 지속성 HDD 스토리지를 사용하는 Amazon FSx for Lustre 파일 시스템을 생성합니다. Amazon S3에서 데이터 가져오기 및 Amazon S3로 데이터 내보내기 옵션을 선택합니다. 파일 시스템을 EC2 인스턴스에 마운트합니다.
D. Amazon FSx for NetApp ONTAP 파일 시스템을 생성합니다. 각 볼륨의 티어링 정책을 NONE으로 설정합니다. 원시 데이터를 파일 시스템으로 가져옵니다. 파일 시스템을 EC2 인스턴스에 마운트합니다.

```
A research laboratory needs to process approximately 8 TB of data. The laboratory requires sub-millisecond latencies and a minimum throughput of 6 GBps for the storage subsystem. Hundreds of Amazon EC2 instances that run Amazon Linux will distribute and process the data.  
  
Which solution will meet the performance requirements?

- A. Create an Amazon FSx for NetApp ONTAP file system. Sat each volume’ tiering policy to ALL. Import the raw data into the file system. Mount the fila system on the EC2 instances.
- B. Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent SSD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.
- C. Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent HDD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.
- D. Create an Amazon FSx for NetApp ONTAP file system. Set each volume’s tiering policy to NONE. Import the raw data into the file system. Mount the file system on the EC2 instances.
```

정답 : `B`

- FSx for Lustre는 HPC/분산 처리 워크로드용 병렬 파일 시스템으로 서브 ms 대기시간과 수십~수백 GB/s로 확장 가능한 처리량 제공
- 지속성(Persistent) SSD 옵션은 높은 IOPS/처리량 요구 (최소 6 GBps)를 안정적으로 충족
- 수백 대 EC2가 동시에 마운트해도 스케일 아웃되어 높은 집계 처리량 제공
- S3에서 가져오기/내보내기 옵션으로 원본을 S3에 두고, 작업 시 Lustre로 빠르게 조회/처리 후 결과를 다시 S3로 내보낼 수 있어 운영이 간단

오답 이유

- **A/D: FSx for NetApp ONTAP**
    - ONTAP는 범용 NAS(다중 프로토콜, 스냅샷 등)에 강점이 있으나, HPC급 병렬 처리에서 요구하는 **수백 클라이언트 동시 처리 시의 집계 처리량/대기시간** 면에서는 Lustre가 적합합니다. 티어링 정책(ALL/NONE) 조정과 무관하게 **요구된 최소 6 GBps**와 서브 ms 대기시간을 **수백 클라이언트 규모로** 일관되게 충족하기 어렵습니다.
    
- **C: FSx for Lustre 지속성 HDD**
    - HDD는 **IOPS/대기시간**이 SSD 대비 부족합니다. 서브 ms 대기시간과 6 GBps 처리량 요구를 안정적으로 만족시키기 어렵습니다.


## #300
회사는 하드웨어 용량 제약 때문에 온프레미스 데이터 센터에서 AWS 클라우드로 레거시 애플리케이션을 마이그레이션해야 합니다. 애플리케이션은 24시간 365일 실행됩니다. 애플리케이션의 데이터베이스 스토리지는 시간이 지남에 따라 계속 증가합니다.

가장 비용 효율적으로 이러한 요구 사항을 충족하려면 솔루션스 아키텍트는 무엇을 해야 합니까?

A. 애플리케이션 계층을 Amazon EC2 스팟 인스턴스로 마이그레이션합니다. 데이터 스토리지 계층을 Amazon S3로 마이그레이션합니다.
B. 애플리케이션 계층을 Amazon EC2 예약 인스턴스로 마이그레이션합니다. 데이터 스토리지 계층을 Amazon RDS 온디맨드 인스턴스로 마이그레이션합니다.
C. 애플리케이션 계층을 Amazon EC2 예약 인스턴스로 마이그레이션합니다. 데이터 스토리지 계층을 Amazon Aurora 예약 인스턴스로 마이그레이션합니다.
D. 애플리케이션 계층을 Amazon EC2 온디맨드 인스턴스로 마이그레이션합니다. 데이터 스토리지 계층을 Amazon RDS 예약 인스턴스로 마이그레이션합니다.

```
A company needs to migrate a legacy application from an on-premises data center to the AWS Cloud because of hardware capacity constraints. The application runs 24 hours a day, 7 days a week. The application’s database storage continues to grow over time.  
  
What should a solutions architect do to meet these requirements MOST cost-effectively?

- A. Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to Amazon S3.
- B. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon RDS On-Demand Instances.
- C. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.
- D. Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage layer to Amazon RDS Reserved Instances.
```

정답 : `C`

- 24x7 상시 구동 워크로드는 사용량이 예측 가능하므로 EC2 예약 인스턴스(RI)가 온디맨드 대비 큰 비용 절감 제공
- 데이터베이스 스토리지가 지속 증가 → Amazon Aurora는 스토리지가 자동 확장(최대 수십 TB)되며, 예약 인스턴스로 DB 컴퓨팅 비용까지 절감 가능
	- 운영･비용 측면 모두 유리

오답 이유

- **A. EC2 스팟 + S3**: 스팟은 **중단 가능**하여 24×7 레거시 서비스에 부적합. 또한 관계형 DB를 S3로 대체할 수 없습니다(객체 스토리지).
    
- **B. EC2 RI + RDS 온디맨드**: DB도 상시 구동이므로 **RDS 온디맨드**보다 **예약 인스턴스**가 비용 효율적입니다.
    
- **D. EC2 온디맨드 + RDS RI**: 컴퓨팅이 24×7임에도 온디맨드는 비용 최적화에 불리합니다.