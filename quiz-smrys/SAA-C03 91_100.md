---
created: 2025-09-26 11:48:32
last_modified: 2025-09-26 18:07:27
---
## #91
한 회사는 VPC 내 Amazon EC2 인스턴스에서 애플리케이션을 실행합니다. 그 애플리케이션 중 하나는 Amazon S3 API를 호출하여 객체를 저장하고 읽어야 합니다. 회사의 보안 규정에 따르면, 애플리케이션에서 나가는 트래픽은 인터넷을 경유할 수 없습니다.  
어떤 솔루션이 이러한 요구 사항을 충족합니까?

- A. S3 게이트웨이 엔드포인트를 구성합니다.
- B. S3 버킷을 프라이빗 서브넷에 생성합니다.
- C. S3 버킷을 EC2 인스턴스와 동일한 AWS 리전에 생성합니다.
- D. EC2 인스턴스와 동일한 서브넷에 NAT 게이트웨이를 구성합니다.

```
A company has applications that run on Amazon EC2 instances in a VPC. One of the applications needs to call the Amazon S3 API to store and read objects. According to the company's security regulations, no traffic from the applications is allowed to travel across the internet.  
Which solution will meet these requirements?

- A. Configure an S3 gateway endpoint.
- B. Create an S3 bucket in a private subnet.
- C. Create an S3 bucket in the same AWS Region as the EC2 instances.
- D. Configure a NAT gateway in the same subnet as the EC2 instances.
```

정답 : `A`

- S3 게이트웨이 엔드포인트(VPC Endpoint)를 사용하면, 트래픽이 인터넷을 거치지 않고 AWS 네트워크 내부에서 Amazon S3에 안전하게 연결
- 회사 규정에서 요구하는 인터넷 경유 금지 조건 충족
- VPC 라우팅 테이블에 엔드포인트를 추가하고, S3 버킷 정책과 보안그룹을 적절히 구성하면 EC2 인스턴스에서 안전하게 S3에 접근

오답 이유

- **B. S3 버킷을 프라이빗 서브넷에 생성**
	- S3는 리전 기반 글로벌 서비스이며, 서브넷 내에 직접 생성할 수 없습니다. 따라서 접근 제한 목적을 달성할 수 없습니다.
    
- **C. S3 버킷을 동일 리전에 생성**
	- 동일 리전에 있다고 해도 인터넷 없이 접근 가능하다는 보장은 없습니다. 기본적으로 S3 접근은 퍼블릭 인터넷을 경유할 수 있습니다.
    
- **D. NAT 게이트웨이 구성**
	- NAT 게이트웨이는 프라이빗 서브넷의 인스턴스가 인터넷으로 나가도록 허용합니다. 회사 규정에서 인터넷 트래픽을 금지했기 때문에 부적합합니다.


## #92
한 회사는 민감한 사용자 정보를 Amazon S3 버킷에 저장하고 있습니다. 회사는 VPC 내부에서 실행되는 Amazon EC2 인스턴스의 애플리케이션 계층에서 이 버킷에 안전하게 접근할 수 있도록 하고 싶습니다.  
이 목표를 달성하기 위해 솔루션 아키텍트가 수행해야 할 단계 조합은 무엇입니까? (두 가지 선택)

- A. VPC 내에서 Amazon S3용 게이트웨이 엔드포인트를 구성합니다.
- B. S3 버킷의 객체를 공개하도록 버킷 정책을 생성합니다.
- C. VPC에서 실행되는 애플리케이션 계층에만 접근을 제한하는 버킷 정책을 생성합니다.
- D. S3 접근 정책이 적용된 IAM 사용자를 생성하고 IAM 자격 증명을 EC2 인스턴스에 복사합니다.
- E. NAT 인스턴스를 생성하고 EC2 인스턴스가 NAT 인스턴스를 통해 S3 버킷에 접근하도록 구성합니다.

```
A company is storing sensitive user information in an Amazon S3 bucket. The company wants to provide secure access to this bucket from the application tier running on Amazon EC2 instances inside a VPC.  
Which combination of steps should a solutions architect take to accomplish this? (Choose two.)

- A. Configure a VPC gateway endpoint for Amazon S3 within the VPC.
- B. Create a bucket policy to make the objects in the S3 bucket public.
- C. Create a bucket policy that limits access to only the application tier running in the VPC.
- D. Create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance.
- E. Create a NAT instance and have the EC2 instances use the NAT instance to access the S3 bucket.
```


정답 : `A, C`

- **A. VPC 게이트웨이 엔드포인트 구성**: EC2 인스턴스가 인터넷을 경유하지 않고 안전하게 S3에 접근할 수 있게 함.
- **C. 버킷 정책으로 VPC 애플리케이션 계층 접근 제한**: S3 버킷에 대한 접근을 특정 VPC 또는 보안 주체(IAM 역할/사용자)로 제한하여 보안을 강화함.
- 이 두 가지 조치를 함께 사용하면 민감한 데이터를 안전하게 보호하면서 VPC 내부 EC2 인스턴스에서 S3를 접근할 수 있음.

오답 이유

- **B. S3 버킷 공개**
	- 민감한 데이터를 포함한 버킷을 공개하면 보안 규정 위반이므로 부적합.
    
- **D. IAM 사용자와 자격 증명 복사**
	- EC2 인스턴스에 자격 증명을 저장하는 것은 보안 위험이 크며, IAM 역할을 통한 접근이 권장됨.
    
- **E. NAT 인스턴스 사용**
	- NAT를 통한 접근은 인터넷 경유를 허용하므로 보안 규정상 부적합하며 비용과 관리 부담이 증가함.


## #93
한 회사는 MySQL 데이터베이스를 사용하는 온프레미스 애플리케이션을 운영하고 있습니다. 회사는 애플리케이션의 탄력성과 가용성을 높이기 위해 애플리케이션을 AWS로 마이그레이션하고자 합니다.  
현재 아키텍처에서는 정상 운영 시 데이터베이스에서 읽기 작업이 많습니다. 개발팀은 4시간마다 프로덕션 데이터베이스의 전체 내보내기(full export)를 수행하여 스테이징 환경의 데이터베이스를 채웁니다. 이 동안 사용자들은 애플리케이션 지연(latency)이 허용 불가 수준으로 증가합니다. 개발팀은 이 절차가 완료될 때까지 스테이징 환경을 사용할 수 없습니다.  
솔루션 아키텍트는 애플리케이션 지연 문제를 완화하고, 개발팀이 스테이징 환경을 지연 없이 계속 사용할 수 있는 교체 아키텍처를 추천해야 합니다.  
이 요구사항을 충족하는 솔루션은 무엇입니까?

- A. 프로덕션용으로 Multi-AZ Aurora Replica가 있는 Amazon Aurora MySQL을 사용합니다. 스테이징 데이터베이스를 채우기 위해 mysqldump 유틸리티를 사용한 백업 및 복원 프로세스를 구현합니다.
- B. 프로덕션용으로 Multi-AZ Aurora Replica가 있는 Amazon Aurora MySQL을 사용합니다. 데이터베이스 클로닝(database cloning)을 사용하여 온디맨드로 스테이징 데이터베이스를 생성합니다.
- C. 프로덕션용으로 Multi-AZ 배포 및 읽기 전용 리플리카(read replica)가 있는 Amazon RDS for MySQL을 사용합니다. 스테이징 데이터베이스용으로 스탠바이 인스턴스를 사용합니다.
- D. 프로덕션용으로 Multi-AZ 배포 및 읽기 전용 리플리카(read replica)가 있는 Amazon RDS for MySQL을 사용합니다. 스테이징 데이터베이스를 채우기 위해 mysqldump 유틸리티를 사용한 백업 및 복원 프로세스를 구현합니다.

```
A company runs an on-premises application that is powered by a MySQL database. The company is migrating the application to AWS to increase the application's elasticity and availability.  
The current architecture shows heavy read activity on the database during times of normal operation. Every 4 hours, the company's development team pulls a full export of the production database to populate a database in the staging environment. During this period, users experience unacceptable application latency. The development team is unable to use the staging environment until the procedure completes.  
A solutions architect must recommend replacement architecture that alleviates the application latency issue. The replacement architecture also must give the development team the ability to continue using the staging environment without delay.  
Which solution meets these requirements?

- A. Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Populate the staging database by implementing a backup and restore process that uses the mysqldump utility.
- B. Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Use database cloning to create the staging database on-demand.
- C. Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Use the standby instance for the staging database.
- D. Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Populate the staging database by implementing a backup and restore process that uses the mysqldump utility.
```

정답 : `B`

- Amazon Aurora MySQL의 **데이터베이스 클로닝(Database Cloning)** 기능은 프로덕션 데이터베이스의 스냅샷을 기반으로 **몇 초 만에 스테이징 환경을 생성**할 수 있습니다.
- 이를 통해 개발팀은 스테이징 데이터베이스를 지연 없이 사용 가능하며, 프로덕션 DB의 **읽기 성능에도 영향을 주지 않습니다.**
- Multi-AZ Aurora Replicas는 고가용성과 읽기 확장성을 제공하여 기존 문제였던 읽기 부하로 인한 지연도 완화됩니다.


오답 이유

- **A. mysqldump 기반 백업/복원**
	- 전체 덤프를 수행하면 프로덕션 DB에 읽기 부하가 증가하며, 스테이징 생성에도 시간이 오래 걸려 지연 문제 해결에 부적합.
    
- **C. RDS Multi-AZ 및 스탠바이 인스턴스 사용**
	- 스탠바이 인스턴스는 장애 조치(failover)용으로만 사용되므로 스테이징 DB로 직접 사용 불가.
    
- **D. RDS Multi-AZ + mysqldump**
	- mysqldump 사용은 A와 동일하게 **시간 소모와 부하 증가 문제**가 여전히 존재.


## #94
한 회사는 사용자가 Amazon S3에 작은 파일을 업로드하는 애플리케이션을 설계하고 있습니다. 사용자가 파일을 업로드한 후, 파일은 데이터를 변환하고 JSON 형식으로 저장하기 위한 **일회성 단순 처리(one-time simple processing)**가 필요합니다.  
각 파일은 업로드 직후 **가능한 한 빠르게 처리**되어야 합니다. 수요는 변동적입니다. 어떤 날에는 사용자가 많은 파일을 업로드하고, 어떤 날에는 몇 개 또는 전혀 업로드하지 않을 수 있습니다.  
이 요구사항을 **가장 적은 운영 오버헤드로 충족하는 솔루션**은 무엇입니까?

- A. Amazon EMR을 구성하여 Amazon S3에서 텍스트 파일을 읽습니다. 데이터를 변환하는 스크립트를 실행합니다. 결과 JSON 파일을 Amazon Aurora DB 클러스터에 저장합니다.
- B. Amazon S3를 구성하여 이벤트 알림을 Amazon Simple Queue Service(Amazon SQS) 큐로 전송합니다. Amazon EC2 인스턴스를 사용하여 큐에서 읽고 데이터를 처리합니다. 결과 JSON 파일을 Amazon DynamoDB에 저장합니다.
- C. Amazon S3를 구성하여 이벤트 알림을 Amazon Simple Queue Service(Amazon SQS) 큐로 전송합니다. AWS Lambda 함수를 사용하여 큐에서 읽고 데이터를 처리합니다. 결과 JSON 파일을 Amazon DynamoDB에 저장합니다.
- D. Amazon EventBridge(Amazon CloudWatch Events)를 구성하여 새 파일이 업로드될 때 이벤트를 Amazon Kinesis Data Streams로 전송합니다. AWS Lambda 함수를 사용하여 스트림에서 이벤트를 소비하고 데이터를 처리합니다. 결과 JSON 파일을 Amazon Aurora DB 클러스터에 저장합니다.

```
A company is designing an application where users upload small files into Amazon S3. After a user uploads a file, the file requires one-time simple processing to transform the data and save the data in JSON format for later analysis.  
Each file must be processed as quickly as possible after it is uploaded. Demand will vary. On some days, users will upload a high number of files. On other days, users will upload a few files or no files.  
Which solution meets these requirements with the LEAST operational overhead?

- A. Configure Amazon EMR to read text files from Amazon S3. Run processing scripts to transform the data. Store the resulting JSON file in an Amazon Aurora DB cluster.
- B. Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use Amazon EC2 instances to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB.
- C. Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB.
- D. Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon Kinesis Data Streams when a new file is uploaded. Use an AWS Lambda function to consume the event from the stream and process the data. Store the resulting JSON file in an Amazon Aurora DB cluster.
```

정답 : `C`

- 요구사항: **파일 업로드 직후 빠른 처리**, **변동적 수요**, **최소 운영 오버헤드**.
- **S3 이벤트 → SQS → Lambda** 조합은 서버리스 방식으로 **자동 확장(auto-scaling)**되며, 사용자가 적게 업로드하든 많이 업로드하든 Lambda가 처리량에 맞춰 자동으로 실행됩니다.
- EC2나 EMR처럼 서버를 운영하거나 관리할 필요가 없으므로 **운영 오버헤드가 최소화**됩니다.
- 처리 결과를 DynamoDB에 저장하면 JSON 데이터를 빠르게 조회할 수 있어 분석 용도로 적합합니다.

오답 이유

- **A. EMR + Aurora**
	- EMR 클러스터를 항상 켜야 하거나, 클러스터 시작/종료 오버헤드가 발생하여 단순 파일 처리에 과도함. 운영 오버헤드가 높음.
    
- **B. S3 → SQS → EC2 + DynamoDB**
	- EC2 인스턴스 관리, Auto Scaling 설정 필요. 서버리스보다 운영 부담이 큼.
    
- **D. EventBridge → Kinesis → Lambda + Aurora**:
	- Kinesis 스트림과 Lambda 조합은 처리량 제어 가능하지만, 스트림 관리와 Aurora 관리 비용이 있어 단순 처리 요구에 과도. 운영 오버헤드가 높음.


## #95
한 애플리케이션은 회사 본사 사용자가 제품 데이터에 접근할 수 있도록 허용합니다. 제품 데이터는 Amazon RDS MySQL DB 인스턴스에 저장됩니다. 운영팀은 애플리케이션 성능 저하를 격리하여 읽기 트래픽과 쓰기 트래픽을 분리하고자 합니다. 솔루션스 아키텍트는 애플리케이션 성능을 **신속하게 최적화**해야 합니다.  
솔루션스 아키텍트가 권장해야 하는 방법은 무엇입니까?

- A. 기존 데이터베이스를 Multi-AZ 배포로 변경합니다. 읽기 요청은 기본 가용 영역에서 제공합니다.  
- B. 기존 데이터베이스를 Multi-AZ 배포로 변경합니다. 읽기 요청은 보조 가용 영역에서 제공합니다.  
- C. 데이터베이스에 읽기 전용 복제본(read replica)을 생성합니다. 읽기 복제본을 원본 데이터베이스의 계산 및 저장 용량의 절반으로 구성합니다.  
- D. 데이터베이스에 읽기 전용 복제본(read replica)을 생성합니다. 읽기 복제본을 원본 데이터베이스와 동일한 계산 및 저장 용량으로 구성합니다.

```
An application allows users at a company's headquarters to access product data. The product data is stored in an Amazon RDS MySQL DB instance. The operations team has isolated an application performance slowdown and wants to separate read traffic from write traffic. A solutions architect needs to optimize the application's performance quickly.  
What should the solutions architect recommend?

- A. Change the existing database to a Multi-AZ deployment. Serve the read requests from the primary Availability Zone.
- B. Change the existing database to a Multi-AZ deployment. Serve the read requests from the secondary Availability Zone.
- C. Create read replicas for the database. Configure the read replicas with half of the compute and storage resources as the source database.
- D. Create read replicas for the database. Configure the read replicas with the same compute and storage resources as the source database.
```

정답 : `D`

- 요구사항: 읽기 트래픽과 쓰기 트래픽 분리, 애플리케이션 성능 **즉시 최적화**.
- **RDS Multi-AZ**는 고가용성(HA)을 제공하지만 읽기 요청 분산에는 적합하지 않습니다. 보조 AZ는 읽기를 처리하지 않고 대기용 스탠바이입니다.
- 읽기 트래픽을 분리하려면 **읽기 전용 복제본(Read Replica)**을 생성해야 합니다.
- 복제본의 용량을 원본과 동일하게 설정하면 읽기 트래픽 급증 시에도 충분한 성능을 제공합니다.
- 따라서 D가 가장 신속하고 효과적인 성능 최적화 방법입니다.

오답 이유

- **A. Multi-AZ 기본 AZ 읽기**: 기본 AZ는 쓰기와 읽기 트래픽을 모두 처리하므로 읽기 분리 불가. 성능 개선 효과 없음.
- **B. Multi-AZ 보조 AZ 읽기**: 보조 AZ는 스탠바이이며, 읽기 요청 처리 불가. 읽기 트래픽 분리 목적과 맞지 않음.
- **C. 읽기 복제본, 용량 절반**: 복제본 용량을 줄이면 읽기 트래픽 분리 목적은 달성되지만, 읽기 트래픽 급증 시 성능이 부족할 수 있음. 최적화 효과가 제한적임.


## #96
Amazon EC2 관리자가 여러 사용자가 포함된 IAM 그룹에 연결된 다음 정책을 만들었습니다:  
```
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Effect": "Allow",
			"Action": "ec2:TerminateInstances",
			"Resource": "*",
			"Condition": {
				"IpAddress": {
					"aws:SourceIp": "10.100.100.0/24"
				}
			}
		},
		{
			"Effect": "Deny",
			"Action": "ec2:*",
			"Resource": "*",
			"Condition": {
				"StringNotEquals": {
					"ec2:Region": "us-east-1"
				}
			}
		}
	]
}

```
이 정책의 효과는 무엇입니까?

- A. 사용자는 us-east-1을 제외한 모든 AWS 리전에서 EC2 인스턴스를 종료할 수 있습니다.  
- B. 사용자는 us-east-1 리전에서 IP 주소가 10.100.100.1인 EC2 인스턴스를 종료할 수 있습니다.  
- C. 사용자는 사용자의 소스 IP가 10.100.100.254일 때 us-east-1 리전에서 EC2 인스턴스를 종료할 수 있습니다.  
- D. 사용자는 사용자의 소스 IP가 10.100.100.254일 때 us-east-1 리전에서 EC2 인스턴스를 종료할 수 없습니다.

```
An Amazon EC2 administrator created the following policy associated with an IAM group containing several users:  
---
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Effect": "Allow",
			"Action": "ec2:TerminateInstances",
			"Resource": "*",
			"Condition": {
				"IpAddress": {
					"aws:SourceIp": "10.100.100.0/24"
				}
			}
		},
		{
			"Effect": "Deny",
			"Action": "ec2:*",
			"Resource": "*",
			"Condition": {
				"StringNotEquals": {
					"ec2:Region": "us-east-1"
				}
			}
		}
	]
}
---
What is the effect of this policy?

- A. Users can terminate an EC2 instance in any AWS Region except us-east-1.
- B. Users can terminate an EC2 instance with the IP address 10.100.100.1 in the us-east-1 Region.
- C. Users can terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.100.100.254.
- D. Users cannot terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.100.100.254.
```

정답 : `C`

- 정책 첫 번째 문: **Allow ec2:TerminateInstances**는 소스 IP가 10.100.100.0/24일 경우에만 허용.
- 정책 두 번째 문: **Deny ec2:***는 **us-east-1이 아닌 리전**에서 모든 EC2 작업을 거부.
- IAM 정책 평가 순서:
    1. **Explicit Deny**가 가장 강력함 → 허용보다 우선.
    2. Allow와 Deny 조건이 충돌하면 Deny가 우선.
    
- 따라서:
    - us-east-1 리전에서는 Deny 조건이 적용되지 않음.
    - 소스 IP가 10.100.100.0/24 범위이면 Allow 조건이 적용됨.
    - 예: 10.100.100.254는 해당 서브넷에 포함됨 → EC2 인스턴스 종료 가능.

오답 이유

- **A.** us-east-1 제외한 리전에서 종료 가능?
    - 잘못됨. 정책의 Deny 조건 때문에 us-east-1이 아닌 리전에서는 모든 EC2 작업이 명시적 거부됨.
    
- **B.** IP가 10.100.100.1일 때 종료 가능?
    - SourceIp는 인스턴스 IP가 아닌 사용자의 소스 IP
    
- **D.** IP가 10.100.100.254일 때 종료 불가?
    - 잘못됨. Allow 조건이 적용되어 종료 가능.


## #97
한 회사는 온프레미스에서 Microsoft SharePoint를 대규모로 배포하고 있으며 Microsoft Windows 공유 파일 스토리지가 필요합니다.  
회사는 이 워크로드를 AWS 클라우드로 마이그레이션하고자 하며 다양한 스토리지 옵션을 고려하고 있습니다.  
스토리지 솔루션은 고가용성을 제공해야 하며, 접근 제어를 위해 Active Directory와 통합되어야 합니다.  
이 요구사항을 충족하는 솔루션은 무엇입니까?

- A. Amazon EFS 스토리지를 구성하고 인증을 위해 Active Directory 도메인을 설정합니다.  
- B. AWS Storage Gateway 파일 게이트웨이에 SMB 파일 공유를 생성하고 두 개의 가용 영역에서 구성합니다.  
- C. Amazon S3 버킷을 생성하고 Microsoft Windows Server에서 이를 볼륨으로 마운트하도록 구성합니다.  
- D. AWS에서 Amazon FSx for Windows File Server 파일 시스템을 생성하고 인증을 위해 Active Directory 도메인을 설정합니다.

```
A company has a large Microsoft SharePoint deployment running on-premises that requires Microsoft Windows shared file storage. The company wants to migrate this workload to the AWS Cloud and is considering various storage options. The storage solution must be highly available and integrated with Active Directory for access control.  
Which solution will satisfy these requirements?

- A. Configure Amazon EFS storage and set the Active Directory domain for authentication.
- B. Create an SMB file share on an AWS Storage Gateway file gateway in two Availability Zones.
- C. Create an Amazon S3 bucket and configure Microsoft Windows Server to mount it as a volume.
- D. Create an Amazon FSx for Windows File Server file system on AWS and set the Active Directory domain for authentication.
```

정답 : `D`

- Microsoft SharePoint는 **Windows 파일 시스템과 SMB 프로토콜** 기반의 파일 공유를 필요로 하며, Active Directory 통합이 필수적입니다.
- **Amazon FSx for Windows File Server**는 **SMB 프로토콜**, **Windows ACL**, **Active Directory 통합**, **멀티-AZ 고가용성**을 지원하여 SharePoint 워크로드와 완전히 호환됩니다.
- 따라서 SharePoint 마이그레이션 시 가장 적합하고 관리 부담이 적습니다.

오답 이유

- **A. Amazon EFS**
    - EFS는 **NFS 프로토콜**을 사용하며 Windows 기반 SMB/NTFS 액세스와 호환되지 않음.
    - Active Directory와 통합이 가능하지만, Windows 워크로드에는 적합하지 않음.
    
- **B. AWS Storage Gateway 파일 게이트웨이**
    - 파일 게이트웨이는 온프레미스와 클라우드 간 파일 캐싱을 위한 솔루션이며, **SMB 액세스를 제공하지만 완전 관리형 Windows 파일 서버와 달리 고가용성/AD 통합**은 제한적임.
    
- **C. Amazon S3 마운트**
    - Windows에서 S3를 볼륨으로 마운트하려면 서드파티 솔루션 필요.
    - S3는 객체 스토리지이며, SharePoint에서 요구하는 **SMB, NTFS, ACL, 고가용성**을 기본적으로 제공하지 않음.


## #98
이미지 처리 회사는 사용자가 이미지를 업로드하는 웹 애플리케이션을 운영하고 있습니다.  
애플리케이션은 이미지를 Amazon S3 버킷에 업로드합니다.  
회사는 S3 이벤트 알림을 설정하여 객체 생성 이벤트를 Amazon Simple Queue Service (Amazon SQS) 표준 큐로 게시하도록 구성했습니다.  
SQS 큐는 이미지를 처리하고 결과를 이메일로 사용자에게 전송하는 AWS Lambda 함수의 이벤트 소스로 사용됩니다.  
사용자들은 업로드한 이미지마다 여러 개의 이메일을 받고 있다고 보고했습니다.  
솔루션스 아키텍트는 SQS 메시지가 Lambda 함수를 여러 번 호출하여 여러 이메일 메시지가 전송되는 문제를 확인했습니다.  
이 문제를 **가장 적은 운영 부담**으로 해결하려면 어떻게 해야 합니까?

- A. SQS 큐에서 롱 폴링을 설정하고 ReceiveMessage 대기 시간을 30초로 늘립니다.  
- B. SQS 표준 큐를 SQS FIFO 큐로 변경합니다. 메시지 중복 제거 ID를 사용하여 중복 메시지를 제거합니다.  
- C. SQS 큐의 가시성 타임아웃을 함수 타임아웃과 배치 윈도우 타임아웃의 합보다 크게 증가시킵니다.  
- D. Lambda 함수가 메시지를 처리하기 전에 읽은 즉시 SQS 큐에서 각 메시지를 삭제하도록 수정합니다.

```
An image-processing company has a web application that users use to upload images. The application uploads the images into an Amazon S3 bucket. The company has set up S3 event notifications to publish the object creation events to an Amazon Simple Queue Service (Amazon SQS) standard queue. The SQS queue serves as the event source for an AWS Lambda function that processes the images and sends the results to users through email.  
Users report that they are receiving multiple email messages for every uploaded image. A solutions architect determines that SQS messages are invoking the Lambda function more than once, resulting in multiple email messages.  
What should the solutions architect do to resolve this issue with the LEAST operational overhead?

- A. Set up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30 seconds.
- B. Change the SQS standard queue to an SQS FIFO queue. Use the message deduplication ID to discard duplicate messages.
- C. Increase the visibility timeout in the SQS queue to a value that is greater than the total of the function timeout and the batch window timeout.
- D. Modify the Lambda function to delete each message from the SQS queue immediately after the message is read before processing.
```

정답 : `B`

- SQS **표준 큐**는 **최소 1회 이상 전달(At-Least-Once Delivery)**을 보장하므로, Lambda 함수가 동일한 메시지를 여러 번 처리할 수 있습니다.
- 이를 해결하려면 **SQS FIFO 큐**를 사용하고 **메시지 중복 제거 ID(Deduplication ID)**를 설정하면, 동일한 메시지가 여러 번 처리되지 않고 한 번만 Lambda로 전달됩니다.
- FIFO 큐는 **순서 보장(First-In-First-Out)**과 **중복 제거**를 지원하므로, 중복 이메일 문제를 가장 간단하게 해결할 수 있습니다.
- Deduplication ID (메시지 중복 제거 ID)
	- **목적**: **SQS FIFO 큐에서 동일 메시지가 중복으로 큐에 저장되는 것을 방지**
	- **동작 원리**:
	    1. FIFO 큐에 메시지 전송 시 MessageDeduplicationId 제공
	    2. SQS는 동일 ID의 메시지를 **5분 동안 중복으로 받지 않음**
	    3. 이 시간 동안 같은 ID 메시지는 **큐에 들어가지 않음**
	- **사용 예**:
	    - 중복 전송 방지: 예를 들어, 동일 결제 이벤트가 여러 번 발생해도 큐에는 한 번만 저장
	    - 메시지 전송 레벨에서 중복 방지 → 소비자 측에서도 중복 처리 최소화  
	- **한계**:
	    - FIFO 큐에서만 사용 가능
	    - 5분간만 중복 방지 → 장기 중복은 다른 방법 필요


오답 이유

- **A. 롱 폴링 설정**
    - 롱 폴링은 큐에서 메시지를 기다리는 시간을 늘려 불필요한 API 호출을 줄이는 기능입니다.
    - Lambda 중복 호출 문제 해결에는 직접적인 효과가 없습니다.
    
- **C. 가시성 타임아웃 증가**
    - 가시성 타임아웃은 Lambda가 메시지를 처리하는 동안 다른 소비자가 해당 메시지를 가져가지 못하도록 하는 시간입니다.
    - 현재 문제는 메시지가 중복 전달되는 것이므로, 단순히 타임아웃을 늘리는 것만으로는 중복 처리 문제를 완전히 해결할 수 없습니다.
    
- **D. Lambda에서 메시지를 즉시 삭제**
    - Lambda와 SQS 통합 시, **Lambda가 메시지를 성공적으로 처리하면 자동으로 삭제**됩니다.
    - 메시지를 처리하기 전에 삭제하면, 실패 시 메시지가 사라져 **데이터 유실 가능성**이 발생합니다.
    - 따라서 안전하지 않으며 문제 해결에는 적합하지 않습니다


## #99
한 회사는 온프레미스 데이터 센터에서 호스팅되는 게임 애플리케이션을 위해 **공유 스토리지 솔루션**을 구현하려고 합니다.  
회사는 **Lustre 클라이언트**를 사용하여 데이터를 액세스할 수 있어야 합니다.  
솔루션은 **완전 관리형**이어야 합니다.  
이 요구 사항을 충족하는 솔루션은 무엇입니까?

- A. AWS Storage Gateway 파일 게이트웨이를 생성합니다. 필요한 클라이언트 프로토콜을 사용하는 파일 공유를 생성합니다. 애플리케이션 서버를 파일 공유에 연결합니다.  
- B. Amazon EC2 Windows 인스턴스를 생성합니다. 인스턴스에 Windows 파일 공유 역할을 설치하고 구성합니다. 애플리케이션 서버를 파일 공유에 연결합니다.  
- C. Amazon Elastic File System(Amazon EFS) 파일 시스템을 생성하고, Lustre를 지원하도록 구성합니다. 파일 시스템을 원본 서버에 연결합니다. 애플리케이션 서버를 파일 시스템에 연결합니다.  
- D. Amazon FSx for Lustre 파일 시스템을 생성합니다. 파일 시스템을 원본 서버에 연결합니다. 애플리케이션 서버를 파일 시스템에 연결합니다.

```
A company is implementing a shared storage solution for a gaming application that is hosted in an on-premises data center. The company needs the ability to use Lustre clients to access data. The solution must be fully managed.  
Which solution meets these requirements?

- A. Create an AWS Storage Gateway file gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.
- B. Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.
- C. Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system.
- D. Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.
```


정답 : `D`

- **Amazon FSx for Lustre**는 완전 관리형 Lustre 파일 시스템을 제공하며, **Lustre 클라이언트를 바로 연결**할 수 있습니다.
- 고성능 공유 스토리지를 제공하고, 게임 애플리케이션과 같이 **고속 읽기/쓰기와 낮은 지연 시간**이 필요한 워크로드에 적합합니다.
- 온프레미스 서버에서도 FSx for Lustre를 **Direct Connect 또는 VPN**을 통해 연결할 수 있습니다.

오답 이유

- **A. AWS Storage Gateway 파일 게이트웨이**
    - Storage Gateway는 NFS 또는 SMB 파일 공유를 온프레미스 애플리케이션과 연결할 수 있으나, **Lustre 프로토콜을 지원하지 않습니다**.
    
- **B. Amazon EC2 Windows 인스턴스**
    - 수동으로 Windows 파일 서버를 운영하는 것은 **완전 관리형이 아니며**, Lustre 클라이언트를 직접 지원하지 않습니다.
    - 관리 오버헤드가 높고 Lustre 지원 불가.
    
- **C. Amazon EFS 파일 시스템**
    - EFS는 NFS 기반 공유 스토리지이며, **Lustre 클라이언트를 직접 지원하지 않습니다**.
    - EFS와 FSx for Lustre는 서로 다른 프로토콜과 성능 특성을 가집니다.


## #100
한 회사의 컨테이너화된 애플리케이션이 Amazon EC2 인스턴스에서 실행됩니다.  
애플리케이션은 다른 비즈니스 애플리케이션과 통신하기 전에 **보안 인증서를 다운로드**해야 합니다.  
회사는 인증서를 **실시간에 가까운 수준으로 암호화 및 복호화**할 수 있는 매우 안전한 솔루션을 원합니다.  
또한, 데이터를 암호화한 후에는 **고가용성 스토리지에 저장**해야 합니다.  
이 요구 사항을 가장 **낮은 운영 오버헤드로** 충족하는 솔루션은 무엇입니까?

- A. AWS Secrets Manager에 암호화된 인증서용 시크릿을 생성합니다. 필요에 따라 인증서를 수동으로 업데이트합니다. IAM 세분화 권한을 사용하여 데이터 액세스를 제어합니다.  
- B. Python cryptography 라이브러리를 사용하는 AWS Lambda 함수를 생성하여 암호화 작업을 수행합니다. 함수를 Amazon S3 버킷에 저장합니다.  
- C. AWS Key Management Service(AWS KMS) 고객 관리 키(CMK)를 생성합니다. EC2 역할이 KMS 키를 사용하여 암호화 작업을 수행하도록 허용합니다. 암호화된 데이터를 Amazon S3에 저장합니다.  
- D. AWS Key Management Service(AWS KMS) 고객 관리 키(CMK)를 생성합니다. EC2 역할이 KMS 키를 사용하여 암호화 작업을 수행하도록 허용합니다. 암호화된 데이터를 Amazon Elastic Block Store(Amazon EBS) 볼륨에 저장합니다.


```
A company's containerized application runs on an Amazon EC2 instance. The application needs to download security certificates before it can communicate with other business applications. The company wants a highly secure solution to encrypt and decrypt the certificates in near real time. The solution also needs to store data in highly available storage after the data is encrypted.  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Create AWS Secrets Manager secrets for encrypted certificates. Manually update the certificates as needed. Control access to the data by using fine-grained IAM access.
- B. Create an AWS Lambda function that uses the Python cryptography library to receive and perform encryption operations. Store the function in an Amazon S3 bucket.
- C. Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon S3.
- D. Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon Elastic Block Store (Amazon EBS) volumes.
```

정답 : `C`

- **AWS KMS**를 사용하면 인증서를 안전하게 암호화/복호화할 수 있고, EC2에서 실시간 접근이 가능합니다.
- **Amazon S3**는 **높은 내구성과 가용성**을 제공하며, 암호화된 데이터를 안전하게 저장할 수 있습니다.
- 이 조합은 **운영 오버헤드가 낮고 관리가 용이**하며, Secrets Manager처럼 인증서 회전이나 버전 관리가 필요한 경우 추가 기능과 통합이 가능합니다.

오답 이유

- **A. AWS Secrets Manager**
    - Secrets Manager는 시크릿 저장에 적합하지만, **수동으로 인증서를 업데이트해야 하는 경우 운영 부담이 증가**합니다.
    - 실시간 암호화/복호화와 직접 연결하기에는 KMS보다 효율이 떨어집니다.
    
- **B. Lambda + cryptography 라이브러리**
    - 암호화/복호화 로직을 직접 구현하면 **운영 오버헤드가 매우 높고**, 고가용성 스토리지에 대한 책임도 수동으로 관리해야 합니다.
    - S3에 Lambda 코드를 저장하는 것은 코드 배포일 뿐, 데이터 스토리지로 사용하기 적합하지 않습니다.
    
- **D. EBS에 암호화된 데이터 저장**
    - EBS는 AZ 내에서만 고가용성을 제공하므로, **다중 AZ 내 고가용성을 보장하지 않습니다**.
    - S3처럼 내구성과 확장성을 제공하지 못합니다.