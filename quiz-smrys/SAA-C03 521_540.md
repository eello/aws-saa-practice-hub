---
created: 2025-10-16 10:24:36
last_modified: 2025-10-16 16:32:36
---
## #521
한 소매 회사는 여러 사업부를 보유하고 있습니다. 각 사업부의 IT 팀은 자체 AWS 계정을 관리합니다. 각 팀 계정은 AWS Organizations의 한 조직에 속해 있습니다. 각 팀은 자체 AWS 계정의 Amazon DynamoDB 테이블에서 제품 재고 수준을 모니터링합니다.

회사는 공유 AWS 계정에 중앙 인벤토리 보고 애플리케이션을 배포하고 있습니다. 이 애플리케이션은 모든 팀의 DynamoDB 테이블에서 항목을 읽을 수 있어야 합니다.

가장 안전하게 이 요구 사항을 충족하는 인증 옵션은 무엇입니까?

A. 인벤토리 애플리케이션 계정에서 AWS Secrets Manager와 DynamoDB를 통합한다. 애플리케이션이 Secrets Manager의 올바른 비밀을 사용하여 인증하고 DynamoDB 테이블을 읽도록 구성한다. 비밀을 30일마다 회전하도록 예약한다.
B. 각 사업부 계정에서 프로그래매틱 액세스가 가능한 IAM 사용자를 생성한다. 애플리케이션이 올바른 IAM 사용자 액세스 키 ID와 비밀 액세스 키를 사용하여 인증하고 DynamoDB 테이블을 읽도록 구성한다. IAM 액세스 키를 30일마다 수동으로 교체한다.
C. 각 사업부 계정에서 BU_ROLE이라는 IAM 역할을 생성하고, 해당 역할에 DynamoDB 테이블에 대한 액세스 정책과 인벤토리 애플리케이션 계정의 특정 역할을 신뢰(trust)하는 트러스트 정책을 설정한다. 인벤토리 계정에는 STS AssumeRole API 작업에 대한 액세스를 허용하는 APP_ROLE이라는 역할을 생성한다. 애플리케이션이 APP_ROLE을 사용하여 크로스계정 역할 BU_ROLE을 가정(assume)해 DynamoDB 테이블을 읽도록 구성한다.
D. AWS Certificate Manager(ACM)과 DynamoDB를 통합한다. DynamoDB를 인증하기 위한 아이덴티티 인증서를 생성한다. 애플리케이션이 올바른 인증서를 사용하여 인증하고 DynamoDB 테이블을 읽도록 구성한다.

```
A retail company has several businesses. The IT team for each business manages its own AWS account. Each team account is part of an organization in AWS Organizations. Each team monitors its product inventory levels in an Amazon DynamoDB table in the team's own AWS account.  
  
The company is deploying a central inventory reporting application into a shared AWS account. The application must be able to read items from all the teams' DynamoDB tables.  
  
Which authentication option will meet these requirements MOST securely?

- A. Integrate DynamoDB with AWS Secrets Manager in the inventory application account. Configure the application to use the correct secret from Secrets Manager to authenticate and read the DynamoDB table. Schedule secret rotation for every 30 days.
- B. In every business account, create an IAM user that has programmatic access. Configure the application to use the correct IAM user access key ID and secret access key to authenticate and read the DynamoDB table. Manually rotate IAM access keys every 30 days.
- C. In every business account, create an IAM role named BU_ROLE with a policy that gives the role access to the DynamoDB table and a trust policy to trust a specific role in the inventory application account. In the inventory account, create a role named APP_ROLE that allows access to the STS AssumeRole API operation. Configure the application to use APP_ROLE and assume the crossaccount role BU_ROLE to read the DynamoDB table.
- D. Integrate DynamoDB with AWS Certificate Manager (ACM). Generate identity certificates to authenticate DynamoDB. Configure the application to use the correct certificate to authenticate and read the DynamoDB table.
```

정답 : `C`

- 크로스계정 IAM 역할(AssumeRole + STS) 패턴이 AWS 모범 사례
- 각 팀 계정에 최소 권한의 역할(BU_ROLE)을 만들고, 중앙 인벤토리 계정의 APP_ROLE을 신뢰하도록 트러스트 정책을 구성하면
	- 애플리케이션은 STS 임시 자격 증명으로 각 계정의 DynamoDB 테이블을 안전하게 읽을 수 있음

오답 이유

- **A. Secrets Manager로 DynamoDB 인증 관리**
    - Secrets Manager는 DB/서비스 자격 증명 저장·회전에 유용하지만, **DynamoDB는 IAM 기반 인증**을 사용합니다. 또한 비밀에 **장기 키를 저장**하는 방식은 불필요하고 보안상 열위입니다.
    
- **B. 계정마다 IAM 사용자 키 발급·저장**    
    - **장기 액세스 키**를 여러 개 관리·보관해야 하며, **주기적 키 로테이션**과 배포가 번거롭고 유출 위험 증가. 크로스계정 역할 대비 **보안·운영성 모두 열위**입니다.
    
- **D. ACM 인증서로 DynamoDB 인증**
    - ACM은 **TLS/서버 인증서 관리** 서비스이며, **DynamoDB 인증 수단이 아님**. 본 요구와 무관합니다.


## #522
회사는 Amazon Elastic Kubernetes Service (Amazon EKS)를 사용하여 컨테이너 애플리케이션을 운영하고 있습니다. 회사의 워크로드는 하루 종일 일정하지 않습니다. 회사는 Amazon EKS가 워크로드에 따라 자동으로 확장 및 축소되기를 원합니다.

이 요구 사항을 가장 적은 운영 오버헤드로 충족하는 단계 조합은 무엇입니까? (두 개 선택)

A. AWS Lambda 함수를 사용하여 EKS 클러스터의 크기를 조정한다.
B. Kubernetes Metrics Server를 사용하여 Horizontal Pod Autoscaling을 활성화한다.
C. Kubernetes Cluster Autoscaler를 사용하여 클러스터의 노드 수를 관리한다.
D. Amazon API Gateway를 사용하고 이를 Amazon EKS에 연결한다.
E. AWS App Mesh를 사용하여 네트워크 활동을 관찰한다.

```
A company runs container applications by using Amazon Elastic Kubernetes Service (Amazon EKS). The company's workload is not consistent throughout the day. The company wants Amazon EKS to scale in and out according to the workload.  
  
Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)

- A. Use an AWS Lambda function to resize the EKS cluster.
- B. Use the Kubernetes Metrics Server to activate horizontal pod autoscaling.
- C. Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.
- D. Use Amazon API Gateway and connect it to Amazon EKS.
- E. Use AWS App Mesh to observe network activity.
```

정답 : `B, C`

- B. Kubernetes Metrics Server + HPA : 파드 수준의 자동 확장. CPU/메모리 등의 메트릭 기반으로 Horizontal Pod Autoscaler가 레플리카 수를 자동 증감하여 워크로드 변화에 대응
- C. Kubernetes Cluster Autoscaler : 노드 수준의 자동 확장. 스케줄 불가 피드가 생기면 노드를 증설, 유휴 노드는 축소하여 비용과 성능을 균형 있게 유지

오답 이유

- **A. Lambda로 클러스터 리사이즈**: 가능하더라도 수동/커스텀 오케스트레이션이 필요해 **운영 복잡도↑**. 표준 오토스케일링 컴포넌트(B/C)가 더 적합.
    
- **D. API Gateway 연결**: API 프런트도어/라우팅 서비스로 **EKS 자동 확장과 무관**.
    
- **E. App Mesh 관찰**: 서비스 메시/트래픽 관측·제어에 유용하지만 **스케일 인/아웃을 수행하지 않음**.


## #523
한 회사가 마이크로서비스 기반의 서버리스 웹 애플리케이션을 운영하고 있습니다. 애플리케이션은 여러 Amazon DynamoDB 테이블에서 데이터를 조회할 수 있어야 합니다. 솔루션스 아키텍트는 애플리케이션의 기준(베이스라인) 성능에 영향을 주지 않고 데이터를 조회할 수 있는 기능을 제공해야 합니다.

이 요구 사항을 가장 운영 효율적으로 충족하는 솔루션은 무엇입니까?

A. AWS AppSync 파이프라인 리졸버
B. Lambda@Edge 함수를 사용하는 Amazon CloudFront
C. AWS Lambda 함수를 사용하는 엣지 최적화(Edge-optimized) Amazon API Gateway
D. DynamoDB 커넥터가 있는 Amazon Athena Federated Query

```
A company runs a microservice-based serverless web application. The application must be able to retrieve data from multiple Amazon DynamoDB tables A solutions architect needs to give the application the ability to retrieve the data with no impact on the baseline performance of the application.  
  
Which solution will meet these requirements in the MOST operationally efficient way?

- A. AWS AppSync pipeline resolvers
- B. Amazon CloudFront with Lambda@Edge functions
- C. Edge-optimized Amazon API Gateway with AWS Lambda functions
- D. Amazon Athena Federated Query with a DynamoDB connector
```

정답 : `A`

- AppSync는 여러 데이터 소스(DynamoDB 테이블 다수, Lambda 등)를 단일 GraphQL API로 통합해주는 서버리스 서비스
- 파이프라인 리졸버를 사용하면 여러 테이블 조회를 조합/병렬화하고 필요 시 변환을 수행하여 애플리케이션 코드 변경 최소화와 운영 오버헤드 최소화를 동시 달성
- AppSync 캐시를 사용하면 백엔드 조회 부담을 줄여 기존(베이스라인) 성능에 미치는 영향을 최소화

오답 이유

- **B. CloudFront + Lambda@Edge**
    - CDN/엣지 변형용이며 **DynamoDB 조회를 직접 통합**하기에 제약이 큼(리전 제한, 네트워크/권한 복잡도). 데이터 소스 집계에 적합하지 않음.
    
- **C. Edge-optimized API Gateway + Lambda**
    - 가능은 하지만 **여러 테이블 조합 로직을 Lambda에 직접 구현/운영**해야 하며, 스케일/성능 최적화(캐시, 병렬화 등)도 수동 관리 필요 → **운영 효율성** 측면에서 AppSync 대비 열위.
    
- **D. Athena Federated Query + DynamoDB 커넥터**
    - **분석/애드혹 SQL 조회** 용도에 적합. **실시간 애플리케이션 조회 경로**로 사용 시 지연/비용/운영 측면에서 부적절.


## #524
회사는 IAM 권한과 관련된 Access Denied 오류와 Unauthorized 오류를 분석하고 문제를 해결하고자 합니다. 회사는 AWS CloudTrail을 활성화해 둔 상태입니다.

가장 적은 노력으로 이러한 요구 사항을 충족할 수 있는 솔루션은 무엇입니까?

A. AWS Glue를 사용하고 사용자 지정 스크립트를 작성하여 CloudTrail 로그에서 오류를 쿼리한다.
B. AWS Batch를 사용하고 사용자 지정 스크립트를 작성하여 CloudTrail 로그에서 오류를 쿼리한다.
C. Amazon Athena 쿼리로 CloudTrail 로그를 검색하여 오류를 식별한다.
D. Amazon QuickSight로 CloudTrail 로그를 검색한다. 오류를 식별하기 위한 대시보드를 생성한다.

```
A company wants to analyze and troubleshoot Access Denied errors and Unauthorized errors that are related to IAM permissions. The company has AWS CloudTrail turned on.  
  
Which solution will meet these requirements with the LEAST effort?

- A. Use AWS Glue and write custom scripts to query CloudTrail logs for the errors.
- B. Use AWS Batch and write custom scripts to query CloudTrail logs for the errors.
- C. Search CloudTrail logs with Amazon Athena queries to identify the errors.
- D. Search CloudTrail logs with Amazon QuickSight. Create a dashboard to identify the errors.
```

정답 : `C`

- CloudTrail 로그가 S3에 존재하면 Athena로 즉시 스키마를 정의하고 표준 SQL로 검색할 수 있어 구축･운영 노력이 최소
- AccessDenied/UnauthorizedOperation 같은 이벤트명을 조건으로 신속히 필터링하며, 추가 파이프라인이나 배치 코드가 필요 없음

오답 이유

- **A. AWS Glue + 사용자 지정 스크립트**
    - Glue ETL을 짜는 것은 불필요한 오버헤드입니다. 단순 조회·분석에는 **Athena가 즉시성·운영 효율**에서 우위입니다.
    
- **B. AWS Batch + 사용자 지정 스크립트**
    - Batch로 컨테이너 잡을 돌려 로그를 파싱하는 것은 **구축과 운영이 가장 무겁고 복잡**합니다. 요구사항 대비 과도한 설계입니다.
    
- **D. Amazon QuickSight 대시보드**
    - QuickSight는 **시각화·리포팅** 용도입니다. 근본 검색·추출은 Athena 등으로 수행해야 하며, “가장 적은 노력의 원인 분석”에는 **직접 쿼리 가능한 Athena**가 적합합니다.


## #525
회사는 기존의 AWS 사용 비용을 운영 비용 대시보드에 추가하려고 합니다. 솔루션스 아키텍트는 회사가 사용 비용에 프로그램 방식으로 접근할 수 있게 하는 솔루션을 추천해야 합니다. 회사는 당해 연도의 비용 데이터에 접근할 수 있어야 하며, 향후 12개월의 비용을 예측할 수 있어야 합니다.

이 요구 사항을 가장 적은 운영 오버헤드로 충족하는 솔루션은 무엇입니까?

A. 페이지네이션을 사용하여 AWS Cost Explorer API로 사용 비용 관련 데이터에 접근한다.
B. 다운로드 가능한 AWS Cost Explorer 보고서 .csv 파일을 사용하여 사용 비용 관련 데이터에 접근한다.
C. AWS Budgets 작업을 구성하여 FTP를 통해 회사에 사용 비용 데이터를 전송한다.
D. 사용 비용 데이터에 대한 AWS Budgets 보고서를 생성한다. SMTP를 통해 회사에 데이터를 전송한다.

```
A company wants to add its existing AWS usage cost to its operation cost dashboard. A solutions architect needs to recommend a solution that will give the company access to its usage cost programmatically. The company must be able to access cost data for the current year and forecast costs for the next 12 months.  
  
Which solution will meet these requirements with the LEAST operational overhead?

- A. Access usage cost-related data by using the AWS Cost Explorer API with pagination.
- B. Access usage cost-related data by using downloadable AWS Cost Explorer report .csv files.
- C. Configure AWS Budgets actions to send usage cost data to the company through FTP.
- D. Create AWS Budgets reports for usage cost data. Send the data to the company through SMTP.
```

정답 : `A`

- AWS Cost Explorer API는 프로그래밍 방식으로 당해 연도 비용/사용량 조회와 향후 최대 12개월 비용 예측을 지원
- API 호출만으로 대시보드에 쉽게 통합할 수 있어 운영 오버헤드가 최소이며, 필요 시 페이지네이션으로 대량 데이터도 안정적으로 수집 가능

오답 이유

- **B. Cost Explorer CSV 다운로드**
    - 수동/반자동 다운로드 기반으로 **프로그램적·실시간 통합이 불편**합니다. 또한 예측(포어캐스트)까지 포함해 일관적으로 공급하기 어렵습니다.
    
- **C. Budgets + FTP 전송**
    - Budgets는 예산 임계치 알림 중심이며, **정량 비용/사용량 데이터 피드 제공 목적이 아님**. FTP 전송도 **운영·보안 오버헤드**가 큼.
    
- **D. Budgets 보고서 + SMTP**
    - 이메일 보고서는 **사람이 보는 요약용**에 가깝고, 대시보드 연동을 위한 **API 기반 통합**에 부적합합니다. 또한 예측치 제공에도 제약이 있습니다.


## #526
솔루션스 아키텍트가 애플리케이션의 복원력을 검토하고 있습니다. 솔루션스 아키텍트는 데이터베이스 관리자가 확장 연습의 일환으로 애플리케이션의 Amazon Aurora PostgreSQL 데이터베이스 writer 인스턴스를 최근에 장애 조치(failover)했음을 발견했습니다. 이 장애 조치로 인해 애플리케이션에 3분의 다운타임이 발생했습니다.

확장 연습에서 다운타임을 최소 운영 오버헤드로 줄일 수 있는 솔루션은 무엇입니까?

A. 장애 조치 중 부하를 처리할 수 있도록 클러스터에 더 많은 Aurora PostgreSQL 읽기 복제본을 생성한다.
B. 동일한 AWS 리전에 보조 Aurora PostgreSQL 클러스터를 설정한다. 장애 조치 중에 애플리케이션을 보조 클러스터의 writer 엔드포인트를 사용하도록 업데이트한다.
C. 장애 조치 중 부하를 처리할 수 있도록 Amazon ElastiCache for Memcached 클러스터를 생성한다.
D. 데이터베이스에 대해 Amazon RDS Proxy를 설정한다. 애플리케이션이 프록시 엔드포인트를 사용하도록 업데이트한다.

```
A solutions architect is reviewing the resilience of an application. The solutions architect notices that a database administrator recently failed over the application's Amazon Aurora PostgreSQL database writer instance as part of a scaling exercise. The failover resulted in 3 minutes of downtime for the application.  
  
Which solution will reduce the downtime for scaling exercises with the LEAST operational overhead?

- A. Create more Aurora PostgreSQL read replicas in the cluster to handle the load during failover.
- B. Set up a secondary Aurora PostgreSQL cluster in the same AWS Region. During failover, update the application to use the secondary cluster's writer endpoint.
- C. Create an Amazon ElastiCache for Memcached cluster to handle the load during failover.
- D. Set up an Amazon RDS proxy for the database. Update the application to use the proxy endpoint.
```

정답 : `D`

- RDS Proxy는 연결 풀링과 빠른 엔드포인트 리다이렉션을 제공해 Aurora 장애 조치 시 연결 재설정 시간을 크게 단축
- 애플리케이션은 프록시 엔드포인트만 바라보면 되고, 프록시는 새로운 writer로 자동 전환하므로 운영 오버헤드가 매우 낮고 다운타임을 수 초 수준으로 축소 가능

오답 이유

- **A. 읽기 복제본 추가**
    - 읽기 복제본은 **읽기 스케일 아웃** 용도일 뿐, **writer 장애 조치 시 연결 단절 문제**를 해소하지 못합니다. 다운타임 단축과 직접 관련이 없습니다.
    
- **B. 보조 클러스터 구성 후 애플리케이션 엔드포인트 변경**
    - 애플리케이션 측에서 **엔드포인트 스위칭 작업**이 필요해 **운영 복잡도↑**. 같은 리전 내에서라면 Aurora의 기존 클러스터 내 장애 조치에 비해 이점이 적고, 다운타임이 줄어든다고 보장하기 어렵습니다.
    
- **C. ElastiCache for Memcached 추가**
    - 캐시는 데이터베이스 부하를 줄일 수는 있으나 **writer 전환 시 커넥션 재수립 문제**를 해결하지 못합니다. 장애 조치 다운타임 단축과는 직접적 연관이 없음.


## #527
한 회사는 단일 AWS 리전에서 지역 구독형 스트리밍 서비스를 운영하고 있습니다. 아키텍처는 Amazon EC2 인스턴스의 웹 서버와 애플리케이션 서버로 구성되어 있으며, 이 EC2 인스턴스들은 Elastic Load Balancer 뒤의 Auto Scaling 그룹에 포함되어 있습니다. 아키텍처에는 여러 가용 영역에 걸쳐 확장되는 Amazon Aurora 글로벌 데이터베이스 클러스터가 포함됩니다.

회사는 전 세계로 확장하고 애플리케이션의 다운타임을 최소화하려고 합니다.

가장 높은 내결함성을 제공하는 솔루션은 무엇입니까?

A. 웹 계층과 애플리케이션 계층의 Auto Scaling 그룹을 확장하여 두 번째 리전의 가용 영역에 인스턴스를 배포한다. 기본 리전과 두 번째 리전에 데이터베이스를 배포하기 위해 Aurora 글로벌 데이터베이스를 사용한다. Amazon Route 53 상태 확인과 장애 조치 라우팅 정책을 사용해 두 번째 리전으로 장애 조치한다.
B. 웹 계층과 애플리케이션 계층을 두 번째 리전에 배포한다. 두 번째 리전에 교차 리전 Aurora PostgreSQL 복제본(Aurora Replica)을 추가한다. Amazon Route 53 상태 확인과 장애 조치 라우팅 정책을 사용해 두 번째 리전으로 장애 조치한다. 필요 시 보조를 기본으로 승격한다.
C. 웹 계층과 애플리케이션 계층을 두 번째 리전에 배포한다. 두 번째 리전에 Aurora PostgreSQL 데이터베이스를 생성한다. AWS Database Migration Service(AWS DMS)를 사용해 기본 데이터베이스를 두 번째 리전으로 복제한다. Amazon Route 53 상태 확인과 장애 조치 라우팅 정책을 사용해 두 번째 리전으로 장애 조치한다.
D. 웹 계층과 애플리케이션 계층을 두 번째 리전에 배포한다. 기본 리전과 두 번째 리전에 데이터베이스를 배포하기 위해 Amazon Aurora 글로벌 데이터베이스를 사용한다. Amazon Route 53 상태 확인과 장애 조치 라우팅 정책을 사용해 두 번째 리전으로 장애 조치한다. 필요 시 보조를 기본으로 승격한다.

```
A company has a regional subscription-based streaming service that runs in a single AWS Region. The architecture consists of web servers and application servers on Amazon EC2 instances. The EC2 instances are in Auto Scaling groups behind Elastic Load Balancers. The architecture includes an Amazon Aurora global database cluster that extends across multiple Availability Zones.  
  
The company wants to expand globally and to ensure that its application has minimal downtime.  
  
Which solution will provide the MOST fault tolerance?

- A. Extend the Auto Scaling groups for the web tier and the application tier to deploy instances in Availability Zones in a second Region. Use an Aurora global database to deploy the database in the primary Region and the second Region. Use Amazon Route 53 health checks with a failover routing policy to the second Region.
- B. Deploy the web tier and the application tier to a second Region. Add an Aurora PostgreSQL cross-Region Aurora Replica in the second Region. Use Amazon Route 53 health checks with a failover routing policy to the second Region. Promote the secondary to primary as needed.
- C. Deploy the web tier and the application tier to a second Region. Create an Aurora PostgreSQL database in the second Region. Use AWS Database Migration Service (AWS DMS) to replicate the primary database to the second Region. Use Amazon Route 53 health checks with a failover routing policy to the second Region.
- D. Deploy the web tier and the application tier to a second Region. Use an Amazon Aurora global database to deploy the database in the primary Region and the second Region. Use Amazon Route 53 health checks with a failover routing policy to the second Region. Promote the secondary to primary as needed.
```

정답 : `D`

- Aurora Global Database는 리전 간 스토리지 레벨 복제로 RPO ~1초, RTO 수십 초 수준의 빠른 재해 복구를 제공해 가장 높은 내결함성 제공
- 웹/앱 계층을 두 번째 리전에 별도로 배포하고 Route 53 상태 확인 + 페일오버 라우팅을 적용하면, 애플리케이션 전 계층이 리전 장애 시 신속히 전환

오답 이유

- **A**: “Auto Scaling 그룹을 확장하여 두 번째 리전에 배포”는 **불가능**합니다. ASG와 로드밸런서는 **리전 범위 리소스**이므로 리전마다 **별도 ASG/ELB**가 필요합니다. 의도는 D와 유사하지만 기술적으로 부정확.
    
- **B**: 교차 리전 **Aurora Read Replica**는 가능하나 **Aurora Global Database 대비 RPO/RTO가 열위**이며, 장애 조치 속도와 일관성에서 **최고 수준의 내결함성**을 제공하지 못합니다.
    
- **C**: DMS 복제는 **마이그레이션/ETL 성격**으로 지속적, 저지연 DR 복제에 적합하지 않으며 운영 복잡도↑, 장애 조치 성능도 낮습니다.


## #528
데이터 분석 회사가 배치 처리 시스템을 AWS로 마이그레이션하려고 합니다. 이 회사는 하루 동안 주기적으로 FTP를 통해 수천 개의 작은 데이터 파일을 수신합니다. 온프레미스 배치 작업은 밤에 데이터 파일을 처리합니다. 그러나 배치 작업은 실행을 완료하는 데 몇 시간이 걸립니다.

회사는 AWS 솔루션이 가능한 한 빨리 들어오는 데이터 파일을 처리하길 원하며, 파일을 보내는 FTP 클라이언트에 대한 변경은 최소화하고자 합니다. 이 솔루션은 파일이 성공적으로 처리된 후 들어오는 데이터 파일을 삭제해야 합니다. 각 파일의 처리는 3~8분이 필요합니다.

이 요구 사항을 가장 운영 효율적으로 충족하는 솔루션은 무엇입니까?

A. Amazon EC2 인스턴스에서 FTP 서버를 실행하여 수신 파일을 Amazon S3 Glacier Flexible Retrieval의 객체로 저장한다. AWS Batch에서 작업 대기열을 구성한다. Amazon EventBridge 규칙을 사용하여 매일 밤 S3 Glacier Flexible Retrieval에서 객체를 처리하도록 작업을 호출한다. 작업이 객체를 처리한 후 객체를 삭제한다.
B. Amazon EC2 인스턴스에서 FTP 서버를 실행하여 수신 파일을 Amazon Elastic Block Store(Amazon EBS) 볼륨에 저장한다. AWS Batch에서 작업 대기열을 구성한다. Amazon EventBridge 규칙을 사용하여 매일 밤 EBS 볼륨에서 파일을 처리하도록 작업을 호출한다. 작업이 파일을 처리한 후 파일을 삭제한다.
C. AWS Transfer Family를 사용하여 Amazon Elastic Block Store(Amazon EBS) 볼륨에 수신 파일을 저장하는 FTP 서버를 생성한다. AWS Batch에서 작업 대기열을 구성한다. 각 파일이 도착할 때 Amazon S3 이벤트 알림을 사용하여 AWS Batch의 작업을 호출한다. 작업이 파일을 처리한 후 파일을 삭제한다.
D. AWS Transfer Family를 사용하여 수신 파일을 Amazon S3 Standard에 저장하는 FTP 서버를 생성한다. AWS Lambda 함수를 생성하여 파일을 처리하고 처리 후 파일을 삭제한다. 파일이 도착하면 S3 이벤트 알림을 사용하여 Lambda 함수를 호출한다.

```
A data analytics company wants to migrate its batch processing system to AWS. The company receives thousands of small data files periodically during the day through FTP. An on-premises batch job processes the data files overnight. However, the batch job takes hours to finish running.  
  
The company wants the AWS solution to process incoming data files as soon as possible with minimal changes to the FTP clients that send the files. The solution must delete the incoming data files after the files have been processed successfully. Processing for each file needs to take 3-8 minutes.  
  
Which solution will meet these requirements in the MOST operationally efficient way?

- A. Use an Amazon EC2 instance that runs an FTP server to store incoming files as objects in Amazon S3 Glacier Flexible Retrieval. Configure a job queue in AWS Batch. Use Amazon EventBridge rules to invoke the job to process the objects nightly from S3 Glacier Flexible Retrieval. Delete the objects after the job has processed the objects.
- B. Use an Amazon EC2 instance that runs an FTP server to store incoming files on an Amazon Elastic Block Store (Amazon EBS) volume. Configure a job queue in AWS Batch. Use Amazon EventBridge rules to invoke the job to process the files nightly from the EBS volume. Delete the files after the job has processed the files.
- C. Use AWS Transfer Family to create an FTP server to store incoming files on an Amazon Elastic Block Store (Amazon EBS) volume. Configure a job queue in AWS Batch. Use an Amazon S3 event notification when each file arrives to invoke the job in AWS Batch. Delete the files after the job has processed the files.
- D. Use AWS Transfer Family to create an FTP server to store incoming files in Amazon S3 Standard. Create an AWS Lambda function to process the files and to delete the files after they are processed. Use an S3 event notification to invoke the Lambda function when the files arrive.
```

정답 : `D`

- AWS Transfer Family(FTP) → S3 Standard로 수신하면, 기존 FTP 클라이언트 변경을 최소화하면서 곧바로 S3 이벤트 알림 → Lambda로 즉시 처리(이벤트 기반)
- 각 파일 처리 시간이 3~8분이므로 Lambda 최대 타임아웃(15분) 내에 충분히 처리 가능

오답 이유

- **A. EC2 FTP + S3 Glacier Flexible Retrieval + 야간 Batch**
    - Glacier는 **검색/복구 지연**(분~시간)이 있어 **즉시 처리 요건에 부적합**하며, 야간 배치는 요구사항(가능한 한 빨리 처리)에 맞지 않습니다. EC2 운영 부담도 큼.
    
- **B. EC2 FTP + EBS + 야간 Batch**
    - 여전히 **야간 일괄 처리**로 즉시 처리 요건 미충족. EC2/EBS 운영 및 스케줄 관리로 **운영 복잡도↑**.
    
- **C. Transfer Family → EBS 저장 + S3 이벤트로 Batch 호출**
    - Transfer Family의 **백엔드는 S3 또는 EFS**가 표준이며 **EBS 저장을 지원하지 않습니다.** 또한 **S3 이벤트**는 EBS에 저장된 파일에 대해 발생하지 않습니다. 설계 자체가 불일치.


## #529
회사는 워크로드를 AWS로 마이그레이션하고 있습니다. 회사의 데이터베이스에는 트랜잭션 데이터와 민감한 데이터가 있습니다. 회사는 데이터베이스의 보안을 강화하고 운영 오버헤드를 줄이기 위해 AWS 클라우드 솔루션을 사용하려고 합니다.

이 요구 사항을 충족할 솔루션은 무엇입니까?

A. 데이터베이스를 Amazon EC2로 마이그레이션한다. 암호화를 위해 AWS Key Management Service (AWS KMS)에서 AWS 관리형 키를 사용한다.
B. 데이터베이스를 Amazon RDS로 마이그레이션한다. 저장 시 암호화를 구성한다.
C. 데이터를 Amazon S3로 마이그레이션한다. 데이터 보안 및 보호를 위해 Amazon Macie를 사용한다.
D. 데이터베이스를 Amazon RDS로 마이그레이션한다. 데이터 보안 및 보호를 위해 Amazon CloudWatch Logs를 사용한다.

```
A company is migrating its workloads to AWS. The company has transactional and sensitive data in its databases. The company wants to use AWS Cloud solutions to increase security and reduce operational overhead for the databases.  
  
Which solution will meet these requirements?

- A. Migrate the databases to Amazon EC2. Use an AWS Key Management Service (AWS KMS) AWS managed key for encryption.
- B. Migrate the databases to Amazon RDS Configure encryption at rest.
- C. Migrate the data to Amazon S3 Use Amazon Macie for data security and protection
- D. Migrate the database to Amazon RDS. Use Amazon CloudWatch Logs for data security and protection.
```

정답 : `B`

- Amazon RDS는 패치, 백업, 장애조치 등 운영 오버헤드를 크게 줄여주는 관리형 데이터베이스 서비스
- RDS 저장 시 암호화를 활성화하면 AWS KMS 키로 스토리지 암호화되어 민감/트랜잭션 데이터 보호에 적합

오답 이유

- **A (EC2에 DB 직접 운영 + KMS 키)**: 자체 DB 관리(패치/백업/모니터링/HA)로 **운영 오버헤드가 큼**. 문제의 “운영 오버헤드 축소” 요구와 불일치.
    
- **C (S3 + Macie)**: S3는 객체 스토리지로 **트랜잭션 DB 대체가 아님**. Macie는 **민감 데이터 식별/분류** 서비스이지 DB 보안/운영 간소화 솔루션이 아님.
    
- **D (RDS + CloudWatch Logs)**: CloudWatch Logs는 **로그 수집/모니터링** 용도이며 **데이터 보호 수단이 아님**. 보안 강화를 위해서는 **암호화** 등 핵심 통제가 필요.


## #530
한 회사는 TCP와 UDP 멀티플레이어 기능을 가진 온라인 게임 애플리케이션을 운영합니다. 회사는 Amazon Route 53을 사용해 애플리케이션 트래픽을 여러 AWS 리전의 Network Load Balancer(NLB)로 라우팅하고 있습니다. 회사는 사용자 증가에 대비해 애플리케이션 성능을 개선하고 지연 시간을 줄여야 합니다.

이 요구 사항을 충족할 솔루션은 무엇입니까?

A. NLB 앞에 Amazon CloudFront 배포를 추가한다. Cache-Control max-age 파라미터를 늘린다.  
B. NLB를 Application Load Balancer(ALB)로 교체한다. Route 53에서 지연 시간 기반 라우팅을 사용하도록 구성한다.  
C. NLB 앞단에 AWS Global Accelerator를 추가한다. Global Accelerator 엔드포인트를 올바른 리스너 포트로 구성한다.  
D. NLB 뒤에 Amazon API Gateway 엔드포인트를 추가한다. API 캐싱을 활성화한다. 단계별로 메서드 캐싱을 오버라이드한다.

```
A company has an online gaming application that has TCP and UDP multiplayer gaming capabilities. The company uses Amazon Route 53 to point the application traffic to multiple Network Load Balancers (NLBs) in different AWS Regions. The company needs to improve application performance and decrease latency for the online game in preparation for user growth.  
  
Which solution will meet these requirements?

- A. Add an Amazon CloudFront distribution in front of the NLBs. Increase the Cache-Control max-age parameter.
- B. Replace the NLBs with Application Load Balancers (ALBs). Configure Route 53 to use latency-based routing.
- C. Add AWS Global Accelerator in front of the NLBs. Configure a Global Accelerator endpoint to use the correct listener ports.
- D. Add an Amazon API Gateway endpoint behind the NLBs. Enable API caching. Override method caching for the different stages.
```

정답 : `C`

- Global Accelerator(GA)는 전 세계 엣지에서 Anycast 고정 IP로 트래픽을 수용하여 AWS 글로벌 백본을 통해 가까운 리전의 NLB 엔드포인트로 최적 경로 전달
- TCP와 UDP 모두 지원하며, 게임 트래픽(세션 유지/저지연)에 적합
- 헬스 체크 및 지연 시간 기반으로 가장 가까운/건강한 엔드포인트로 자동 라우팅하여 성능과 가용성을 동시에 향상

오답 이유

- **A. CloudFront + 캐시 확장**
	- CloudFront는 **HTTP/HTTPS 콘텐츠 캐싱**용. 게임 프로토콜의 **TCP/UDP 세션 트래픽** 최적화에 적절치 않으며 **원본이 NLB의 비HTTP 포트**인 경우 효과 없음.

- **B. ALB + Route 53 지연 시간 라우팅**
	- **ALB는 UDP 미지원**(HTTP/HTTPS 전용) → 게임의 UDP 트래픽 요구와 불일치. Route 53 LBR만으로는 **네트워크 경로 최적화/백본 전송**을 제공하지 못함.

- **D. API Gateway + 캐싱**
	- API Gateway는 **REST/HTTP API** 프런트로 게임의 **실시간 TCP/UDP 세션**과 무관. 캐싱은 상태성 게임 트래픽 지연 개선에 도움이 되지 않음.


## #531
회사는 서드파티 데이터 피드와 통합해야 합니다.  
이 데이터 피드는 새로운 데이터가 사용할 준비가 되었을 때 외부 서비스에 알리기 위해 **웹훅(webhook)** 을 전송합니다.  
개발자는 회사가 웹훅 콜백을 받을 때 데이터를 가져오기 위해 **AWS Lambda 함수**를 작성했습니다.  
이제 이 Lambda 함수를 서드파티가 호출할 수 있도록 공개해야 합니다.  

운영 효율성을 가장 높게 유지하면서 이러한 요구사항을 충족할 수 있는 솔루션은 무엇입니까?

A. Lambda 함수에 대한 **Function URL**을 생성합니다. 해당 Lambda Function URL을 서드파티에 웹훅 주소로 제공합니다.  
B. Lambda 함수 앞단에 **Application Load Balancer (ALB)** 를 배포합니다. ALB URL을 서드파티에 웹훅 주소로 제공합니다.  
C. **Amazon SNS 주제(Topic)** 를 생성하고, 이를 Lambda 함수에 연결합니다. SNS 주제의 공개 호스트 이름을 서드파티에 웹훅 주소로 제공합니다.  
D. **Amazon SQS 큐**를 생성하고, 이를 Lambda 함수에 연결합니다. SQS 큐의 공개 호스트 이름을 서드파티에 웹훅 주소로 제공합니다.

```
A company needs to integrate with a third-party data feed. The data feed sends a webhook to notify an external service when new data is ready for consumption. A developer wrote an AWS Lambda function to retrieve data when the company receives a webhook callback. The developer must make the Lambda function available for the third party to call.  
  
Which solution will meet these requirements with the MOST operational efficiency?

- A. Create a function URL for the Lambda function. Provide the Lambda function URL to the third party for the webhook.
- B. Deploy an Application Load Balancer (ALB) in front of the Lambda function. Provide the ALB URL to the third party for the webhook.
- C. Create an Amazon Simple Notification Service (Amazon SNS) topic. Attach the topic to the Lambda function. Provide the public hostname of the SNS topic to the third party for the webhook.
- D. Create an Amazon Simple Queue Service (Amazon SQS) queue. Attach the queue to the Lambda function. Provide the public hostname of the SQS queue to the third party for the webhook.
```

정답 : `A`

- Lambda Function URL은 별도의 API Gateway나 ALB 없이도 HTTP(S) 엔드포인트를 직접 노출 가능
- 서드파티의 웹훅은 단순히 HTTP POST 요청을 보내는 구조이므로, Function URL을 사용하면 Lambda를 외부에 바로 연결 가능

오답 이유

- **B. ALB + Lambda**
	- **ALB**는 Lambda를 트리거할 수 있지만, ALB 배포 및 관리 비용이 발생합니다.
	- 인증, 스케일링, 로깅 설정 등 추가 관리 필요 → **운영 오버헤드 증가**
	- Function URL이 동일한 역할을 더 간단하게 수행하므로 비효율적입니다.

- **C. SNS + Lambda**
	- **SNS 주제**는 외부 HTTP 엔드포인트가 아닙니다. → 서드파티가 SNS에 직접 HTTP POST를 할 수 없습니다.
	- SNS는 **AWS 내부 서비스 간 통신용**이지 외부 webhook 수신용이 아닙니다. → 서드파티가 접근 불가

- **D. SQS + Lambda**
	- SQS는 **HTTP 엔드포인트가 아닙니다.** → 서드파티가 큐에 직접 webhook을 보낼 수 없음.
	- 외부에서 메시지를 보내려면 **AWS SDK 또는 서명된 요청**이 필요하며, webhook의 단순 HTTP POST 호출 방식과 맞지 않습니다.


## #532
한 회사가 하나의 AWS 리전에 워크로드를 가지고 있습니다. 고객들은 Amazon API Gateway REST API를 사용하여 워크로드에 연결하고 액세스합니다. 이 회사는 Amazon Route 53을 DNS 공급자로 사용하고 있습니다.  
회사는 모든 고객에게 개별적이고 안전한 URL을 제공하고자 합니다.  
  
다음 중 어떤 단계 조합이 **가장 높은 운영 효율성**으로 이러한 요구 사항을 충족합니까? (3개 선택)

A. 필요한 도메인을 등록기관에 등록합니다. Route 53 호스티드 존에 와일드카드 사용자 지정 도메인 이름을 생성하고, 존에 API Gateway 엔드포인트를 가리키는 레코드를 만듭니다.  
B. AWS Certificate Manager(ACM)의 다른 리전에서 도메인과 일치하는 와일드카드 인증서를 요청합니다.  
C. 필요한 각 고객에 대해 Route 53에 호스티드 존을 생성합니다. API Gateway 엔드포인트를 가리키는 존 레코드를 만듭니다.  
D. AWS Certificate Manager(ACM)의 같은 리전에서 사용자 지정 도메인 이름과 일치하는 와일드카드 인증서를 요청합니다.  
E. 각 고객에 대해 여러 API 엔드포인트를 API Gateway에 생성합니다.  
F. REST API에 대한 사용자 지정 도메인 이름을 API Gateway에서 생성합니다. AWS Certificate Manager(ACM)에서 인증서를 가져옵니다.

```
A company has a workload in an AWS Region. Customers connect to and access the workload by using an Amazon API Gateway REST API. The company uses Amazon Route 53 as its DNS provider. The company wants to provide individual and secure URLs for all customers.  
  
Which combination of steps will meet these requirements with the MOST operational efficiency? (Choose three.)

- A. Register the required domain in a registrar. Create a wildcard custom domain name in a Route 53 hosted zone and record in the zone that points to the API Gateway endpoint.
- B. Request a wildcard certificate that matches the domains in AWS Certificate Manager (ACM) in a different Region.
- C. Create hosted zones for each customer as required in Route 53. Create zone records that point to the API Gateway endpoint.
- D. Request a wildcard certificate that matches the custom domain name in AWS Certificate Manager (ACM) in the same Region.
- E. Create multiple API endpoints for each customer in API Gateway.
- F. Create a custom domain name in API Gateway for the REST API. Import the certificate from AWS Certificate Manager (ACM).
```

정답 : `A, D, F`

- 운영 효율을 높이기 위해 각 고객에 대해 별도의 API 엔드포인트나 호스티드 존을 만드는 것은 비효율적
- 와일드카드 도메인(\*.example.com)과 와일드카드 인증서를 사용하면 하나의 도메인 구조에서 모든 고객에 대해 개별적이고 보안된 URL을 자동으로 커버 가능
- 따라서 Route 53에 와일드카드 도메인 생성(A), 같은 리전의 ACM에서 와일드카드 인증서 발급(D), 그리고 API Gateway에 커스텀 도메인을 생성하고 인증서를 연결(F)

오답 이유

- **B.** 잘못된 리전에서 발급된 ACM 인증서는 API Gateway에서 사용할 수 없습니다. API Gateway와 동일한 리전에서 생성해야 합니다.
    
- **C.** 각 고객별로 호스티드 존을 만드는 것은 관리 복잡도를 크게 증가시키며 운영 효율성이 낮습니다.
    
- **E.** 고객마다 API 엔드포인트를 개별적으로 만들면 배포와 관리가 비효율적이며, 비용과 운영 부담이 커집니다.


## #533
한 회사가 Amazon S3에 데이터를 저장하고 있습니다. 규정에 따라, 데이터에는 개인 식별 정보(PII, Personally Identifiable Information)가 포함되어서는 안 됩니다.  
회사는 최근 S3 버킷에 일부 객체가 PII를 포함하고 있음을 발견했습니다.  
회사는 S3 버킷에서 PII를 자동으로 탐지하고, 보안 팀에 이를 알릴 수 있어야 합니다.  

이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. Amazon Macie를 사용합니다. Macie의 결과에서 SensitiveData 이벤트 유형을 필터링하기 위한 Amazon EventBridge 규칙을 생성하고, Amazon Simple Notification Service(Amazon SNS) 알림을 보안 팀에 보냅니다.  
B. Amazon GuardDuty를 사용합니다. GuardDuty의 결과에서 CRITICAL 이벤트 유형을 필터링하기 위한 Amazon EventBridge 규칙을 생성하고, Amazon Simple Notification Service(Amazon SNS) 알림을 보안 팀에 보냅니다.  
C. Amazon Macie를 사용합니다. Macie의 결과에서 SensitiveData:S3Object/Personal 이벤트 유형을 필터링하기 위한 Amazon EventBridge 규칙을 생성하고, Amazon Simple Queue Service(Amazon SQS) 알림을 보안 팀에 보냅니다.  
D. Amazon GuardDuty를 사용합니다. GuardDuty의 결과에서 CRITICAL 이벤트 유형을 필터링하기 위한 Amazon EventBridge 규칙을 생성하고, Amazon Simple Queue Service(Amazon SQS) 알림을 보안 팀에 보냅니다.

```
A company stores data in Amazon S3. According to regulations, the data must not contain personally identifiable information (PII). The company recently discovered that S3 buckets have some objects that contain PII. The company needs to automatically detect PII in S3 buckets and to notify the company’s security team.  
  
Which solution will meet these requirements?

- A. Use Amazon Macie. Create an Amazon EventBridge rule to filter the SensitiveData event type from Macie findings and to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.
- B. Use Amazon GuardDuty. Create an Amazon EventBridge rule to filter the CRITICAL event type from GuardDuty findings and to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.
- C. Use Amazon Macie. Create an Amazon EventBridge rule to filter the SensitiveData:S3Object/Personal event type from Macie findings and to send an Amazon Simple Queue Service (Amazon SQS) notification to the security team.
- D. Use Amazon GuardDuty. Create an Amazon EventBridge rule to filter the CRITICAL event type from GuardDuty findings and to send an Amazon Simple Queue Service (Amazon SQS) notification to the security team.
```

정답 : `A`

- Amazon Macie는 S3 내에서 PII 및 민감한 데이터를 자동으로 스캔하고 탐지하는 서비스
- Macie 탐지 결과는 EventBridge 이벤트로 전송할 수 있으며, 그 중 SensitiveData 이벤트 유형은 PII 관련 탐지를 의미
- 따라서 EventBridge 규칙을 통해 SensitiveData 유형의 이벤트를 필터링하고, SNS 주제로 알림을 전송하면 보안 팀이 즉시 통보 받음

오답 이유

- **B.** GuardDuty는 보안 위협(예: 악성 트래픽, 침입 시도)을 탐지하는 서비스이며, **PII나 데이터 내용 분석**은 수행하지 않습니다.
    
- **C.** Macie는 맞지만, **보안 알림에는 SNS가 적합**하며 SQS는 큐잉을 위한 용도입니다. 또한 SensitiveData:S3Object/Personal은 특정 세부 유형으로, 일반적인 SensitiveData 이벤트 필터보다 범위가 제한됩니다.
    
- **D.** GuardDuty는 PII를 탐지하지 않으며, SQS를 통한 알림은 실시간 통보 방식에 적합하지 않습니다.


## #534
한 회사가 여러 AWS 계정을 위해 로깅 솔루션을 구축하려고 합니다. 현재 회사는 모든 계정의 로그를 중앙 집중식 계정에 저장하고 있습니다. 회사는 중앙 계정에 Amazon S3 버킷을 생성하여 VPC 흐름 로그와 AWS CloudTrail 로그를 저장했습니다. 모든 로그는 잦은 분석을 위해 30일 동안 고가용성을 유지해야 하고, 백업 목적으로 추가 60일 동안 유지해야 하며, 생성 후 90일이 지나면 삭제되어야 합니다.

다음 중 어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적으로 충족합니까?

A. 객체를 생성 후 30일에 S3 Standard 스토리지 클래스으로 전환합니다. 90일 후 객체를 삭제하도록 Amazon S3에 지시하는 만료 동작을 작성합니다.
B. 객체를 생성 후 30일에 S3 Standard-Infrequent Access(S3 Standard-IA) 스토리지 클래스로 전환합니다. 90일 후 모든 객체를 S3 Glacier Flexible Retrieval 스토리지 클래스로 이동합니다. 90일 후 객체를 삭제하도록 Amazon S3에 지시하는 만료 동작을 작성합니다.
C. 객체를 생성 후 30일에 S3 Glacier Flexible Retrieval 스토리지 클래스로 전환합니다. 90일 후 객체를 삭제하도록 Amazon S3에 지시하는 만료 동작을 작성합니다.
D. 객체를 생성 후 30일에 S3 One Zone-Infrequent Access(S3 One Zone-IA) 스토리지 클래스로 전환합니다. 90일 후 모든 객체를 S3 Glacier Flexible Retrieval 스토리지 클래스로 이동합니다. 90일 후 객체를 삭제하도록 Amazon S3에 지시하는 만료 동작을 작성합니다.

```
A company wants to build a logging solution for its multiple AWS accounts. The company currently stores the logs from all accounts in a centralized account. The company has created an Amazon S3 bucket in the centralized account to store the VPC flow logs and AWS CloudTrail logs. All logs must be highly available for 30 days for frequent analysis, retained for an additional 60 days for backup purposes, and deleted 90 days after creation.  
  
Which solution will meet these requirements MOST cost-effectively?

- A. Transition objects to the S3 Standard storage class 30 days after creation. Write an expiration action that directs Amazon S3 to delete objects after 90 days.
- B. Transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class 30 days after creation. Move all objects to the S3 Glacier Flexible Retrieval storage class after 90 days. Write an expiration action that directs Amazon S3 to delete objects after 90 days.
- C. Transition objects to the S3 Glacier Flexible Retrieval storage class 30 days after creation. Write an expiration action that directs Amazon S3 to delete objects after 90 days.
- D. Transition objects to the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class 30 days after creation. Move all objects to the S3 Glacier Flexible Retrieval storage class after 90 days. Write an expiration action that directs Amazon S3 to delete objects after 90 days.
```

정답 : `C`

- 요구사항은 처음 30일은 고가용성 + 잦은 분석, 이후 50일은 백업 보관 후 90에 삭제
- 0~30일: S3 Standard
- 30~90일: 접근이 거의 없는 백업 보관이므로 가장 저렴한 Glacier 계열이 비용 최적
	- Glacier Flexible Retrieval은 90일 최소 저장 요금이 저장되지만, 어차피 30일 시점에 전환해 90일에 삭제하면 총 과금은 여전히 Glacier가 Standard-IA보다 저렴한 경우가 일반적

오답 이유

- **A.** 생성 30일 후 **Standard로 전환**은 사실상 **변화가 없고**(기본이 Standard), 30~90일 구간에서 더 저렴한 클래스(IA/Glacier)를 쓰지 않으므로 **비용 비효율**입니다.
    
- **B.** 30일에 **Standard-IA로 전환**은 일부 절감이 되지만, **90일에 Glacier로 이동**하고 **동시에 삭제**하도록 설정하는 것은 **모순**입니다(이동과 삭제가 같은 시점). 또한 Glacier 이동이 불필요합니다.
    
- **D.** 30일에 **One Zone-IA**는 단일 AZ라 내구성/가용성 리스크가 있고(특히 로그 보관에 일반적으로 권장되지 않음), B와 마찬가지로 **90일에 Glacier로 이동하면서 곧바로 삭제**는 **모순**이며 불필요한 단계입니다.


## #535
한 회사가 워크로드를 위해 Amazon Elastic Kubernetes Service(Amazon EKS) 클러스터를 구축하고 있습니다. Amazon EKS에 저장되는 모든 시크릿은 Kubernetes etcd 키-값 저장소에서 암호화되어야 합니다.

이 요구 사항을 충족하는 솔루션은 무엇입니까?

A. 새 AWS Key Management Service(AWS KMS) 키를 생성합니다. AWS Secrets Manager를 사용하여 Amazon EKS의 모든 시크릿을 관리, 로테이션 및 저장합니다.
B. 새 AWS Key Management Service(AWS KMS) 키를 생성합니다. Amazon EKS 클러스터에서 Amazon EKS KMS 시크릿 암호화를 활성화합니다.
C. 기본 옵션으로 Amazon EKS 클러스터를 생성합니다. 추가 기능으로 Amazon Elastic Block Store(Amazon EBS) Container Storage Interface(CSI) 드라이버를 사용합니다.
D. alias/aws/ebs 별칭이 지정된 새 AWS Key Management Service(AWS KMS) 키를 생성합니다. 계정에 대해 기본 Amazon Elastic Block Store(Amazon EBS) 볼륨 암호화를 활성화합니다.

```
A company is building an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for its workloads. All secrets that are stored in Amazon EKS must be encrypted in the Kubernetes etcd key-value store.  
  
Which solution will meet these requirements?

- A. Create a new AWS Key Management Service (AWS KMS) key. Use AWS Secrets Manager to manage, rotate, and store all secrets in Amazon EKS.
- B. Create a new AWS Key Management Service (AWS KMS) key. Enable Amazon EKS KMS secrets encryption on the Amazon EKS cluster.
- C. Create the Amazon EKS cluster with default options. Use the Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver as an add-on.
- D. Create a new AWS Key Management Service (AWS KMS) key with the alias/aws/ebs alias. Enable default Amazon Elastic Block Store (Amazon EBS) volume encryption for the account.
```

정답 : `B`

- EKS의 etcd에 저장되는 Kubernetes Secret을 암호화하려면 EKS의 "Secrets Encryption" 기능을 활성화하고 KMS 키를 연동해 envelope encryption을 적용해야 함

오답 이유

- **A.** Secrets Manager는 **클러스터 외부**에 비밀을 저장·관리하는 서비스이며, 이를 사용해도 **etcd에 저장된 Kubernetes Secret 자체의 암호화**가 자동으로 보장되지는 않습니다.
    
- **C.** EBS CSI 드라이버는 **퍼시스턴트 볼륨** 관련이며, etcd 내 Secret 암호화와 직접 관련이 없습니다.
    
- **D.** EBS 기본 암호화(별칭 alias/aws/ebs)는 **블록 스토리지** 암호화로, **etcd의 Secret 암호화** 요건을 충족하지 않습니다.


## #536
한 회사는 자사의 프로덕션 Amazon RDS for PostgreSQL 데이터베이스에 대해 데이터 사이언티스트들에게 거의 실시간(near real-time) 읽기 전용 액세스를 제공하고자 합니다. 해당 데이터베이스는 현재 Single-AZ 데이터베이스로 구성되어 있습니다. 데이터 사이언티스트들은 프로덕션 데이터베이스에 영향을 주지 않을 복잡한 쿼리를 사용합니다. 회사는 고가용성이 필요한 솔루션을 원합니다.

다음 중 어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적으로 충족합니까?

A. 유지보수 기간에 기존 프로덕션 데이터베이스의 규모를 확장하여 데이터 사이언티스트에게 충분한 성능을 제공합니다.
B. Single-AZ에서 Multi-AZ 인스턴스 배포로 변경하고, 더 큰 보조 스탠바이 인스턴스를 구성합니다. 데이터 사이언티스트에게 보조 인스턴스에 대한 액세스를 제공합니다.
C. Single-AZ에서 Multi-AZ 인스턴스 배포로 변경합니다. 데이터 사이언티스트를 위해 두 개의 추가 읽기 전용 복제본(read replica)을 제공합니다.
D. Single-AZ에서 두 개의 읽기 가능한 스탠바이 인스턴스가 있는 Multi-AZ 클러스터 배포로 변경합니다. 데이터 사이언티스트에게 읽기 엔드포인트를 제공합니다.

```
A company wants to provide data scientists with near real-time read-only access to the company's production Amazon RDS for PostgreSQL database. The database is currently configured as a Single-AZ database. The data scientists use complex queries that will not affect the production database. The company needs a solution that is highly available.  
  
Which solution will meet these requirements MOST cost-effectively?

- A. Scale the existing production database in a maintenance window to provide enough power for the data scientists.
- B. Change the setup from a Single-AZ to a Multi-AZ instance deployment with a larger secondary standby instance. Provide the data scientists access to the secondary instance.
- C. Change the setup from a Single-AZ to a Multi-AZ instance deployment. Provide two additional read replicas for the data scientists.
- D. Change the setup from a Single-AZ to a Multi-AZ cluster deployment with two readable standby instances. Provide read endpoints to the data scientists.
```

정답 : `C`

- 프로덕션의 고가용성을 위해 쓰기 인스턴스는 Multi-AZ 인스턴스 배포로 전환하고 데이터 사이언티스트 쿼리는 읽기 전용 복제본으로 오프로드하는 것이 표준 해법
- 읽기 복제본은 비동기 복제로 "거의 실시간"에 부합하며, 두 개의 복제본을 두면 읽기 측면의 가용성/스케일링도 확보

오답 이유

- **A.** 프로덕션 인스턴스 스케일업은 **읽기 전용 격리**가 되지 않아 분석 쿼리가 프로덕션에 영향을 줄 수 있고, **고가용성 요건**도 충족하지 못합니다.
    
- **B.** RDS **Multi-AZ 인스턴스 배포의 스탠바이 인스턴스는 읽기 불가**입니다. 또한 “더 큰 스탠바이”라는 개념도 없습니다(클래스 동일). 읽기 전용 접근 제공 불가.
    
- **D.** **Multi-AZ DB 클러스터(읽기 가능 스탠바이)** 는 높은 내구성과 빠른 장애 조치를 제공하지만, **읽기 복제본 대비 비용이 높아** 문제의 “가장 비용 효율적” 조건과 상충합니다.


## #537
한 회사가 AWS 클라우드에서 3계층 웹 애플리케이션을 운영하고 있으며, 이는 세 개의 가용 영역에 걸쳐 동작합니다. 애플리케이션 아키텍처는 Application Load Balancer, 사용자 세션 상태를 호스팅하는 Amazon EC2 웹 서버, 그리고 EC2 인스턴스에서 실행되는 MySQL 데이터베이스로 구성되어 있습니다. 회사는 애플리케이션 트래픽이 갑자기 증가할 것으로 예상합니다. 회사는 향후 애플리케이션 용량 수요를 충족하기 위해 확장할 수 있고, 세 개의 가용 영역 전체에서 고가용성을 보장하고자 합니다.

어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. MySQL 데이터베이스를 Amazon RDS for MySQL의 Multi-AZ DB 클러스터 배포로 마이그레이션합니다. 세션 데이터를 저장하고 읽기 캐시를 위해 고가용성으로 Amazon ElastiCache for Redis를 사용합니다. 웹 서버를 세 개의 가용 영역에 있는 Auto Scaling 그룹으로 마이그레이션합니다.
B. MySQL 데이터베이스를 Amazon RDS for MySQL의 Multi-AZ DB 클러스터 배포로 마이그레이션합니다. 세션 데이터를 저장하고 읽기 캐시를 위해 고가용성으로 Amazon ElastiCache for Memcached를 사용합니다. 웹 서버를 세 개의 가용 영역에 있는 Auto Scaling 그룹으로 마이그레이션합니다.
C. MySQL 데이터베이스를 Amazon DynamoDB로 마이그레이션합니다. 읽기 캐시를 위해 DynamoDB Accelerator (DAX)를 사용합니다. 세션 데이터를 DynamoDB에 저장합니다. 웹 서버를 세 개의 가용 영역에 있는 Auto Scaling 그룹으로 마이그레이션합니다.
D. MySQL 데이터베이스를 단일 가용 영역의 Amazon RDS for MySQL로 마이그레이션합니다. 세션 데이터를 저장하고 읽기 캐시를 위해 고가용성으로 Amazon ElastiCache for Redis를 사용합니다. 웹 서버를 세 개의 가용 영역에 있는 Auto Scaling 그룹으로 마이그레이션합니다.

```
A company runs a three-tier web application in the AWS Cloud that operates across three Availability Zones. The application architecture has an Application Load Balancer, an Amazon EC2 web server that hosts user session states, and a MySQL database that runs on an EC2 instance. The company expects sudden increases in application traffic. The company wants to be able to scale to meet future application capacity demands and to ensure high availability across all three Availability Zones.  
  
Which solution will meet these requirements?

- A. Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment. Use Amazon ElastiCache for Redis with high availability to store session data and to cache reads. Migrate the web server to an Auto Scaling group that is in three Availability Zones.
- B. Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment. Use Amazon ElastiCache for Memcached with high availability to store session data and to cache reads. Migrate the web server to an Auto Scaling group that is in three Availability Zones.
- C. Migrate the MySQL database to Amazon DynamoDB Use DynamoDB Accelerator (DAX) to cache reads. Store the session data in DynamoDB. Migrate the web server to an Auto Scaling group that is in three Availability Zones.
- D. Migrate the MySQL database to Amazon RDS for MySQL in a single Availability Zone. Use Amazon ElastiCache for Redis with high availability to store session data and to cache reads. Migrate the web server to an Auto Scaling group that is in three Availability Zones.
```

정답 : `A`

- DB 계층: RDS for MySQL Multi-AZ DB 클러스터로 전환하면 다중 AZ에 읽기 가능 스탠바이/리더를 둬 고가용성과 확장성(읽기 분산)을 확보
- 세션/캐시 계층: 세션 상태는 인스턴스 외부로 분리해야 스케일아웃이 가능하며, Redis가 세션 저장과 캐싱에 적합
- 웹 계층: 3개 AZ에 걸친 오토 스케일링 그룹으로 급증 트래픽에 자동 확장 및 AZ 장애 대비를 달성

오답 이유

- **B (Memcached 사용)**: Memcached는 **영속성/복제/자동 장애조치가 없고** 세션 저장에 부적합합니다. 고가용 세션 스토어로는 Redis가 표준입니다.
    
- **C (DynamoDB/DAX로 전환)**: NoSQL로의 **데이터 모델 변경/마이그레이션 비용**이 크고, 기존 3계층 RDBMS 워크로드 요구와 동떨어집니다. 또한 “가장 비용 효율적” 관점에서 과한 리팩토링입니다.
    
- **D (RDS 단일 AZ)**: 데이터베이스가 **단일 AZ**이면 전체 요건의 핵심인 **3개 AZ 고가용성**을 충족하지 못합니다.


## #538
한 글로벌 비디오 스트리밍 회사가 콘텐츠 전송 네트워크(CDN)로 Amazon CloudFront를 사용하고 있습니다.  
회사는 여러 국가에 걸쳐 콘텐츠를 단계적으로 롤아웃(배포)하려고 합니다.  
회사는 롤아웃 대상 국가 외의 시청자가 콘텐츠를 볼 수 없도록 해야 합니다.  

어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. CloudFront에서 허용 목록(allow list)을 사용하여 콘텐츠에 지리적 제한을 추가합니다. 사용자 지정 오류 메시지를 설정합니다.  
B. 제한된 콘텐츠에 대해 새 URL을 설정합니다. 서명된 URL과 쿠키를 사용하여 액세스를 승인합니다. 사용자 지정 오류 메시지를 설정합니다.  
C. 배포하는 콘텐츠 데이터를 암호화합니다. 사용자 지정 오류 메시지를 설정합니다.  
D. 제한된 콘텐츠에 대해 새 URL을 생성합니다. 서명된 URL에 대해 시간 제한 액세스 정책을 설정합니다.

```
A global video streaming company uses Amazon CloudFront as a content distribution network (CDN). The company wants to roll out content in a phased manner across multiple countries. The company needs to ensure that viewers who are outside the countries to which the company rolls out content are not able to view the content.  
  
Which solution will meet these requirements?

- A. Add geographic restrictions to the content in CloudFront by using an allow list. Set up a custom error message.
- B. Set up a new URL tor restricted content. Authorize access by using a signed URL and cookies. Set up a custom error message.
- C. Encrypt the data for the content that the company distributes. Set up a custom error message.
- D. Create a new URL for restricted content. Set up a time-restricted access policy for signed URLs.
```

정답 : `A`

- CloudFront의 지리적 제한 기능은 국가 기반으로 콘텐츠 접근을 허용하거나 차단 가능
- Allow List를 설정하면, 지정한 국가의 사용자만 콘텐츠에 접근할 수 있고, 다른 지역의 사용자는 접근이 차단

오답 이유

- **B.** 서명된 URL/쿠키는 **인증 기반 접근 제어**에 적합하지만, **국가별 지리적 접근 제한**은 수행하지 않습니다.
    
- **C.** 콘텐츠 암호화는 **보호(보안)** 목적이며, 지리적 위치에 따른 접근 제한 기능은 없습니다.
    
- **D.** 서명된 URL의 시간 제한 정책은 **유효 시간 제어**용으로, 특정 국가나 지역 기반의 접근 제어에는 사용할 수 없습니다.


## #539
한 회사는 온프레미스 재해 복구(DR) 구성을 개선하기 위해 AWS 클라우드를 사용하려고 합니다. 회사의 핵심 프로덕션 비즈니스 애플리케이션은 가상 머신(VM)에서 실행되는 Microsoft SQL Server Standard를 사용합니다. 애플리케이션의 복구 지점 목표(RPO)는 30초 이내이며 복구 시간 목표(RTO)는 60분입니다. DR 솔루션은 가능한 한 비용을 최소화해야 합니다.

어떤 솔루션이 이러한 요구 사항을 충족합니까?

A. Microsoft SQL Server Enterprise의 Always On 가용성 그룹을 사용하여 온프레미스 서버와 AWS 간에 멀티사이트 액티브/액티브 구성을 구성합니다.
B. AWS에서 웜 스탠바이 Amazon RDS for SQL Server 데이터베이스를 구성합니다. AWS Database Migration Service(AWS DMS)를 변경 데이터 캡처(CDC)를 사용하도록 구성합니다.
C. AWS Elastic Disaster Recovery를 사용하여 디스크 변경 사항이 AWS로 파일럿 라이트로 복제되도록 구성합니다.
D. 타사 백업 소프트웨어를 사용하여 매일 밤 백업을 캡처합니다. Amazon S3에 2차 백업 세트를 저장합니다.

```
A company wants to use the AWS Cloud to improve its on-premises disaster recovery (DR) configuration. The company's core production business application uses Microsoft SQL Server Standard, which runs on a virtual machine (VM). The application has a recovery point objective (RPO) of 30 seconds or fewer and a recovery time objective (RTO) of 60 minutes. The DR solution needs to minimize costs wherever possible.  
  
Which solution will meet these requirements?

- A. Configure a multi-site active/active setup between the on-premises server and AWS by using Microsoft SQL Server Enterprise with Always On availability groups.
- B. Configure a warm standby Amazon RDS for SQL Server database on AWS. Configure AWS Database Migration Service (AWS DMS) to use change data capture (CDC).
- C. Use AWS Elastic Disaster Recovery configured to replicate disk changes to AWS as a pilot light.
- D. Use third-party backup software to capture backups every night. Store a secondary set of backups in Amazon S3.
```

정답 : `C`

- AWS Elastic Disaster Recovery(EDR)는 블록-레벨로 지속 복제를 수행하므로 일반적으로 초 단위 RPO(수십 초 이내)를 달성할 수 있고, 장애 시 표준 런북으로 인스턴스를 기동하여 분 단위~수십 분 내 RTO를 달성하기에 RTO 60분, RPO 30초 요구에 부합
- 평상시에는 최소 리소스(복제 서버/스토리지)만 사용하고, 복구 시에만 대상 인스턴스를 기동하므로 파일럿 라이트 아키텍처로 비용을 최소화
- 기존 VM 기반 SQL Server Standard 환경을 그대로 이미지 수준으로 재현하므로 애플리케이션 변경이 거의 필요 없음

오답 이유

- **A.** SQL Server **Enterprise + Always On AG(액티브/액티브)** 는 낮은 RPO/RTO를 달성할 수 있으나 **엔터프라이즈 라이선스/이중 운영 비용**이 매우 높아 “가능한 한 비용 최소화” 요건에 부합하지 않습니다. 또한 현재 표준판(Standard) 사용 중입니다.
    
- **B.** **RDS SQL Server(웜 스탠바이) + DMS CDC** 는 근실시간 동기화가 가능하지만, 상시 구동되는 RDS 인스턴스 비용과 DMS 사용 비용이 발생합니다. 또한 스키마/호환성, 에이전트 구성 등 **마이그레이션/운영 복잡도**가 증가하며, **RPO 30초 보장**도 워크로드에 따라 불확실할 수 있습니다. 비용 측면에서 파일럿 라이트보다 불리합니다.
    
- **D.** **야간 백업**은 일반적으로 **RPO가 24시간 수준**으로, 요구 RPO(30초) 및 RTO(60분)를 충족할 수 없습니다. 단순 백업은 DR 요구에 부족합니다.


## #540
한 회사는 온프레미스 서버에서 Oracle 데이터베이스를 사용해 고객 정보를 처리하고 저장하고 있습니다. 회사는 더 높은 가용성을 달성하고 애플리케이션 성능을 개선하기 위해 AWS 데이터베이스 서비스를 사용하려고 합니다. 또한 기본 데이터베이스 시스템에서 보고(리포팅) 부하를 오프로드하고자 합니다.

가장 운영 효율적인 방식으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?

A. AWS Database Migration Service(AWS DMS)를 사용하여 여러 AWS 리전에 Amazon RDS DB 인스턴스를 생성합니다. 보고 기능은 기본 DB 인스턴스와 별도의 DB 인스턴스를 사용하도록 지정합니다.
B. 단일 가용 영역(Single-AZ) 배포로 Amazon RDS에 Oracle 데이터베이스를 생성합니다. 기본 DB 인스턴스와 같은 가용 영역에 읽기 전용 복제본을 생성합니다. 보고 기능을 읽기 복제본으로 라우팅합니다.
C. 다중 가용 영역(Multi-AZ) 클러스터 배포로 Amazon RDS에 Oracle 데이터베이스를 생성합니다. 보고 기능이 클러스터 배포의 리더(Reader) 인스턴스를 사용하도록 지정합니다.
D. 다중 가용 영역(Multi-AZ) 인스턴스 배포로 Amazon RDS에 Amazon Aurora 데이터베이스를 생성합니다. 보고 기능을 리더 인스턴스들로 라우팅합니다.

```
A company has an on-premises server that uses an Oracle database to process and store customer information. The company wants to use an AWS database service to achieve higher availability and to improve application performance. The company also wants to offload reporting from its primary database system.  
  
Which solution will meet these requirements in the MOST operationally efficient way?

- A. Use AWS Database Migration Service (AWS DMS) to create an Amazon RDS DB instance in multiple AWS Regions. Point the reporting functions toward a separate DB instance from the primary DB instance.
- B. Use Amazon RDS in a Single-AZ deployment to create an Oracle database. Create a read replica in the same zone as the primary DB instance. Direct the reporting functions to the read replica.
- C. Use Amazon RDS deployed in a Multi-AZ cluster deployment to create an Oracle database. Direct the reporting functions to use the reader instance in the cluster deployment.
- D. Use Amazon RDS deployed in a Multi-AZ instance deployment to create an Amazon Aurora database. Direct the reporting functions to the reader instances.
```

정답 : `D`

- Amazon Aurora는 기본적으로 다중 AZ 고가용성과 자동 장애 조치를 제공하고, 리더(읽기 전용) 인스턴스/리더 엔드포인트로 보고 쿼리를 오프로드하여 성능을 높일 수 있음

오답 이유

- **A.** 여러 리전에 별도 RDS 인스턴스를 만들고 DMS로 동기화하는 구성은 **운영 복잡도와 비용**이 큽니다. 또한 **크로스리전 지연**과 **충분히 짧은 RPO 보장**이 어려울 수 있습니다. “가장 운영 효율적” 조건에 부적합.
    
- **B.** **Single-AZ는 고가용성 미충족**입니다. 게다가 RDS for Oracle의 읽기 복제본은 **에디션/기능 제약(Active Data Guard 등)** 이 있고, 같은 AZ에 두는 것도 내결함성 측면에서 취약합니다.
    
- **C.** RDS **Multi-AZ DB 클러스터의 ‘읽기 가능 스탠바이/리더’ 개념은 MySQL/PostgreSQL 계열**에 해당합니다. **Oracle은 Multi-AZ 인스턴스 배포(스탠바이 읽기 불가)** 로, 제시된 “리더 인스턴스에 보고 라우팅”이 성립하지 않습니다.